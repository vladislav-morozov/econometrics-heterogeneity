--- 
description: "Learn (Lecture Notes)"

open-graph:
    description: "Learn (Lecture Notes)"
---

# Interlude: Characteristic Functions and Deconvolution {#sec-linear-chf}

::: {.callout-note appearance="simple" icon=false}

## Summary and Learning Outcomes

This section  

By the end of this section, you should be able to:

- Goal 1
:::

We briefly review the 
This section can be freely skipped if you are familiar with this materials



## Characteristic Functions

If $\bV$ is a random $T$-vector, then the characteristic function $\varphi_{\bV}(s): \R^T\to \C$ of $\bV$ is defined as follows:
$$
 \varphi_{\bV}(\bs) = \E[\exp(i\bs'\bV)].
$$
See @Durrett2019 (or other probability textbooks) regarding general properties of characteristic functions.


For our purposes, we will need the following three key properties:

1. The characteristic function uniquely determines the distribution.
	
2. Let $\bV, \bU$ be two independent random vectors. Then the characteristic function of their sum $\bV+\bU$ is equal to the product of characteristic function of $\bV$ and $\bU$:
$$
\begin{aligned}
		\varphi_{\bV+\bU}(\bs) & =	\E\left[e^{i\bs'(\bV+\bU)}\right] = \E\left[e^{i\bs'\bV}e^{i\bs'\bU} \right]\\
        & = \E\left[e^{i\bs'\bV}\right]\E\left[e^{i\bs'\bU}\right]\\
        &  =  \varphi_{\bV}(\bs) \varphi_{\bU}(\bs).
\end{aligned}
$$ {#eq-linear-chf-independence}
	
3. Let $\bbeta$ be a random $p$-vector and $\bX$ a matrix. Then
$$
\begin{aligned}
		\varphi_{\bX\bbeta}(\bs) & = \E\left[\exp(i\bs'(\bX\bbeta)) \right] \\
        & = \E\left[\exp(i(\bX'\bs)'\bbeta) \right]\\
        &  = \varphi_{\bbeta}(\bX'\bs)
\end{aligned}
$$

Conditional characteristic functions may be defined analogously using conditional expectations in place of unconditional ones. 


## Nonparametric Deconvolution

Property ([-@eq-linear-chf-independence]) is a particularly useful feature of characteristic functions for statistical applications. It forms the basis of an estimation and identification approach known as *deconvolution* â€” the approach we will take to identify the distribution of the coefficients $\bbeta_i$ in the next section. 

At heart, deconvolution is simple. Suppose that we observe a random vector $\bY$. $\bY$ is generated as a sum of two independent vector $\bV$ and $\bU$. The distribution of $\bU$ is known, while the distribution of $\bV$ is the object of interest. 

By property ([-@eq-linear-chf-independence]) i

If $\varphi_{\bU}(\bs)\neq 0$, we can divide

(or at least does not have "too many" zeros)


The name of the procedure stems from the fact that the distribution of $\bY$ is the convolution of distributions of $\bV$ and $\bU$. We now 

::: {.callout-note appearance="simple" icon=false}


It is possible to relax the assumption that the distribution of $\bU$ using mulitple observations. We will see one approach in the following section. Another approach uses a second observation of $\bY$ using a result called Kotlarski's lemma [@Kotlarski1967CharacterizingGammaNormal] (see @Evdokimov2012, @Lewbel2022 for extensions). Used with measurement error [see a review in @Schennach2016RecentAdvancesMeasurement] in nonparametric panel data models @Evdokimov2010.

:::


---

#### Next Section {.unnumbered}

In the next section, we  