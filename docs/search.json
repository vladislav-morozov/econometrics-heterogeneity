[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics with Unobserved Heterogeneity",
    "section": "",
    "text": "Course Information\nInstructor: Vladislav Morozov\nEmail: morozov [at] uni-bonn.de\nLevel: Second-year Master’s and PhD students\nCourse Website: eCampus and this website\nDOI:",
    "crumbs": [
      "Course Information"
    ]
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "Econometrics with Unobserved Heterogeneity",
    "section": "",
    "text": "Short Description\nCausal inference with unobserved heterogeneity is the core challenge of empirical economics. Whether due to heterogeneous treatment effects, omitted confounders, or measurement error, unobserved factors create confounding in the data and require developing appropriate methods. This course surveys some modern identification strategies and estimation methods designed to recover causal relationships from observational data, where treatments are endogenously selected and key determinants of outcomes are hidden.\nThe course is structured into three main parts:\n\nLinear models with heterogeneous coefficients.\nNonparametric models with unobserved heterogeneity.\nQuantile and distributional treatment effects and quantile and distributional regression methods.\n\nWe focus exclusively on observational cross-sectional and panel data, where unobserved heterogeneity cannot be ignored. The emphasis is on identification, rather than asymptotic theory or prediction.\n\n\nCourse Materials\nTextbook: None. This course is based on lecture notes and background articles, which are listed in the references.\nRecommended Readings: Please see the bibliography for a detailed reading list.\n\n\nAbout These Notes\nThese lectures notes are based on a topics course I delivered at the University of Bonn. They are currently being uploaded in blocks as I organize and transform my notes. These notes are a work in progress. If you find any typos or have suggestions, please open an issue on GitHub using the link on the right!\nCurrent version (February 10, 2026): the first two blocks are fully complete, with the third block in progress. Latest addition: introduction to quantile and distributional treatment effects, along with their identification.\n\n\nLearning Outcomes\nBy the end of this course, students will be able to:\n\nIdentify and explain different sources of unobserved heterogeneity in economic data.\nApply econometric methods, such as heterogeneous coefficient models and quantile regression, to account for unobserved heterogeneity.\nEvaluate the trade-offs and assumptions underlying different modeling approaches in empirical research.\n\n\n\nSyllabus\nBlock 0: Introduction to Unobserved Heterogeneity\n\nDefinition and a brief classification\nExamples in applied econometrics\nOverview of key methodological challenges\n\nBlock 1: Linear Models with Heterogeneous Coefficients\n\nLinear models and their applicability\n\nHeterogeneity in linear models\n\nAverage effects:\n\nIssues with the within estimator under heterogeneity\n\nInterlude: Dynamic panels with random intercepts\n\nIssues with dynamic panel IV estimators under heterogeneity\n\nRobust estimation with the mean group estimator\n\n\nVariance of coefficients\n\nDistribution of coefficients:\n\nA primer on deconvolution\n\nIdentification of the distribution\n\nEstimation with discrete covariates\n\n\nBlock 2: Nonparametric Models with Unobserved Heterogeneity\n\nA partial classification of nonparametric models with unobserved heterogeneity\n\nIntroduction to nonseparable models\n\nHeterogeneity bias and issues with identification in cross-sectional settings\n\nIdentification and estimation of average marginal effects in fully nonseparable models with panel data:\n\nIdentification for stayers under strong stationarity\n\nExtending identification results beyond stayers\n\nAccommodating changes in the structural function\n\n\nVariance of marginal effects in nonseparable models\n\nBlock 3: Quantile and Distribution Treatment Effects:\n\nBackground: Quantiles and their properties\n\nCausal framework: Quantile and distributional treatment effects and their interpretation\n\nIdentification of QTEs and DTEs under unconfoundedness\n\nMethods:\n\nQuantile regression:\n\nGeneral formulation\n\nWhen is quantile regression correctly specified?\n\nQuantile crossing and rearrangement techniques\n\n\nDistribution regression:\n\nGeneral formulation\n\nRearrangement for distribution regression\n\n\n\nEstimation of QTEs and DTEs",
    "crumbs": [
      "Course Information"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction: Causal Inference with Unobserved Heterogeneity",
    "section": "",
    "text": "1.1 The Problem of Unobserved Heterogeneity",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Causal Inference with Unobserved Heterogeneity</span>"
    ]
  },
  {
    "objectID": "intro.html#the-problem-of-unobserved-heterogeneity",
    "href": "intro.html#the-problem-of-unobserved-heterogeneity",
    "title": "1  Introduction: Causal Inference with Unobserved Heterogeneity",
    "section": "",
    "text": "1.1.1 Introduction\nIn economics, business, and the social sciences, heterogeneity is the rule. No two individuals, firms, or countries are identical, even when they appear statistically similar in observed data. A worker with the same years of education as another may earn a different wage due to unobserved ability. Two firms in the same industry may respond differently to a policy shock because of latent managerial practices. A country’s growth trajectory may diverge from its peers’ due to unmeasured institutions or cultural norms. These unobserved differences — preferences, productivity, measurement errors, or idiosyncratic shocks — determine how agents respond to treatments, policies, or market conditions.\nOur objective is to estimate causal parameters: treatment effects (or their moments and distributions), policy impacts, or other structural quantities that describe how outcomes would change under interventions. We focus on counterfactuals, rather than associational patterns.\nIn observational data, however, this task is complicated by unobserved heterogeneity. Treatments (broadly defined) are endogenously selected based on unobserved factors, creating confounding. Naive comparisons fail to isolate causal effects because the treatment-outcome relationship is confounded by unobserved determinants. The challenge of causal inference with unobserved heterogeneity is central to empirical research and the focus of this course.\n\n\n1.1.2 Broad Setting\nFormally, suppose we observe a dataset \\(\\lbrace(Y_i, X_i)\\rbrace_{i=1}^N\\) (cross-section) or \\(\\{(Y_{it}, X_{it})\\}_{i=1}^N\\) (panel), where \\(Y_i\\) is the realized outcome for unit \\(i\\) and \\(X_i\\) includes observed covariates (e.g., treatments, controls).\nAt all points in the course, we adopt the following potential outcomes framework. The outcome of unit \\(i\\) (in period \\(t\\)) under “treatment” \\(x\\) is determined as\n\\[\n\\begin{aligned}\n    Y_i^x & = \\phi(x, A_i),  && \\text{(cross-section)}\\\\\n  Y_{it}^x & = \\phi(x, A_i, U_{it}), && \\text{(panel)}\n\\end{aligned}\n\\tag{1.1}\\]\nwhere:\n\n\\(\\phi(\\cdot)\\) is an unknown structural function.\n\\(A_i\\) is time-invariant unobserved heterogeneity.\n\\(U_{it}\\) is time-varying unobserved heterogeneity.\n\nOur goal is causal: to infer some feature of \\(\\phi(\\cdot)\\), such as average treatment or marginal effects, the variance or distribution of such effects, or distributional features of potential outcomes.\nA key challenge to our work is the presence of \\((A_i, U_{it})\\) which are never observed but systematically influence both treatments and outcomes.\n\n\n1.1.3 Types of Unobserved Heterogeneity\nWhat are the \\((A_i, U_{it})\\)? Unobserved heterogeneity arises from three (overlapping) primary sources:\n\nOmitted Confounders:\n\nLatent variables correlated with both \\(X_i\\) and \\(Y_i\\) .\nExample: Estimating the returns to education \\(X_i\\) when unobserved ability \\(A_i\\) affects both education and wages \\(Y_i\\).\n\nHeterogeneous Treatment Effects:\n\nThe impact of \\(X_i\\) varies across units due to \\((A_i, U_{it})\\), meaning effects are individual-specific.\nExample: The benefits \\(A_i\\) of job training program \\(X_i\\) may be larger for some units than for others.\n\nMeasurement Error:\n\nObserved covariates \\(X_i\\) are noisy proxies for true values.\nExample: Using self-reported income to proxy true earnings.\n\n\n\n\n1.1.4 Consequences of Ignoring Unobserved Heterogeneity\nIn randomized experiments, unobserved heterogeneity is less problematic for causal inference because: \\[\n\\curl{(X_{it}}_{t=1}^T \\perp\\!\\!\\!\\perp A_i, U_{it} \\quad \\text{(by design)},\n\\] where independence may hold conditionally on some further variables. In other words, treatment assignment is independent of the individual determinants of the potential outcomes. As a consequence, various transformations and comparisons of treatment groups directly estimate causal parameters of interest.\nHowever, in observational data, this independence fails: \\[\n\\curl{X_{it}}_{t=1}^T \\not\\!\\perp\\!\\!\\!\\perp (A_i, U_{it}).\n\\] Real-world data is generated by agents making choices based on all information, including the information that is not recorded in the final datasets. A student selects a college \\(X_i\\) based on unobserved ambition \\(A_i\\); a firm adopts a technology \\(X_{it}\\) based on unobserved costs \\((A_i, U_{it})\\); a patient complies with a medical treatment based on unobserved health beliefs. The resulting data is the result of these endogenous decisions, meaning that naive statistical associations between \\(Y\\) and \\(X\\) are almost always confounded by \\((A_i, U_{it})\\). This is the core problem of causal inference with unobserved heterogeneity. The following graphs schematically illustrate the point:\n\n\n\n\n\n\n\nCombinedGraph\n\n\ncluster_exp\n\nExperimental data: no confounding\n\n\ncluster_obs\n\nObservational data: confounding\n\n\n\nA1\n\nUnobserved\n\n\n\nC1\n\nCovariates\n\n\n\nA1-&gt;C1\n\n\n\n\n\nB1\n\nObserved\n\n\n\nB1-&gt;C1\n\n\n\n\n\nA2\n\nUnobserved\n\n\n\nB2\n\nObserved\n\n\n\nA2-&gt;B2\n\n\n\n\nC2\n\nCovariates\n\n\n\nA2-&gt;C2\n\n\n\n\n\nB2-&gt;C2\n\n\n\n\n\n\n Comparison of causal structures: experimental vs observational \n\n\n\nIgnoring this confounding leads to misleading conclusions. The issues manifests even in simple linear regression, with some basic examples including:\n\nOmitted variable bias: if an important explanatory variable is missing and correlated with observed covariates, coefficient estimates may be biased and inconsistent.\nAttenuation bias: When covariates suffer from measurement error, estimated effects tend to be systematically biased toward zero.\n\nIn nonlinear models, the resulting biases become completely unpredictable even in parametric models (e.g. Stefanski and Carroll 1985).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Causal Inference with Unobserved Heterogeneity</span>"
    ]
  },
  {
    "objectID": "intro.html#this-course",
    "href": "intro.html#this-course",
    "title": "1  Introduction: Causal Inference with Unobserved Heterogeneity",
    "section": "1.2 This Course",
    "text": "1.2 This Course\n\n1.2.1 Course Description\nAs a response to this issue, econometricians have developed a range of statistical techniques that are suitable for observational data and robust to unobserved heterogeneity in varying senses.\nThis course surveys some of the advances in this field, structured into three key topics:\n\nLinear models with heterogeneous coefficients: extending traditional regression models to allow for individual-specific responses to treatment.\nNonparametric models with unobserved heterogeneity: models that do not restrict the form of heterogeneity or how it affects the outcome.\nQuantile and distribution regression: approaches that focus on quantile and distributional treatment effects, rather than the distribution of treatment effects.\n\nThroughout, we focus on non-experimental (observational) data, where unobserved heterogeneity cannot be ignored. We also allow for non-binary and continuous treatments throughout. Finally, we emphasize identification strategies over asymptotic theory.\n\n\n1.2.2 A Common Theme\nA common theme is that there always will be a price to pay for learning any feature of the counterfactual distribution. This price is paid in the form of assumptions and restrictions on the model (1.1) — another manifestation of the fundamental problem of causal inference. Some potential courses of action will include:\n\nImposing functional form restrictions on \\(\\phi\\) (e.g. parametric assumptions such as linearity, or assuming that \\(\\alpha_i\\) is scalar and that it enters \\(\\phi\\) monotonically);\nRestricting the extent of unobserved heterogeneity in the model (e.g. assuming that there is scalar unobserved variables or a vector of heterogeneous coefficients);\nRestricting the relationship between the observed and unobserved variables (e.g. assuming that \\(\\alpha_i\\) is independent from \\(x_i\\));\nFocusing on particular parts of the distribution of the outcome (such as quantiles).\n\n\n\n1.2.3 Overview\nWe will see examples of each approach throughout the course, and one should see each particular model discussed as a specific variation of model (1.1). The table below provides a taste of what is to come and a quick summary of this course:\n\n\n\n\n\n\n\n\n\nModel Class\nWhat We Pay\nWhat We Get\nUpside\n\n\n\n\nLinear Heterogeneous Models\nLinearity of \\(\\phi\\): \\(\\phi(X_{it}, A_i, U_{it})\\) \\(=\\) \\(X_{it}'\\beta(A_i)\\) \\(+ U_{it}\\) and sufficient number of observations per unit\nDistribution and moments of individual treatment effects\nFull distribution of treatment effects\n\n\nNonparametric Models\nIdentification only for subpopulation of stayers or assumptions on the relationship between \\((X_{it})\\) and \\((A_i, U_{it})\\)\nAverage marginal effects (more with additive separability of \\(U_{it})\\)\nNo restrictions on the form of \\(\\phi(\\cdot)\\) or \\(A_i\\)\n\n\nQuantile and Distributional Regression Models\nFocus on distribution of potential outcomes, not treatment effects.\nQuantile and distributional treatment effects (QTEs and DTEs)\nNo need for panel data\n\n\n\n\n\nNext Section\nIn the next section, we begin by examining how unobserved heterogeneity arises naturally in linear models and setting the stage for the first block of the course.\n\n\n\n\nStefanski, Leonard A., and Raymond J. Carroll. 1985. “Covariate Measurement Error in Logistic Regression.” The Annals of Statistics 13 (4): 1335–51. https://doi.org/10.1214/aos/1176349741.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction: Causal Inference with Unobserved Heterogeneity</span>"
    ]
  },
  {
    "objectID": "linear/linear-introduction.html",
    "href": "linear/linear-introduction.html",
    "title": "2  Intro: Linear Models with Heterogeneous Coefficients",
    "section": "",
    "text": "2.1 Linearity and Heterogeneity",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro: Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-introduction.html#linearity-and-heterogeneity",
    "href": "linear/linear-introduction.html#linearity-and-heterogeneity",
    "title": "2  Intro: Linear Models with Heterogeneous Coefficients",
    "section": "",
    "text": "2.1.1 Models with Homogeneous Slopes\nWe begin our journey where standard textbooks and first-year foundational courses in econometrics leave off. The “standard” linear models considered in such courses often assume homogeneity in individual responses to covariates (e.g., Hansen (2022)). A common cross-sectional specification of a linear potential outcomes model takes the form\n\\[\nY^{\\bx}_i = \\bbeta'\\bx + U_{i},\n\\tag{2.1}\\] where \\(i=1, \\dots, N\\) indexes cross-sectional units, \\(\\bx\\) is some potential treatment value (possibly vector-valued), and \\(Y^{\\bx}_i\\) is the potential outcome of unit \\(i\\) under \\(\\bx\\).\nIn panel data, models often include unit-specific \\((i)\\) and time-specific \\((t)\\) intercepts while maintaining a common slope vector \\(\\bbeta\\):\n\\[\nY_{it}^{\\bx} = A_i + \\Gamma_t +  \\bbeta'\\bx + U_{it},\n\\tag{2.2}\\] where \\(A_i\\) and \\(\\Gamma_t\\) are not observed.\n\n\n2.1.2 Heterogeneity in Slopes. Examples\nHowever, modern economic theory rarely supports the assumption of homogeneous slopes \\(\\bbeta\\). Theoretical models recognize that observationally identical individuals, firms, and countries can respond differently to the same stimulus. In a linear model, this requires us to consider more flexible models with heterogeneous coefficients:\n\nCross-sectional model (2.1) generalizes to\n\\[\nY_i^{\\bx} = \\bbeta_{i}'\\bx + U_i.\n\\tag{2.3}\\]\nPanel data model (2.2) generalizes to\n\\[\nY_{it}^{\\bx}  = \\bbeta_{it}'\\bx + U_{it}.\n\\tag{2.4}\\]\n\nSuch models are worth studying, as they naturally arise in a variety of contexts:\n\nStructural models with parametric restrictions: Certain parametric restrictions yield linear relationships in coefficients. An example is given by firm-level Cobb-Douglas production functions where firm-specific productivity differences induce heterogeneous coefficients (Combes et al. (2012); Sury (2011)).\nBinary covariates and interaction terms: if all covariates are binary and all interactions are included, a linear model encodes all treatment effects without loss of generality (e.g., Wooldridge (2005)).\nLog-linearized models: Nonlinear models may be approximated by linear models around a steady-state. For example, Heckman and Vytlacil (1998) demonstrate how the nonlinear Card (2001) education model simplifies to a heterogeneous linear specification after linearization.\n\n\n\n\n\n\n\nIn this block, we use notation \\(\\bbeta_i\\) and \\(\\bbeta_{it}\\) for the heterogeneous coefficients. This convention is in line with the well-established literature and models (2.1-2.2). However, note that this notation can be connected to the general model in Equation 1.1 as \\[\nY_i^{\\bx} = \\phi(\\bx, A_i) = \\beta(A_i)'\\bx + U_i,\n\\] where \\(U_i\\) is a component of the general vector \\(A_i\\) of unobserved components and \\(\\beta(\\cdot)\\) is some vector-valued function.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro: Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-introduction.html#what-do-we-care-about-identification",
    "href": "linear/linear-introduction.html#what-do-we-care-about-identification",
    "title": "2  Intro: Linear Models with Heterogeneous Coefficients",
    "section": "2.2 What Do We Care About? Identification",
    "text": "2.2 What Do We Care About? Identification\n\n2.2.1 Parameters of Interest\nThe parameters of interest in models (2.1) and (2.2) are straightforward. The common slope \\(\\bbeta\\) simultaneously plays the role of both the average treatment effect and all the individual treatment effects. Estimating \\(\\bbeta\\) is enough for policy analysis.\nThe situation is more complicated for the more general models (2.3) and (2.4). Consider model (2.3). Parameters of interest now include:\n\nIndividual effects: the coefficient vector \\(\\bbeta_i\\) for specific units.\nMoments of the distribution: the average coefficient vector (\\(\\E[\\bbeta_i]\\)), variance \\(\\var(\\bbeta_i)\\), and higher-order moments.\nDistributional properties: The full distribution of \\(\\bbeta_i\\) or its quantiles, or just the tail behavior of the distribution.\n\nSimilar objects are relevant for the panel model in Equation 2.4.\n\n\n2.2.2 Regarding Identification\nUnfortunately, greater flexibility in terms of parameters also leads to greater challenges in terms of identification. Models (2.3) and (2.4) are too general to permit identification of the above parameters without further assumptions. This failure of identification is driven by the combination of the following two issues:\n\nLimited observations per coefficient vector. Since each unit \\(i\\) (or pair \\((i,t)\\)) provides only indirect information through the realized \\(\\bbeta_i'\\bX_i\\) (or \\(\\bbeta_{it}'\\bX_{it}\\)), there is effectively less than one observation per \\(\\bbeta_i\\).\nUnrestricted dependence between coefficients and covariates. Without assumptions on the relationship between \\(\\bbeta_i\\) and \\(\\bX_i\\), one can generally not separate the influence of the two components on linear indices of the form \\(\\bbeta_i'\\bX_i\\).\n\nIdentification is typically achieved by mitigating one of these challenges. Common strategies to address these challenges include:\n\nIncreasing the effective number of observations per coefficient vector by restricting coefficient variation.\n\nIn panel settings, assuming time-invariant coefficients simplifies Equation 2.4 to:\n\\[\nY_{it}^{\\bx} = \\bbeta_i'\\bx + U_{it}.\n\\tag{2.5}\\]\nAlternative approaches assume a finite number of latent groups, each with its own coefficient vector, yielding the grouped structure:\n\\[\nY_{it}^{\\bx} = \\bbeta_{g_i, t}'\\bx + U_{it}.\n\\tag{2.6}\\]\nThis model in discussed in Bonhomme and Manresa (2015), Bester and Hansen (2016) (see also Bonhomme, Lamadon, and Manresa (2022)).\n\nRestricting dependence between \\(\\bbeta_i\\) and the realized treatment \\(\\bX_i\\). For example, there is a strand of literature that assumes that \\(\\bbeta_i\\) and \\(\\bX_i\\) are independent (Beran, Feuerverger, and Hall 1996; Hoderlein, Klemelä, and Mammen 2010).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro: Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-introduction.html#model-of-this-block",
    "href": "linear/linear-introduction.html#model-of-this-block",
    "title": "2  Intro: Linear Models with Heterogeneous Coefficients",
    "section": "2.3 Model of This Block",
    "text": "2.3 Model of This Block\nThis block primarily focuses on the first strategy. Specifically, we will consider a version of model (2.4) with time-invariant heterogeneous coefficients:\n\\[\nY_{it}^{\\bx} = \\bbeta_i'\\bx + U_{it}.\n\\tag{2.7}\\]\nThe time-invariant coefficients \\(\\bbeta_i\\) play the role of \\(A_i\\) in Equation 1.1.\nThe observed realized values \\((Y_i, \\bX_{it})\\) satisfy \\[\nY_{it}= Y_{ii}^{\\bX_{it}} = \\bbeta_i'\\bX_{it} + U_{it}.\n\\]\nWe impose no restrictions on the dependence between \\(\\bbeta_i\\) and \\(\\bX_{it}\\). As noted in the introduction, it is important to allow for such dependence outside of experimental data — economic agents can select their covariates \\(\\bX_{it}\\) based on knowledge of their own \\(\\bbeta_i\\). Since parametrizing this dependence is non-trivial, we impose no assumptions on it.\nWe will also generally focus on the case where the number \\(N\\) of units is large, while the number \\(T\\) of observations per unit is fixed and not necessarily large.\n\n\n\n\n\n\nIn the panel data literature, approaches that do not restrict the dependence between the unobserved and the observed components are called “fixed effects”.\n\n\n\nNote that model (2.7) includes a particular special case — the random intercept model (confusingly also called the “fixed effects model”). The random intercept model imposes homogeneity on all parameters except the intercept term. In the one-way case, the model takes the form:\n\\[\nY_{it}^{\\bx} = A_i + \\bbeta'\\bx + U_{it}.\n\\tag{2.8}\\] Model (2.8) is one of the oldest ways of including unobserved heterogeneity in linear models and goes back at least to Mundlak (1961).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro: Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-introduction.html#plan-for-this-block",
    "href": "linear/linear-introduction.html#plan-for-this-block",
    "title": "2  Intro: Linear Models with Heterogeneous Coefficients",
    "section": "2.4 Plan for This Block",
    "text": "2.4 Plan for This Block\nIn this block, we will focus on model (2.7) and consider identification of the above parameters of interest. Specifically,\n\nAverage coefficient vector \\(\\E[\\bbeta_i]\\):\n\nFirst, we demonstrate that standard estimators for the random intercept model (2.8) are generally inconsistent for \\(\\E[\\bbeta_i]\\) in the more general model (2.7).\nNext, we introduce a mean group estimator robust to heterogeneity and dynamics.\n\nVariance \\(\\var(\\bbeta_i)\\): we show how one can identify and estimate \\(\\var(\\bbeta_i)\\) by imposing structure on the temporal dependence in the residuals \\(U_{it}\\).\nIdentifying the Full Distribution of \\(\\bbeta_i\\): we show how one can obtain the distribution of \\(\\bbeta_i\\) with a deconvolution argument.\n\nKnowing these features of the distribution of \\(\\bbeta_i\\) allows one to compute the corresponding features of the treatment effects of changing from some treatment value \\(\\bx_1\\) to \\(\\bx_2\\) — these treatment effects are given by \\(\\bbeta_1(\\bx_2-\\bx_1)\\).\n\n\nNext Section\nNext, we show that the within (fixed effects) estimator recovers \\(\\E[\\bbeta_i]\\) only under restrictive assumptions.\n\n\n\n\nBeran, Rudolf, Andrey Feuerverger, and Peter Hall. 1996. “On Nonparametric Estimation of Intercept and Slope Distributions in Random Coefficient Regression.” The Annals of Statistics 24 (6): 2569–92. https://doi.org/10.1214/aos/1032181170.\n\n\nBester, C Alan, and Christian B Hansen. 2016. “Grouped Effects Estimators in Fixed Effects Models.” Journal of Econometrics 190 (1): 197–208. https://doi.org/10.1016/j.jeconom.2012.08.022.\n\n\nBonhomme, Stéphane, Thibaut Lamadon, and Elena Manresa. 2022. “Discretizing Unobserved Heterogeneity.” Econometrica 90 (2): 625–43. https://doi.org/10.3982/ECTA15238.\n\n\nBonhomme, Stéphane, and Elena Manresa. 2015. “Grouped Patterns of Heterogeneity in Panel Data.” Econometrica 83 (3): 1147–84. https://doi.org/10.3982/ecta11319.\n\n\nCard, David. 2001. “Estimating the Return to Schooling: Progress on Some Persistent Econometric Problems.” Econometrica 69 (5): 1127–60. https://doi.org/10.1111/1468-0262.00237.\n\n\nCombes, Pierre Philippe, Gilles Duranton, Laurent Gobillon, Diego Puga, and Sébastien Roux. 2012. “The Productivity Advantages of Large Cities: Distinguishing Agglomeration From Firm Selection.” Econometrica 80 (6): 2543–94. https://doi.org/10.3982/ecta8442.\n\n\nHansen, Bruce. 2022. Econometrics. Princeton University Press.\n\n\nHeckman, James, and Edward Vytlacil. 1998. “Instrumental variables methods for the correlated random coefficient model.” Journal of Human Resources 33 (4): 974–87.\n\n\nHoderlein, Stefan, Jussi Klemelä, and Enno Mammen. 2010. “Analyzing the Random Coefficient Model Nonparametrically.” Econometric Theory 26 (03): 804–37. https://doi.org/10.1017/S0266466609990119.\n\n\nMundlak, Yair. 1961. “Empirical Production Function Free of Management Bias.” Journal of Farm Economics 43 (1): 44. https://doi.org/10.2307/1235460.\n\n\nSury, Tavneet. 2011. “Selection and Comparative Advantage in Technology Adoption.” Econometrica 79 (1): 159–209. https://doi.org/10.3982/ecta7749.\n\n\nWooldridge, Jeffrey M. 2005. “Fixed-effects and related estimators for correlated random-coefficient and treatment-effect panel data models.” The Review of Economics and Statistics 87 (May): 385–90.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Intro: Linear Models with Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-within-estimator.html",
    "href": "linear/linear-within-estimator.html",
    "title": "3  Within (Fixed Effects) Estimator and Heterogeneous Coefficients",
    "section": "",
    "text": "3.1 Introduction\nAs noted in the previous section, in this block we focus our attention on the linear panel data model with unit-specific heterogeneous coefficients (Equation 2.7): \\[\nY_{it}^{\\bx} = \\bbeta_i'\\bx + U_{it}.\n\\tag{3.1}\\]\nOur first key parameter of interest is the average coefficient vector \\(\\E[\\bbeta_i]\\)— the linear model analog to the average treatment effect.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Within (Fixed Effects) Estimator and Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-within-estimator.html#introduction",
    "href": "linear/linear-within-estimator.html#introduction",
    "title": "3  Within (Fixed Effects) Estimator and Heterogeneous Coefficients",
    "section": "",
    "text": "3.1.1 Focus: Workhorse Estimators under Heterogeneity\nCan existing workhorse estimators for linear panel data models — the within (fixed effects) and dynamic panel IV estimators — correctly estimate \\(\\E[\\bbeta_i]\\)? Of course, those methods were developed in the context of the simpler random intercept model (2.8) \\[\nY_{it}^{\\bx} = A_i + \\bbeta'\\bx + U_{it}.\n\\tag{3.2}\\] However, if they can also handle the more general Equation 3.1, then all the better for us — we do not need to develop any new methods.\nUnfortunately, such standard estimators usually fail when coefficients vary across units, as we demonstrate in this and the next few sections. This failure holds both for static and the dynamic formulations of Equation 3.1.\n\n\n3.1.2 This Section: Static Model and Fixed \\(T\\)\nIn this section, we consider the static case and the associated workhorse estimator — the within (fixed effects) estimator. By “static”, we mean that the model does not include lagged dependent variables as regressors. In addition, we assume strict exogeneity \\[\n\\E[\\bU_i|\\bX_i]=0,\n\\] where \\(\\bU_i = (U_{i1}, \\dots, U_{it})\\) and \\(\\bX_i = (\\bX_{i1}', \\dots, \\bX_{iT}')'\\).\nThroughout, we assume that the number \\(N\\) of cross-sectional units is potentially large, while the number \\(T\\) of data points per unit is fixed. This setup is more reflective of typical micropanels.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Within (Fixed Effects) Estimator and Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-within-estimator.html#recall-within-estimator-in-the-random-intercept-model",
    "href": "linear/linear-within-estimator.html#recall-within-estimator-in-the-random-intercept-model",
    "title": "3  Within (Fixed Effects) Estimator and Heterogeneous Coefficients",
    "section": "3.2 Recall: Within Estimator in the Random Intercept Model",
    "text": "3.2 Recall: Within Estimator in the Random Intercept Model\nTo begin, we briefly go through the construction and the properties of the within estimator under the random intercept model (3.2).\n\n3.2.1 Construction\nWithin (“fixed effects”) estimation of the random intercept model has two steps:\n\nApply the within transformation to each unit.\nApply OLS to the resulting pooled data.\n\n\n3.2.1.1 Step 1: Within Transformation\nTo perform the within transformation, we first average the equations for unit \\(i\\) across \\(t\\). Label the average of \\(Y_{it}\\) across \\(t\\) for unit \\(i\\) as\n\\[\nY_{i\\cdot} = \\frac{1}{T} \\sum_{t=1}^{T} Y_{it}.\n\\]\nThe averaged realized outcome \\(Y_{i\\cdot}\\) satisfies the averaged equation\n\\[\nY_{i\\cdot} = A_i + \\bbeta' \\bX_{i\\cdot} + U_{i\\cdot},\n\\] where the averaged variables \\(\\bX_{i\\cdot}\\) and \\(U_{i\\cdot}\\) are defined analogously to \\(Y_{i\\cdot}\\).\nDefine the within-transformed variables by subtracting this averaged equation from the original equation for each \\(t\\): \\[\n\\tilde{Y}_{it} = Y_{it} - Y_{i\\cdot}.\n\\]\nIf Equation 3.2 is true, the realized within transformed variables follow the within-transformed equation:\n\\[\n\\tilde{Y}_{it} = \\bbeta' \\tilde{\\bX}_{it} + \\tilde{Y}_{it}.\n\\tag{3.3}\\]\nUnder Equation 3.2, the within transformation eliminates the individual random intercepts \\(A_i\\). Equation 3.3 now looks like a regular homogeneous regression.\n\n\n3.2.1.2 Step 2: OLS on the Within-Transformed Equation\nThe within (fixed effects) estimator is obtained by simply pooling the data across \\(i\\) and \\(t\\) in Equation 3.3 and applying OLS to it. Specifically, the estimator is given by\n\\[\n\\hat{\\bbeta}^W = \\left(\\sum_{i=1}^{N} \\tilde{\\bX}_i' \\tilde{\\bX}_i \\right)^{-1} \\sum_{i=1}^{N} \\tilde{\\bX}_i \\tilde{\\bY}_i,\n\\tag{3.4}\\] where \\(\\bY_i = (Y_{i1}, \\dots, Y_{iT})\\).\n\n\n\n3.2.2 Properties\nThe within estimator enjoys several desirable properties if the underlying causal model is actually given by the random intercept model (3.2). Most of these properties can be derived from its sampling error representation \\[\n\\hat{\\bbeta}^W = \\bbeta + \\left(\\sum_{i=1}^{N} \\tilde{\\bX}_i' \\tilde{\\bX}_i \\right)^{-1} \\sum_{i=1}^{N} \\tilde{\\bX}_i \\tilde{\\bU}_i.\n\\tag{3.5}\\]\n\n\\(\\hat{\\bbeta}^W\\) is unbiased for \\(\\bbeta\\). To show this, it is sufficient to notice that strict exogeneity of \\(\\bU_i\\) with respect to \\(\\bX_i\\) implies strict exogeneity of \\(\\tilde{\\bU}_i\\) with respect to \\(\\tilde{\\bX}_i\\): \\[\n\\E[\\tilde{\\bU}_i|\\tilde{\\bX}_i] =\n\\E[\\E[\\tilde{\\bU}_i|\\bX_i]|\\tilde{\\bX}_i] = 0.\n\\] It follows that the mean of the second term in Equation 3.5 is 0, and so \\[\n\\E[\\hat{\\bbeta}^W] = \\bbeta.\n\\]\n\\(\\hat{\\bbeta}^W\\) is consistent for \\(\\bbeta\\) and asymptotically normal, provided a standard rank condition holds for \\(\\tilde{\\bX}_i\\): \\[\n\\hat{\\bbeta}^W \\xrightarrow{p} \\bbeta, \\quad \\sqrt{N}(\\hat{\\bbeta}^W - \\bbeta) \\Rightarrow N(0, \\Sigma).  \n\\tag{3.6}\\]\n\nSince \\(\\bbeta\\) is the average coefficient vector in this homogeneous model, we conclude that the within estimator consistently estimates average coefficients under the random intercept model (3.2).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Within (Fixed Effects) Estimator and Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-within-estimator.html#adding-heterogeneous-coefficients",
    "href": "linear/linear-within-estimator.html#adding-heterogeneous-coefficients",
    "title": "3  Within (Fixed Effects) Estimator and Heterogeneous Coefficients",
    "section": "3.3 Adding Heterogeneous Coefficients",
    "text": "3.3 Adding Heterogeneous Coefficients\nHowever, there is usually no theoretical reason for the slopes \\(\\bbeta\\) to be homogeneous. An assumption of slope homogeneity goes against acknowledging heterogeneity and including the random intercept terms \\(A_i\\) in the first place. It is rather more realistic to consider the more general Equation 3.1. Accordingly, we now turn to studying the properties of \\(\\hat{\\bbeta}^W\\) in this more realistic setting.\n\n3.3.1 Sampling Error Form\nApplying the within transformation to the heterogeneous Equation 3.1 yields another version of the within-transformed equation:\n\\[\n\\tilde{Y}_{it} = \\bbeta_i' \\tilde{\\bX}_{it} + \\tilde{U}_{it}.\n\\]\nNote that now the individual heterogeneity is not eliminated! The heterogeneous coefficients \\(\\bbeta_i\\) remain in the equation.\nThe within estimator on the above equation may then be represented as \\[\n\\begin{aligned}\n\\hat{\\bbeta}^W  & = \\left(\\sum_{i=1}^N \\tilde{\\bX}_i'\\tilde{\\bX}_i \\right)^{-1} \\sum_{i=1}^N \\tilde{\\bX}_i \\tilde{\\bY}_i \\\\\n& = \\left(\\sum_{i=1}^N \\tilde{\\bX}_i'\\tilde{\\bX}_i \\right)^{-1} \\sum_{i=1}^N \\tilde{\\bX}_i \\tilde{\\bX}_i\\bbeta_i + \\left(\\sum_{i=1}^N \\tilde{\\bX}_i'\\tilde{\\bX}_i \\right)^{-1} \\sum_{i=1}^N \\tilde{\\bX}_i \\tilde{\\bU}_i.\n\\end{aligned}\n\\] Note the difference of the sampling error representation with Equation 3.5. The first term is now a weighted average of the individual coefficients. The weights themselves depend on the second moments of the within-transformed explanatory variables, which may be viewed as a kind of variance weighting for units.\nDoes the within estimator target \\(\\E[\\bbeta_i]\\)? To proceed, we decompose \\(\\bbeta_i\\) into a common mean component \\(\\E[\\bbeta_i]\\) and an idiosyncratic deviation \\(\\bEta_i\\): \\[\n\\bbeta_i = \\E[\\bbeta_i] + \\bEta_i\n\\]\nWith this representation, we can further analyze the within estimator as \\[\n\\begin{aligned}\n    \\hat{\\bbeta} & = \\E[\\bbeta_i]  + \\left( \\dfrac{1}{N}\\sum_{i=1}^N \\tilde{\\bX}_i'\\tilde{\\bX}_i \\right)^{-1}  \\dfrac{1}{N}\\sum_{i=1}^N \\tilde{\\bX}_i \\tilde{\\bX}_i\\bEta_i \\\\\n    &  \\phantom{ = \\E[\\bbeta_i] } \\hspace{1.3mm} + \\left( \\dfrac{1}{N}\\sum_{i=1}^N  \\tilde{\\bX}_i'\\tilde{\\bX}_i \\right)^{-1} \\dfrac{1}{N}\\sum_{i=1}^N \\tilde{\\bX}_i \\tilde{\\bU}_i\\\\\n    & \\xrightarrow{p} \\E[\\bbeta_i]  + \\left(\\E\\left[\\tilde{\\bX}_i'\\tilde{\\bX}_i \\right] \\right)^{-1} \\E\\left[\\tilde{\\bX}_i'\\tilde{\\bX}_i\\bEta_i \\right]  \\\\\n  &    \\phantom{ \\xrightarrow{p} \\E[\\bbeta_i] } \\hspace{1.3mm} + \\left(\\E\\left[\\tilde{\\bX}_i'\\tilde{\\bX}_i \\right] \\right)^{-1} \\E\\left[\\tilde{\\bX}_i'\\tilde{\\bU}_i \\right]\\\\\n    & = \\E[\\bbeta_i] + \\left(\\E\\left[\\tilde{\\bX}_i'\\tilde{\\bX}_i \\right] \\right)^{-1} \\E\\left[\\tilde{\\bX}_i'\\tilde{\\bX}_i\\bEta_i \\right],\n\\end{aligned}\n\\] where we have assumed that a suitable law of large numbers applies as \\(N\\to\\infty\\) and \\(T\\) is fixed, and where \\(\\E\\left[\\tilde{\\bX}_i'\\tilde{\\bU}_i \\right]=0\\) as above.\n\n\n3.3.2 Conditions for Estimating Average Coefficients\nThe above representation shows that the within estimator is not estimating \\(\\E[\\bbeta_i]\\) unless the following orthogonality condition holds: \\[\n    \\E\\left[\\tilde{\\bX}_i'\\tilde{\\bX}_i\\bEta_i \\right] =0.\n\\tag{3.7}\\] Even though \\(\\E[\\bEta_i]=0\\), the above condition does not necessarily hold if \\(\\bbeta_i\\) and \\(\\bX_i\\) are allowed to dependent.\nIf condition (3.7) holds, the within estimator is consistent for \\(\\E[\\bbeta_i]\\) in the heterogeneous coefficient model (3.1). If it fails, the estimator is biased. The difference between the estimand and \\(\\E[\\bbeta_i]\\) is known as heterogeneity bias (see Campello, Galvao, and Juhl 2019 in the linear case).\nThis orthogonality condition (3.7) is a bit complicated to understand. A simpler sufficient condition is a mean independence on the coefficients given the within transformed covariates: \\[\n\\E[\\boldsymbol{\\eta}_i|\\tilde{\\bX}_i] = 0.  \n\\tag{3.8}\\] Under this condition \\(\\E[\\tilde{\\bX}_i'\\tilde{\\bX}_i\\bEta_i] =0\\), and thus the within estimator is consistent for \\(\\E[\\bbeta_i]\\). These conditions were proposed by Wooldridge (2003), Wooldridge (2005) (see all Murtazashvili and Wooldridge (2008) for the IV within estimator case).\n\n\n\n\n\n\nConditions (3.7) and (3.8) restrict the dependence structure between \\(\\bbeta_i\\) and \\(\\bX_{it}\\). Such conditions are sometimes called correlated random effects (CRE) in the literature. CRE assumptions lie between fixed effects (FE) frameworks — which do not restrict the dependence — and random effects (RE) — which assume that unobserved components are independent of the observed ones.\n\n\n\n\n\n3.3.3 Intuition\nHow can we interpret condition (3.8)? Intuitively, it requires that the changes in \\(\\bX_{it}\\) over time are uncorrelated with the individual coefficients.\nTo see this interpretation, it is helpful to think of the following example framework. Suppose that \\(\\bX_{it}\\) is stationary, that is, its distribution does not depend on \\(t\\). Decompose \\(\\bX_{it}\\) as \\[\n\\bX_{it} = \\E[\\bX_{it}|\\bbeta_i]  + \\bxi_{it},\n\\]\nThen the within transformed variables satisfy: \\[\n    \\tilde{\\bX}_{it} = \\tilde{\\bxi}_{it}.\n\\]\nThe “systemic” component \\(\\E[\\bX_{it}|\\bbeta_i]\\) is not present in \\(\\tilde{\\bX}_{it}\\). Only the deviations across time \\(\\tilde{\\bxi}_{it}\\) are left. Condition (3.8) requires that \\(\\tilde{\\bxi}_{it}\\) and \\(\\bEta_i\\) are unrelated on average. At the same time, it permits an arbitrary relationship between \\(\\bbeta_i\\) and \\(\\E[\\bX_{it}|\\bbeta_i]\\).\nAs an example, suppose that we are working with consumption data. A consumer knows their marginal utilities \\(\\bbeta_i\\) of consuming more of a variety of products. With this knowledge, they choose the optimal desired level of consumption — \\(\\E[\\bX_{it}|\\bbeta_i]\\). When they try to buy this level of products, they may encounter some frictions \\(\\bxi_{it}\\) which cause them to deviate from \\(\\E[\\bX_{it}|\\bbeta_i]\\) — in rough words, the supermarket might not have their favorite cereal. If these “frictions” are uncorrelated with \\(\\bbeta_i\\), then the required consistency holds. In the consumption example, this fact means that unpredictable deviation in short-term choices do not necessarily bias the estimation of overall preferences.\n\n\n3.3.4 Why Panel Data is Useful\nIt is useful to contrast condition (3.8) with the stronger condition that \\[\n\\E[\\bEta_i|\\bX_i]=0.\n\\tag{3.9}\\] Note the difference in the conditioning sets!\nCondition (3.9) is stronger than (3.8) by the tower property of conditional expectation. Intuitively, we can compute \\(\\tilde{\\bX}_i\\) from \\(\\bX_i\\), but not vice versa. The requirement that \\(\\E[\\bEta_i|\\bX_i]=0\\) may be very strong, since it would in general require that \\(\\E[\\bX_{it}|\\bbeta_i]\\) does not depend on \\(\\bbeta_i\\) — we rule out a systemic dependence even on average.\nThis contrast also highlights the advantages of panel data. If you want to consistently estimate \\(\\E[\\bbeta_i]\\) using OLS and cross-sectional data, you need the strong condition (3.9) or at least that \\(\\E[\\bX_i\\bX_i'\\bbeta_i]=0\\). With panel data, weaker conditions are sufficient, and systemic dependence between \\(\\bbeta_i\\) and \\(\\bX_{it}\\) is possible.\n\n\n3.3.5 Example\nWe conclude this section with a small illustration of the results in a tractable model (see this blog post for a simulation with some dramatic examples). Specifically, we consider the following panel model with two periods, coefficient heterogeneity, and a scalar regressor: \\[\nY_{it} = A_i + \\beta_i X_{it} + U_{it}, \\quad t=1,2.\n\\]\nwhere \\(\\bX_{it}\\) and \\(\\bbeta_i\\) are jointly normal:\n\\[\n\\begin{aligned}\n    \\begin{pmatrix}\n      X_{i1}\\\\\n      X_{i2}\\\\\n      \\beta_i\n    \\end{pmatrix} \\sim N\\left(  \\begin{pmatrix}\n    1\\\\\n    2\\\\\n    0.5\n    \\end{pmatrix}, \\begin{pmatrix}\n    1 & \\rho_{1, \\beta}\\rho_{2, \\beta} & \\rho_{1, \\beta}\\\\\n    \\rho_{1, \\beta}\\rho_{2, \\beta} & 1 & \\rho_{2, \\beta}\\\\\n    \\rho_{1, \\beta} & \\rho_{2, \\beta} & 1\n    \\end{pmatrix}   \\right),\n\\end{aligned}\n\\tag{3.10}\\] where the various \\(\\rho\\) parameters are correlations. The setting is such that \\(X_{i1}\\) and \\(X_{i2}\\) are only correlated through \\(\\beta_i\\). The distribution of the \\(A_i\\) does not have to be specified, as it is not involved in the consistency conditions. Likewise, we only need to assume that \\(\\E[U_{it}|X_{i1}, X_{i2}]=0\\) without further specifying the distribution of \\(U_{it}\\).\nThe within transformations yields the following equation: \\[\n                Y_{i2} - Y_{i1} = \\beta_i(X_{i2}-X_{i1}) + (U_{i2}-U_{i1})\n\\] The mean independence condition 3.8 takes form \\[\n    \\E[ \\beta_i|X_{i2} - X_{i1}]  = 0.5.\n\\]\nIt is not difficult to work out (Brockwell and Davis 2016, A.3.1) that \\[\n\\E[ \\beta_i|X_{i2} - X_{i1}]  = 0.5 + (\\rho_{2, \\beta}- \\rho_{1, \\beta})(X_{i2} - X_{i1} - 1)\n\\] Thus, \\(\\E[ \\beta_i|X_{i2} - X_{i1}]=0.5\\) only holds if \\[\n\\rho_{2, \\beta} = \\rho_{1, \\beta}.\n\\tag{3.11}\\] In this case, \\(X_{it}\\) becomes stationary, and does not have dynamics that depend on \\(\\beta_i\\) in the mean.\nIf condition (3.11) holds, the within estimator is consistent for \\(\\E[\\beta_i]\\). It is also possible to show that if condition (3.11) fails, so does the more general Equation 3.7, and the within estimator is inconsistent for \\(\\E[\\beta_i]\\). We represent this fact on Figure 3.1, where we fix \\(\\rho_{1, beta}=0.25\\), and vary \\(\\rho_{2, \\beta}\\) between \\(-1\\) and \\(1\\). Observe that the within estimator is consistent if \\(\\rho_{2, \\beta} = \\rho_{1, \\beta} = 0.25\\). Otherwise, it is biased, potentially severely. For \\(\\rho_{2, \\beta}\\leq -0.6\\), the estimand of the within estimator has a sign different from that of \\(\\E[\\beta_i]\\).\n\n\n\n\n\n\nFigure 3.1: The within estimator under coefficient heterogeneity. Consistency and inconsistency for the average coefficient under data generating process (3.10)\n\n\n\n\n\nNext Section\nIn the next section, we turn to the dynamic case and briefly introduce the “standard” dynamic panel instrumental variable estimators.\n\n\n\n\nBrockwell, Peter J., and Richard A. Davis. 2016. Introduction to Time Series and Forecasting. Springer Texts in Statistics. Springer International Publishing.\n\n\nCampello, Murillo, Antonio F. Galvao, and Ted Juhl. 2019. “Testing for Slope Heterogeneity Bias in Panel Data Models.” Journal of Business and Economic Statistics 37 (4): 749–60. https://doi.org/10.1080/07350015.2017.1421545.\n\n\nMurtazashvili, Irina, and Jeffrey M. Wooldridge. 2008. “Fixed effects instrumental variables estimation in correlated random coefficient panel data models.” Journal of Econometrics 142 (1): 539–52. https://doi.org/10.1016/j.jeconom.2007.09.001.\n\n\nWooldridge, Jeffrey M. 2003. “Fixed Effects Estimation of the Population-Averaged Slopes in a Panel Data Random Coefficient Model.” Econometric Theory 19: 411–13. https://doi.org/10+10170S0266466603002081.\n\n\n———. 2005. “Fixed-effects and related estimators for correlated random-coefficient and treatment-effect panel data models.” The Review of Economics and Statistics 87 (May): 385–90.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Within (Fixed Effects) Estimator and Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-dynamic-panel-iv.html",
    "href": "linear/linear-dynamic-panel-iv.html",
    "title": "4  Interlude: Standard Dynamic Panel Estimators",
    "section": "",
    "text": "4.1 Exogeneity and Dynamic Models",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interlude: Standard Dynamic Panel Estimators</span>"
    ]
  },
  {
    "objectID": "linear/linear-dynamic-panel-iv.html#exogeneity-and-dynamic-models",
    "href": "linear/linear-dynamic-panel-iv.html#exogeneity-and-dynamic-models",
    "title": "4  Interlude: Standard Dynamic Panel Estimators",
    "section": "",
    "text": "4.1.1 Introduction\nThroughout the previous section, we assumed that the idiosyncratic term \\(U_{it}\\) satisfied strict exogeneity with respect to the data: \\[\n    \\E[U_{it}|\\bX_i] = 0.\n\\tag{4.1}\\]\nHowever, strict exogeneity is not an innocent assumption. In particular, it implies that for all pairs of indices \\(s, t\\) it holds that \\[\n    \\E[U_{it}\\bX_{is}] =0.\n\\tag{4.2}\\]\nFor \\(t&lt;s\\), Equation 4.2 means that past shocks are uncorrelated with future values of \\(\\bX\\). In other words, one cannot predict past shocks from future \\(\\bX\\)s.\nThis requirement might fail if \\(\\bX\\) is dynamic and its evolution is affected by \\(U_{it}\\). In this section, we discuss this challenge and some traditional approaches to dealing with it with short panel data.\n\n\n4.1.2 No Strict Exogeneity for Dynamic Models\nA particularly clear example of failure of (4.2) is the case when \\(\\bX_{it}\\) includes lagged values of the outcome. As we show now, strict exogeneity cannot hold in this case.\nFor simplicity, we momentarily forget about the cross-sectional dimension and consider a simple autoregressive model with 1 lag (AR(1)): \\[\n    Y_t = \\alpha + \\lambda Y_{t-1} + U_{t},\n\\] where \\(t=1, \\dots, T\\) and the innovation \\(U_t\\) satisfies \\(\\E[U_t]=0\\).\nTo show that strict exogeneity fails, it is sufficient to find a single pair of \\((t, s)\\) such that (4.2) is not true. Specifically, we take \\(s=t+1\\) and show that \\(\\E[Y_{it+1}U_t]\\) is not necessarily zero.\nTo evaluate this expectation, we write the model at \\(t+1\\) \\[\n    Y_{t+1} = \\alpha + \\lambda Y_t + U_{t+1}.\n\\tag{4.3}\\] We then substitute the model into \\(\\E[Y_{it+1}U_t]\\) \\[\n\\begin{aligned}\n    \\E[Y_{t+1}U_t] & = \\lambda\\E[Y_t U_t] + \\E[U_{t+1}U_t] \\\\\n    &  = \\lambda^2\\E[Y_{t-1}U_t] + \\lambda\\E[U_t^2] + \\E[U_{t+1}U_t] .\n\\end{aligned}\n\\]\nWhat can we say about the right hand side? In many contexts, we may reasonably assume that\n\nOne cannot predict the shocks of tomorrow from what you have today, so that \\[\\E[Y_{t-1}U_t]=0.\\]\n\\(U_{t}\\) is uncorrelated over time, so that \\[\\E[U_{t+1}U_t]=0.\\]\n\nHowever, \\(\\lambda\\E[U_t^2]\\) is zero only in two unreasonable cases:\n\nIf \\(\\lambda=0\\), and so the model is not actually dynamic\n\\(\\E[U_t^2]=0\\), and so the process is actually fully deterministic.\n\nBoth of these options are unpleasant. The first one goes against the idea of allowing any dynamics. The second is extremely unlikely with any kind of social data. We conclude that in dynamic models it is the case \\[\n\\E[Y_{it+1}U_t] \\neq 0.\n\\tag{4.4}\\] A fortiori, strict exogeneity generally fails in dynamic models.\n\n\n4.1.3 Sequential Exogeneity\nSo what can you do if you want to allow for dynamic models? Since strict exogeneity (4.2) is effectively impossible to satisfy, we need a weaker form of exogeneity.\nOne popular weaker assumption compatible with dynamic models is sequential exogeneity (or predeterminedness). In context of the simple univariate time series model (4.3), sequential exogeneity may be stated as \\[\n        \\E[U_{t}| Y_{t-1}, Y_{t-2}, \\dots] =0.\n\\] Intuitively, sequential exogeneity states that future shocks cannot be predicted using past covariates. However, it does not restrict our ability to predict the past from the future — the context which created problems for strict exogeneity.\nMore generally, consider a panel data linear model with homogeneous coefficients, a random intercept, and a dynamic process for the outcome: \\[\nY_{it} = A_i + \\sum_{k=1}^K \\lambda_k Y_{it-k} + \\bbeta'\\bX_{it} + U_{it}.  \n\\tag{4.5}\\] In this model, a sequential exogeneity assumption takes form \\[\n     \\E[U_{it}|\\curl{Y_{is-1}, \\bX_{is}}_{s\\leq t}] =0.\n\\] Model (4.5) is a dynamic counterpart of the static model (3.2) we discussed in the last section.\n\n\nOne may write down the potential outcomes corresponding to Equation 4.5. However, it is a bit inconvenient, and so we work directly with realized equations from the start.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interlude: Standard Dynamic Panel Estimators</span>"
    ]
  },
  {
    "objectID": "linear/linear-dynamic-panel-iv.html#handling-dynamic-panels-with-a-random-intercept",
    "href": "linear/linear-dynamic-panel-iv.html#handling-dynamic-panels-with-a-random-intercept",
    "title": "4  Interlude: Standard Dynamic Panel Estimators",
    "section": "4.2 Handling Dynamic Panels with a Random Intercept",
    "text": "4.2 Handling Dynamic Panels with a Random Intercept\nBefore moving on and introducing heterogeneous coefficients in dynamic linear models, we discuss model (4.5) in more detail. In particular, we develop the standard workhorse instrumental variable estimators. We analyze how these estimators fare in the more general model (2.7) in the next section.\n\n4.2.1 Model\nFor the sake of simplicity, we discuss a simple version of model (4.5) without extra covariates \\(\\bX_{it}\\) and with only one lag of \\(Y_{it}\\): \\[\nY_{it} = A_i + \\lambda Y_{it-1} + U_{it},\n\\tag{4.6}\\] where \\(U_{it}\\) satisfies sequential exogeneity in the form \\(\\E[U_{it}|Y_{is}, s&lt;t] = 0\\). This random intercept dynamic model has extensively considered in the literature (e.g. Anderson and Hsiao 1982; Arellano and Bond 1991; Blundell and Bond 1998).\nThe key parameter of interest is \\(\\lambda\\). By homogeneity of \\(\\lambda\\), it also plays the role of the average coefficient for model (4.6).\nHere, we primarily focus on the large-\\(N\\), fixed-\\(T\\) case, though model (4.5) has also been extensively studied in the large-(\\(N, T\\)) case (e.g. Alvarez and Arellano 2003).\n\n\n4.2.2 Endogeneity\nTo work towards an estimator for \\(\\lambda\\), let us difference Equation 4.5 across \\(t\\) to eliminate the random intercept \\(\\alpha_i\\). The differenced equation takes form \\[\n\\begin{aligned}\n\\Delta Y_{it} & = \\lambda \\Delta Y_{it-1} + \\Delta U_{it}, \\\\\n\\Delta Y_{it} & = Y_{it} - Y_{it-1}.\n\\end{aligned}\n\\tag{4.7}\\] Equation 4.7 seems like a simple regression equation. It is tempting to just apply OLS and regress \\(\\Delta Y_{it}\\) on \\(\\Delta Y_{it-1}\\).\nIt turns out that this approach fails as there is an endogeneity problem in Equation 4.7! Sequential exogeneity is not enough to guarantee that \\[\n\\E[\\Delta Y_{it-1}\\Delta U_{it}] = 0.\n\\]\nTo see why, we expand \\(\\E[\\Delta Y_{it-1}\\Delta U_{it}]\\) as\n\\[\n\\begin{aligned}\n    & \\E\\left[(U_{it}-U_{it-1})(Y_{it-1}-Y_{it-2}) \\right] \\\\\n    & = \\E[ U_{it}Y_{it-1} ] + \\E[(U_{it-1}-U_{it})Y_{it-2}] - \\E[U_{it-1}Y_{it-1}]  \\\\\n    & = -\\E[U_{it-1} Y_{it-1}].\n\\end{aligned}\n\\] Sequential exogeneity is sufficient to immediately eliminate the first 2 expectations. For the last expectation remaining, we can substitute \\(Y_{it-1}\\) to see that \\[\n\\begin{aligned}\n    \\E[U_{it-1}Y_{it-1}] & = \\lambda\\E[U_{it-1}Y_{it-2}] + \\E[U_{it-1}^2]\\\\\n    & = \\E[U_{it-1}^2]\n\\end{aligned}\n\\] As noted above, in general we expect that \\(\\E[U_{it-1}^2] \\neq 0\\). Hence we conclude that \\[\n    \\E[\\Delta Y_{it-1}\\Delta U_{it}]\\neq 0.\n\\tag{4.8}\\] In other words, \\(\\Delta Y_{it-1}\\) is an endogenous regressor in the differenced Equation 4.7.\n\n\n\n\n\n\nOne should not confuse endogeneity in Equation 4.8 with the failure of strict but not necessarily sequential exogeneity in dynamic models (e.g. Equation 4.4).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interlude: Standard Dynamic Panel Estimators</span>"
    ]
  },
  {
    "objectID": "linear/linear-dynamic-panel-iv.html#iv-estimation-of-dynamic-panel-models",
    "href": "linear/linear-dynamic-panel-iv.html#iv-estimation-of-dynamic-panel-models",
    "title": "4  Interlude: Standard Dynamic Panel Estimators",
    "section": "4.3 IV Estimation of Dynamic Panel Models",
    "text": "4.3 IV Estimation of Dynamic Panel Models\nGiven the endogeneity issue in first-differencing, instrumental variable methods offer a potential solution. We only need to find suitable instruments — variables \\(Z_{it}\\) which satisfy relevance and exogeneity conditions: \\[\n\\begin{aligned}\n    \\E[Z_{it}\\Delta Y_{it-1}] \\neq 0,\\\\\n    \\E[Z_{it}\\Delta U_{it}] = 0.\n\\end{aligned}\n\\]\nIn most contexts, one has to look for external variables that can serve as instruments.\nHowever, model (4.7) is a very special case where one can use instruments internal to the model!\n\n4.3.1 The Anderson-Hsiao Estimator\nIn particular, consider using \\(Y_{it-2}\\) as an instrument.\n\nRelevance seems to be relatively straightforward, as the target endogenous variable \\(\\Delta Y_{it-1}= Y_{it-1}-Y_{it-2}\\) actually has the instrument inside.\nExogeneity can be justified by appealing to sequential exogeneity: \\[ \\E[Y_{it-2}\\Delta U_{it}]  = \\E[Y_{it-2}U_{it} - Y_{it-2}U_{it-1}]=0. \\]\n\nHence \\(Y_{it-2}\\) is a valid instrument!\nThe resulting IV estimator is known as the Anderson and Hsiao (1982) estimator and given by \\[\n\\hat{\\lambda}^{AH} = \\dfrac{ \\sum_{t=2}^T Y_{it-2}\\Delta Y_{it} }{\\sum_{t=2}^T Y_{it-2}\\Delta Y_{it-1} }.\n\\]\n\n\n4.3.2 Further Dynamic Panel IV Estimators\nAre there more instruments that can use for \\(\\Delta Y_{it-1}\\)? Using more instruments induces overidentification and potentially leads to more precise estimators, solving one of the key drawbacks of the Anderson and Hsiao (1982) estimator (Arellano 1989).\nAs a partial answer, suppose we consider \\(Y_{it-k}\\) for \\(k\\geq 2\\). By applying sequential exogeneity again we can show that \\(Y_{it-k}\\) is exogenous: \\[  \n\\E[Y_{it-k}\\Delta U_{it}]  = \\E[Y_{it-k}U_{it} - Y_{it-k}U_{it-1}]=0.\n\\] Provided \\(Y_{it-k}\\) is correlated with \\(\\Delta Y_{it-1}\\) and available in the data, it also becomes a valid instrument. Following this chain of reasoning and using all available lags \\(Y_{it-k}\\) yields the Arellano and Bond (1991) estimator.\nEven further instruments have been found for (4.7) by Ahn and Schmidt (1995) and Blundell and Bond (1998), among others.\n\n\n4.3.3 Properties of Dynamic Panel IV Estimators\nUnder some standard conditions, the above instrumental variable estimators are all consistent and asymptotically normal for \\(\\lambda\\) as \\(N\\to\\infty\\) and \\(T\\) is fixed. The matter is rather more delicate if both \\(N\\) and \\(T\\) are large, and the above dynamic panel IV estimators offer an interesting and natural example of the many moments bias. See Alvarez and Arellano (2003).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interlude: Standard Dynamic Panel Estimators</span>"
    ]
  },
  {
    "objectID": "linear/linear-dynamic-panel-iv.html#extra-endogeneity-and-the-within-estimator",
    "href": "linear/linear-dynamic-panel-iv.html#extra-endogeneity-and-the-within-estimator",
    "title": "4  Interlude: Standard Dynamic Panel Estimators",
    "section": "4.4 Extra: Endogeneity and the Within Estimator",
    "text": "4.4 Extra: Endogeneity and the Within Estimator\nOne may think that we had the endogeneity problem (4.8) because we took first differences instead of applying the within transformation.\nHowever, the same endogeneity issue affects the within estimator. First, for \\(T=2\\) the above discussion applies directly, as the within transformation is numerically identical to first differencing. Second, more generally, the within estimator suffers from its own version of endogeneity. To see the issue, note that the within-transformed model (4.6) can be written as (see Equation 3.3): \\[\n    \\tilde{Y}_{it} = \\lambda \\tilde{Y}_{it-1} + \\tilde{U}_{it}.\n\\]\nOne can now immediately see that \\(\\tilde{Y}_{it-1}\\) is also an endogenous regressor as\n\\[\n    \\E[\\tilde{Y}_{it-1}\\tilde{U}_{it}] = \\E\\left[\\left(Y_{it-1}- T^{-1}\\sum_{s=0}^{T-1} Y_{is} \\right)\\left(U_{it} - T^{-1}\\sum_{r=1}^{T}U_{ir} \\right) \\right].\n\\]\nThe expectation on the right-hand side contains “bad” expectations of the form \\(\\E[U_{it}Y_{is}]\\) with \\(t&lt;s\\) (recall Equation 4.4).\nWe conclude that for any fixed \\(T\\) the within estimator also suffers from endogeneity bias. Nickell (1981) shows that it is of the order \\(O(T^{-1})\\). Furthermore, this bias is likely to be large in practice (see Kiviet 1995).\n\n\nNext Section\nNext, we examine how the above “standard” dynamic panel IV estimators break down when individual coefficients vary across observations.\n\n\n\n\nAhn, Seung Chan, and Peter Schmidt. 1995. “Efficient Estimation of Models for Dynamic Panel Data.” Journal of Econometrics 68 (1): 5–27. https://doi.org/10.1016/0304-4076(94)01641-C.\n\n\nAlvarez, Javier, and Manuel Arellano. 2003. “The Time Series and Cross-Section Asymptotics of Dynamic Panel Data Estimators.” Econometrica 71 (4): 1121–59.\n\n\nAnderson, T. W., and Cheng Hsiao. 1982. “Formulation and estimation of dynamic models using panel data.” Journal of Econometrics 18 (1): 47–82. https://doi.org/10.1016/0304-4076(82)90095-1.\n\n\nArellano, Manuel. 1989. “A note on the Anderson-Hsiao estimator for panel data.” Economics Letters 31 (4): 337–41. https://doi.org/10.1016/0165-1765(89)90025-6.\n\n\nArellano, Manuel, and Stephen Bond. 1991. “Some Tests of Specification for Panel Carlo Application to Data: Monte Carlo Evidence and an Application to Employment Equations.” Review of Economic Studies 58: 277–97.\n\n\nBlundell, Richard, and Stephen Bond. 1998. “Initial Conditions and Moment Restrictions in Dynamic Panel Data Models.” Journal of Econometrics 87: 115–43.\n\n\nKiviet, Jan F. 1995. “On Bias, Inconsistency, and Efficiency of Various Estimators in Dynamic Panel Data Models.” Journal of Econometrics 68 (1): 53–78. https://doi.org/10.1016/0304-4076(94)01643-E.\n\n\nNickell, Stephen. 1981. “Biases In Dynamic Models With Fixed Effects.” Econometrica 49 (6): 1417–26.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interlude: Standard Dynamic Panel Estimators</span>"
    ]
  },
  {
    "objectID": "linear/linear-dynamic-panel-heterogeneity.html",
    "href": "linear/linear-dynamic-panel-heterogeneity.html",
    "title": "5  Heterogeneous Coefficient Dynamic Panels and Instrumental Variable Estimators",
    "section": "",
    "text": "5.1 Introduction and Model\nAs in the static case, it is not clear why slopes should be homogenous in dynamic models. If the coefficients are the same between units, all units have the same dynamics. Such an assumption seems rather unrealistic in most scenarios.\nAs a consequence, we now consider heterogeneous dynamic models. In this section we again focus on a simple AR(1) model with no exogenous regressors. Specifically, we look at the following heterogeneous version of model (4.6): \\[\n\\begin{aligned}\n    Y_{it}   & = A_i +  \\lambda_i Y_{it-1} + Y_{it}, \\\\\n0 & = \\E[Y_{it}| \\curl{Y_{is}}_{s\\leq t}] .\n\\end{aligned}\n\\tag{5.1}\\] This model allows for unit-specific dynamics, as the coefficients \\(\\lambda_i\\) can vary across \\(i\\). The above model is also another instance of the general model (2.7).\nAs before, we are interested in the average coefficient \\(\\E[\\lambda_i]\\).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Heterogeneous Coefficient Dynamic Panels and Instrumental Variable Estimators</span>"
    ]
  },
  {
    "objectID": "linear/linear-dynamic-panel-heterogeneity.html#endogeneity-due-to-heterogeneity",
    "href": "linear/linear-dynamic-panel-heterogeneity.html#endogeneity-due-to-heterogeneity",
    "title": "5  Heterogeneous Coefficient Dynamic Panels and Instrumental Variable Estimators",
    "section": "5.2 Endogeneity due to Heterogeneity",
    "text": "5.2 Endogeneity due to Heterogeneity\nCan the IV estimators of the previous section estimate \\(\\E[\\lambda_i]\\) under model (5.1)? Under what conditions? To answer these questions, recall that the IV estimators of the previous section are based on taking first differences in the model and then using lagged values of \\(Y_{it}\\) as instruments.\n\n5.2.1 Endogeneity in Differenced Heterogeneous Equation\nTo replicate the procedure, we take differences in Equation 5.1 and write the differenced model as \\[\n\\begin{aligned}\n    \\Delta Y_{it} & = \\lambda \\Delta Y_{it-1} +  V_{it}, \\\\\n    V_{it} & = \\eta_i\\Delta Y_{it-1} + \\Delta U_{it} =0,\n\\end{aligned}\n\\tag{5.2}\\] where we label \\[\n\\begin{aligned}\n\\lambda & = \\E[\\lambda_i], \\\\\n\\eta_i & = \\lambda_i - \\lambda.\n\\end{aligned}\n\\]\nIt is easy to check that \\(\\Delta Y_{it-1}\\) is still endogenous because it contains \\(U_{it-1}\\), which is correlated with the residual term. The same problematic term \\(\\lambda \\E[U_{it-1}^2]\\) is present in \\(\\E[V_{it}\\Delta Y_{it-1}]\\).\n\n\n5.2.2 Conditions for Instruments\nCan we find a suitable instrument for \\(\\Delta Y_{it-1}\\)? Any valid instrument must satisfy relevance and exogeneity, which now take the following form: \\[\n\\begin{aligned}\n    \\E[Z_{it}\\Delta Y_{it-1}] &  \\neq 0 ,\\\\\n    \\E[Z_{it} V_{it}] & = \\E[ \\eta_i Z_{it}\\Delta Y_{it-1} ] + \\E[Z_{it}U_{it}].\n\\end{aligned}\n\\] Note that a new \\(\\E[ \\eta_i Z_{it}\\Delta Y_{it-1} ]\\) term now appears in the exogeneity condition.\n\n\n5.2.3 Endogeneity of Lagged Outcomes\nUnlike in the homogeneous case, \\(Y_{it-2}\\) is not a valid instrument anymore. We show that \\(Y_{it-2}\\) is not exogenous in a simple case with the following assumptions:\n\n\\(\\abs{\\lambda_i}&lt;1\\).\nEquation 5.1 can be extended infinitely far into the past by recursive substitution, so that \\[\nY_{it} = \\dfrac{A_i}{1-\\lambda_i} + \\sum_{k=0}^{\\infty} \\lambda_i^k U_{it-k}.\n\\]\n\\(\\curl{U_{it}}_t\\) is an IID sequence with finite second moments, and \\(U_{it}\\) is independent of \\((A_i, \\lambda_i)\\).\n\nUnder the above assumptions the first expectation in \\(\\E[Y_{it-2} V_{it}]\\) can be evaluated as follows: \\[\n\\begin{aligned}\n& \\E[ \\eta_i Y_{it-2}\\Delta Y_{it-1} ]\n\\\\\n% & = \\E\\left[ \\eta_i\\left( \\left(\\sum_{k=0}^{\\infty} \\lambda_i^k u_{i,t-k-1}\\right)\\left(\\sum_{k=0}^{\\infty} \\lambda_i^k u_{i,t-k-2}\\right) - \\left(\\sum_{k=0}^{\\infty} \\lambda_i^k u_{i,t-k-2}\\right)^2 \\right) \\right] \\\\\n& = \\E\\left[ \\eta_i\\left( \\left(\\sum_{k=0}^{\\infty} \\lambda_i^{1+2k} U_{i,t-k-2}^2 \\right)  -  \\left(\\sum_{k=0}^{\\infty} \\lambda_i^{2k} U^2_{i,t-k-2}\\right)  \\right)\\right] \\\\\n& = \\E[U_{it}^2] \\E\\left[\\eta_i (1-\\lambda_i) \\sum_{k=0}^{\\infty} \\lambda_i^{2k}   \\right].\n\\end{aligned}\n\\] In general, there is no reason to expect the above expectation to be zero if \\(\\eta_i\\) is not zero. The second term involves a sum of even-order moments of \\(\\eta_i\\) (equivalently, \\(\\lambda_i)\\); such a sum would in general be non-zero.\nWe conclude that \\[\n\\E[ \\eta_i Y_{it-2}\\Delta Y_{it-1} ]\\neq 0,\n\\] and so in general that \\[\n\\E[Y_{it-2} V_{it}]\\neq 0.\n\\] In other words, \\(Y_{it-2}\\) is not a valid instrument under the heterogeneous model (5.1). The same logic applies under more general assumptions and to higher-order lags of \\(Y_{it}\\).\nMore broadly, the problem of finding a valid instrument \\(Z_{it}\\) for Equation 5.2 appears intractable. The relevance condition requires that \\(Z_{it}\\) be correlated with \\(\\Delta Y_{it-1}\\). As a consequence, it is likely that \\(Z_{it}\\) is also correlated with \\(\\eta_i \\Delta Y_{it-1}\\) (particularly since \\(\\eta_i\\) appears inside \\(\\Delta Y_{it-1}\\)). However, it would then be the case that \\(\\E[ \\eta_i Z_{it}\\Delta Y_{it-1} ]\\neq 0\\), and exogeneity would fail.\nWe conclude that the IV estimators of the previous section do not consistently estimate \\(\\E[\\lambda_i]\\), no matter how we choose the instruments, regardless the magnitude of \\(T\\). Moreover, the results of Pesaran and Smith (1995) imply that the estimators may converge to any value (not exceeding 1), regardless of the underlying true values of \\(\\E[\\lambda_i]\\).\n\n\n5.2.4 Aside: Regarding Restrictions on Coefficients\nThere is a further point of contrast with the static case. In section 3 we noted that OLS can consistently estimate the average coefficients in a static model if the coefficients \\(\\bbeta_i\\) are (mean) independent of the regressors. We also derived a weaker condition for the consistency of the within estimator.\nHowever, no such independence assumptions are possible in a dynamic model. By construction, the coefficients \\((A_i, \\lambda_i)\\) are directly embedded in the process for the regressor \\(Y_{it-1}\\) through \\[\nY_{it-1} = A_i + \\lambda_i Y_{it-2} + U_{it-2}.\n\\] As a result, \\(Y_{it-1}\\) and \\((A_i, \\lambda_i)\\) cannot be (mean) independent by construction.\n\n\nNext Section\nIn the next section, we study a simple estimator for \\(\\E[\\bbeta_i]\\) that is robust both to coefficient heterogeneity and dynamics.\n\n\n\n\nPesaran, M. Hashem, and Ron P. Smith. 1995. “Estimating long-run relationships from dynamic heterogeneous panels.” Journal of Econometrics 6061: 473–77.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Heterogeneous Coefficient Dynamic Panels and Instrumental Variable Estimators</span>"
    ]
  },
  {
    "objectID": "linear/linear-mean-group.html",
    "href": "linear/linear-mean-group.html",
    "title": "6  Mean Group: Robust Estimator for Average Coefficients",
    "section": "",
    "text": "6.1 Mean Group Estimation\nThe results of the previous section and section 3 leave us with key two questions:\nAs it turns out, there is indeed an estimator that is robust both to heterogeneity and dynamics. The mean group (MG) estimator, due to M. H. Pesaran and Smith (1995), permits estimating average coefficients in heterogeneous panels, regardless of the relationship between the coefficients and the covariates. It also does not require strict exogeneity and is compatible with dynamic models. However, this generality comes at a cost: the mean group estimator typically has stricter data requirements compared to non-robust alternatives.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean Group: Robust Estimator for Average Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-mean-group.html#mean-group-estimation",
    "href": "linear/linear-mean-group.html#mean-group-estimation",
    "title": "6  Mean Group: Robust Estimator for Average Coefficients",
    "section": "",
    "text": "In static models, can we estimate the average effect without imposing restrictions on the relationship between \\(\\bX_{it}\\) and \\(\\bbeta_i\\)?\nIn dynamic models, can we estimate \\(\\E[\\lambda_i]\\) at all?\n\n\n\n6.1.1 Model\nTo formally define the mean group estimator, we return to the general model (2.7): \\[\n    \\bY_{it}^{\\bx} = \\bbeta_i'\\bx + U_{it},\n\\] where we label the dimension of the vectors \\(\\bx\\) and \\(\\bbeta_i\\) as \\(p\\).\nFor convenience, express the corresponding realized outcome equations in unit-level matrix form by stacking observations across \\(t\\) for each unit \\(i\\): \\[\n\\bY_i = \\bX_i\\bbeta_i + \\bU_i,\n\\] where \\(\\bX_i\\) is now a \\(T\\times p\\) matrix of covariates, with \\(T\\) representing the number of observations for each unit.\nIn what follows, we assume that the regressors \\(\\bX_{it}\\) are orthogonal to the unobserved component \\(U_{it}\\): \\[\n\\E[\\bX_{it}U_{it}] =0.\n\\tag{6.1}\\] This assumption is compatible with both static and dynamic models and is implied by sequential exogeneity.\n\n\n6.1.2 Definition\nFor the mean group estimator to be well-defined, we make two further assumptions:\n\nSufficient unit-level data: each unit must have at least as many observations as regressors, i.e., \\(T\\geq p\\).\nNon-singularity: \\(\\bX_i'\\bX_i\\) has rank maximal rank (\\(p\\)) for all units \\(i\\).\n\nUnder these assumptions, we can compute the OLS estimator of \\(\\bbeta_i\\) for each unit \\(i\\) \\[\n    \\hat{\\bbeta}_i = \\left(\\bX_i'\\bX_i \\right)^{-1}\\bX_i'\\bY_i.\n\\tag{6.2}\\]\nThe mean group estimator (M. H. Pesaran and Smith 1995) is defined as the average of unit-level estimators (6.2): \\[\n    \\hat{\\bbeta}_{MG} = \\dfrac{1}{N} \\sum_{i=1}^N \\hat{\\bbeta}_i =  \\dfrac{1}{N} \\sum_{i=1}^N \\left(\\bX_i'\\bX_i \\right)^{-1}\\bX_i'\\bY_i.\n\\] In essence, the mean group estimator involves running separate regressions for each unit and averaging the resulting coefficients. The first step requires having enough observations per unit — at least one observation is available for each heterogeneous coefficient.\n\n\n\n\n\n\nWe introduce the MG estimator under the orthogonality condition (6.1). However, it is also possible to introduce an analogous estimator if \\(\\E[\\bX_{it}U_{it}]\\neq 0\\), but instruments for \\(\\bX_{it}\\) are available. In this case, suitable unit-level IV/2SLS estimators replace \\(\\hat{\\bbeta}_i\\) (e.g. Bai, Marcellino, and Kapetanios 2023).\n\n\n\n\n\n6.1.3 Properties\nTo analyze the properties of the \\(\\hat{\\bbeta}_{MG}\\), we can substitute the model for \\(\\bY_i\\) into each individual estimator. This substitution yields the sampling error form of the estimator: \\[\n    \\hat{\\bbeta}_{MG}  = \\dfrac{1}{N} \\sum_{i=1}^N \\bbeta_i    + \\dfrac{1}{N} \\sum_{i=1}^N \\left(\\bX_i'\\bX_i \\right)^{-1}\\bX_i'\\bU_i.\n\\]\nAs \\(N\\) grows, the first term converges to the average effect: \\[\n\\dfrac{1}{N} \\sum_{i=1}^N \\bbeta_i   \\xrightarrow{p} \\E[\\bbeta_i].\n\\] This convergence holds regardless of assumptions on the dependence between \\(\\bbeta_i\\) and \\(\\bX_i\\).\nThe second term’s behavior determines the estimator’s overall properties, depending on whether strict exogeneity holds:\n\nIf strict exogeneity holds, the mean group estimator is unbiased and consistent for \\(\\E[\\bbeta_i]\\) even for \\(T\\) fixed. In this case it directly holds that \\[ \\E\\left[ \\left(\\bX_i'\\bX_i \\right)^{-1}\\bX_i'\\bU_i\\right] =0 \\] for any fixed value of \\(T\\).\nIf the model is dynamic or strict exogeneity fails for any other reason, the mean group estimator is typically only be consistent as both \\(N, T\\to\\infty\\). In this case it may be that \\[ \\E\\left[ \\left(\\bX_i'\\bX_i \\right)^{-1}\\bX_i'\\bU_i\\right] \\neq 0 ,\\] and \\(\\hat{\\bbeta}_{MG}\\) is biased for any given finite \\(T\\). However, under standard assumptions that limit dependence across \\(t\\), this bias is typically be of the order \\(O(T^{-1})\\) and vanishes as \\(T\\to\\infty\\).\n\n\n\n\n\n\n\nWe implicitly assume the existence of any required moment involving \\(\\left(\\bX_i'\\bX_i\\right)^{-1}\\). This assumption is not always innocent and imposes certain constraints on the distribution of covariates — requirements of sufficient variation. We do not discuss this point further, but see Graham and Powell (2012).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean Group: Robust Estimator for Average Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-mean-group.html#comparing-estimators",
    "href": "linear/linear-mean-group.html#comparing-estimators",
    "title": "6  Mean Group: Robust Estimator for Average Coefficients",
    "section": "6.2 Comparing Estimators",
    "text": "6.2 Comparing Estimators\n\n6.2.1 Conditions for Consistency\nWe now have two estimation strategies per case. For the strictly exogenous case, we have within and mean group estimation. In the dynamic case, we can use dynamic panel IV estimators or again use the mean group estimators.\nThe following tables summarize and contrast the requirements for each estimation strategy to consistently estimate \\(\\E[\\bbeta_i]\\):\n\nStrict exogeneityDynamic model with \\(k\\) lags\n\n\n\n\n\n\n\n\n\n\n\nWithin\nMG\n\n\n\n\nData requirements\n\\(T \\geq 2\\)\n\\(T \\geq p\\)\n\n\nAssumptions on \\((\\beta_i, X_i)\\)\n\\(\\mathbb{E}[\\tilde{X}_i'\\tilde{X}_i\\eta_i]=0\\)\nNone\n\n\nAsymptotic framework\nFixed-\\(T\\)\nFixed-\\(T\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIV\nMG\n\n\n\n\nData requirements\n\\(T \\geq k+2\\)\n\\(T \\geq p+k\\)\n\n\nAssumptions on \\((\\beta_i, X_i)\\)\nNo consistency\nNone\n\n\nAsymptotic framework\nNo consistency\nLarge-\\((N, T)\\)\n\n\n\n\n\n\n\n\n6.2.2 Efficiency\nWhile the mean group estimator provides robustness to heterogeneity and dynamics, it is generally less efficient than alternative estimators when those alternatives are consistent. This loss of efficiency directly follows from the AM-HM inequality and is well-documented (H. Pesaran, Smith, and Im 1996; M. H. Pesaran, Shin, and Smith 1999; Hsiao, Pesaran, and Tahmiscioglu 1999).\nHowever, one might argue that this loss of efficiency is irrelevant, as the conditions for the consistency of alternatives require unrealistic restrictions on the coefficients. Still, there are some ways of obtaining a more efficient robust estimator, such as applying a Mundlak device, see Breitung and Salish (2021).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean Group: Robust Estimator for Average Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-mean-group.html#addressing-rank-deficiency",
    "href": "linear/linear-mean-group.html#addressing-rank-deficiency",
    "title": "6  Mean Group: Robust Estimator for Average Coefficients",
    "section": "6.3 Addressing Rank Deficiency",
    "text": "6.3 Addressing Rank Deficiency\nBefore moving beyond \\(\\E[\\bbeta_i]\\), we return to the assumption that \\(\\bX_i'\\bX_i\\) is non-singular for all units \\(i\\). This requirement might not hold, in particular if \\(T\\) is only slightly larger than \\(p\\) and the model includes discrete regressors. In these cases, it is more likely that there exist units with insufficient individual variation.\nIf such a situation occurs in practice, a common solution is to drop the units with non-invertible \\(\\bX_i'\\bX_i\\). The mean group estimator is computed as an average over the remaining units. Formally, the corresponding mean group estimator is defined as \\[\n    \\hat{\\bbeta}_{MG}^+  = \\dfrac{1}{\\sum_{i=1}^N \\I\\curl{\\det(\\bX_i'\\bX_i)&gt;0} } \\sum_{i=1}^N \\I\\curl{\\det(\\bX_i'\\bX_i)&gt;0} \\hat{\\bbeta}_i.\n\\] Every unit is checked to see if \\(\\det(\\bX_i'\\bX_i)&gt;0\\). If it is, their \\(\\hat{\\bbeta}_i\\) is computed and added to the average.\nWhat does this modified estimator estimate? For simplicity, suppose that \\(\\E[\\bU_i|\\bX_i]=0\\). Then, as \\(N\\to\\infty\\), \\(\\hat{\\bbeta}_{MG}^+\\) converges to the average for the units that have enough variation in their data (subject to the above warning regarding existence of moments): \\[\n    \\hat{\\bbeta}_{MG}^+ \\to \\E\\left[\\bbeta_i| \\I\\curl{\\det(\\bX_i'\\bX_i)&gt;0}  \\right].\n\\] This limit may be viewed as an average treatment effect of the treated.\n\n\nNext Section\nIn the next section, we move beyond \\(\\E[\\bbeta_i]\\) and discuss identification of the variance of \\(\\bbeta_i\\).\n\n\n\n\nBai, Yu, Massimiliano Marcellino, and George Kapetanios. 2023. “Mean Group Instrumental Variable Estimation of Time-varying Large Heterogeneous Panels With Endogenous Regressors.” Econometrics and Statistics, June, S2452306223000412. https://doi.org/10.1016/j.ecosta.2023.06.004.\n\n\nBreitung, Jörg, and Nazarii Salish. 2021. “Estimation of Heterogeneous Panels with Systematic Slope Variations.” Journal of Econometrics 220 (2): 399–415. https://doi.org/10.1016/j.jeconom.2020.04.007.\n\n\nGraham, Bryan S, and James L Powell. 2012. “Identification and Estimation of Average Partial Effects in \"Irregular\" Correlated Random Coefficient Panel Data Models.” Econometrica 80 (5): 2105–52. https://doi.org/10.3982/ecta8220.\n\n\nHsiao, Cheng, M. Hashem Pesaran, and A. Kamil Tahmiscioglu. 1999. “Bayes Estimation of Short-Run Coefficients in Dynamic Panel Data Models.” In Analysis of Panels and Limited Dependent Variable Models, 268–96. https://doi.org/10.1017/cbo9780511493140.013.\n\n\nPesaran, Hashem, Ron Smith, and Kyung So Im. 1996. “Dynamic Linear Models for Heterogenous Panels.” In The Econometrics of Panel Data, edited by L. Matyas and P. Sevestre, 145–95. https://doi.org/10.1007/978-94-009-0137-7_8.\n\n\nPesaran, M. Hashem, Yongcheol Shin, and Ron P. Smith. 1999. “Pooled Mean Group Estimation of Dynamic Heterogeneous Panels.” Journal of the American Statistical Association 94 (446): 621–34. https://doi.org/10.1080/01621459.1999.10474156.\n\n\nPesaran, M. Hashem, and Ron P. Smith. 1995. “Estimating long-run relationships from dynamic heterogeneous panels.” Journal of Econometrics 6061: 473–77.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mean Group: Robust Estimator for Average Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-coefficient-variance.html",
    "href": "linear/linear-coefficient-variance.html",
    "title": "7  Variance of Heterogeneous Coefficients",
    "section": "",
    "text": "7.1 Beyond Average Coefficients",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variance of Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-coefficient-variance.html#beyond-average-coefficients",
    "href": "linear/linear-coefficient-variance.html#beyond-average-coefficients",
    "title": "7  Variance of Heterogeneous Coefficients",
    "section": "",
    "text": "7.1.1 Potential Objects of Interest\nPreviously, we focused on estimating the average coefficient vector \\(\\E[\\bbeta_i]\\) in our model (2.7): \\[\nY_{it}^{\\bx} = \\bbeta_i'\\bx + U_{it}.\n\\tag{7.1}\\] In particular, section 6 has shown that the mean group estimator can consistently estimate \\(\\E[\\bbeta_i]\\) even without imposing restrictions on the dependence between \\(\\bbeta_i\\) and \\(\\bX_{it}\\).\nAverage effects are informative but limited. They do not capture the full variation in responses across individuals (Heckman, Smith, and Clements (1997)). Other parameters of interest include:\n\nThe moments of \\(\\bbeta_i\\):\n\nVariance: overall dispersion in effects.\nSkewness: asymmetry in responses.\nHigher moments: shape of the distribution.\n\nThe full distribution and quantiles of \\(\\bbeta_i\\). For example, the distribution may be used to compute what proportion of people benefit vs. how many people are hurt by changes in \\(\\bX_{it}\\).\n\n\n\n7.1.2 Model\nAs it turns out, it is possible to identify such distributional features in the static version of model (7.1) without restricting the dependence structure between \\(\\bbeta_i\\) and \\(\\bX_{it}\\) (Arellano and Bonhomme 2012). In this section we discuss a streamlined version of their results for variance, while Chapter 9 shows how to identify the maximal object of interest — the full distribution.\nSpecifically, we consider model (7.1) under a strict exogeneity condition of the form: \\[\n\\E[U_{it}|\\bbeta_i, \\bX_i] =0\n\\tag{7.2}\\] The number \\(T\\) of unit-level observations is assumed to exceed the number \\(p\\) of covariates. We treat \\(T\\) as fixed, and consider large-\\(N\\) identification and estimation arguments.\nAs before, the model can be written in unit matrix form as \\[\n\\bU_i = \\bX_i\\bbeta_i + \\bU_i.\n\\] We again assume that \\(\\det(\\bX_i'\\bX_i)&gt;0\\) for all \\(i\\). If this condition does not hold, our results are about the subpopulation of units with positive determinant, as in section 6.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variance of Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-coefficient-variance.html#identification",
    "href": "linear/linear-coefficient-variance.html#identification",
    "title": "7  Variance of Heterogeneous Coefficients",
    "section": "7.2 Identification",
    "text": "7.2 Identification\nOur object of interest in this section is the variance-covariance matrix \\(\\var(\\bbeta_i)\\) of the coefficients \\(\\bbeta_i\\): \\[\n    \\var(\\bbeta_i) = \\E[\\bbeta_i\\bbeta_i'] - \\E[\\bbeta_i]\\E[\\bbeta_i]'.\n\\] Its diagonal terms are the variances of individual coefficients, which show how dispersed the effects are overall. The off-diagonal covariance terms capture whether the effects of different covariates tend to go in the same or contrary directions.\n\n7.2.1 Variance Decomposition\nTo identify \\(\\var(\\bbeta_i)\\), we first look at the second moments of \\(\\bY_i\\). Specifically, we consider the conditional second moment of \\(\\bY_i\\) given \\(\\bX_i=\\bX\\), where \\(\\bX\\) is some potential value of \\(\\bX_i\\) such that \\(\\det(\\bX'\\bX)&gt;0\\) and \\(\\bX\\) lies in the support of \\(\\bX_i\\).\nUsing model (7.1) and condition (7.2), the conditional second moment of \\(\\bY_i\\) can be represented as \\[\n\\begin{aligned}\n    &\\E[\\bY_i\\bY_i'|\\bX_i=\\bX]\\\\\n    & = \\E\\left[ (\\bX_i\\bbeta_i+ \\bU_i)(\\bX_i\\bbeta_i+\\bU_i)'|\\bX_i=\\bX \\right]\\\\\n    & =\\bX \\E\\left[\\bbeta_i\\bbeta_i'|\\bX_i=\\bX \\right]\\bX' + \\E\\left[\\bU_i\\bU_i'|\\bX_i=\\bX \\right].\n\\end{aligned}\n\\tag{7.3}\\]\nThe above expression decomposes the conditional second moment of \\(\\bY_i\\) into two components — one corresponding to \\(\\bbeta_i\\) and the other one to \\(\\bU_i\\).\nIn general, we cannot separate the two components of the second moment of \\(\\bY_i\\). To see why, consider Equation 7.3 as a system of linear equations. The unknowns are the components of the symmetric matrices\n\n\\(\\E\\left[\\bbeta_i\\bbeta_i'|\\bX_i=\\bX \\right]\\) — a total of \\(p(p+1)/2\\) unknowns.\n\\(\\E\\left[\\bU_i\\bU_i'|\\bX_i=\\bX \\right]\\) — a total of \\(T(T+1)/2\\) unknowns.\n\nThe system is underdetermined, as there is a total of only \\(T(T+1)/2\\) distinct equations.\nAt heart, the underdeterminacy stems from the fact that the model allows any dynamic structure for \\(\\curl{U_{it}}_{t=1}^T\\). As a consequence, \\(\\E[\\bU_i\\bU_i'|\\bX_i=\\bX]\\) has too many free elements relative to the number of equations.\n\n\n7.2.2 Imposing Structure on the Error Term\nThis issue can be resolved by imposing assumptions on the time series dependence of \\(U_{it}\\). The magnitudes of \\(T\\) and \\(p\\) determine how many restrictions are necessary. After taking out the \\(p(p+1)/2\\) parameters of \\(\\E[\\bbeta_i\\bbeta_i'|\\bX_i=\\bx]\\), we have at most \\(\\left[T(T+1)-p(p+1) \\right]/2\\) equations left. This number is the number of possible free parameters in \\(\\E[\\bU_i\\bU_i'|\\bX_i=\\bX]\\). In the most unfavorable case \\(T=p+1\\), and we can allow only \\(T+1\\) possible parameters in \\(\\E[\\bU_i\\bU_i'|\\bX_i=\\bX]\\).\nVarious assumptions are possible, and Arellano and Bonhomme (2012) explore moving average and autoregressive structures for \\(U_{it}\\). Here, we consider the simplest case in which \\(U_{it}\\) is conditionally homoskedastic across time (but not necessarily across \\(\\bX\\)) and uncorrelated across \\(t\\): \\[\n\\begin{aligned}\n    \\E[U_{it}^2|\\bX_i=\\bX] & = \\sigma^2(\\bX), \\\\\n    \\E[U_{it}U_{is}|\\bX_i=\\bX] & = 0, \\quad t\\neq s.\n\\end{aligned}\n\\tag{7.4}\\] Under assumption (7.4) it holds that \\[\n\\E\\left[\\bU_i\\bU_i'|\\bX_i=\\bX \\right] = \\sigma^2(\\bX)\\bI_T.\n\\] There is only one unknown parameter in \\(\\E[\\bU_i\\bU_i'|\\bX_i=\\bX]\\)!\n\n\n7.2.3 Variance of Residuals\nWe can identify the unknown \\(\\sigma^2(\\bX)\\) using a standard argument. Let the annihilator matrix associated with \\(\\bX\\) be given by \\[\n    \\bM(\\bX) = \\bI_T-\\bX(\\bX'\\bX)^{-1}\\bX'.\n\\tag{7.5}\\] Recall three key properties that \\(\\bM(\\cdot)\\) possesses \\[\n\\begin{aligned}\n     \\bM(\\bX)\\bX & =0, \\\\\n      \\bM(\\bX)\\bM(\\bX) & =\\bM(\\bX),  \\\\\n       \\bM(\\bX)' & =\\bM(\\bX).\n\\end{aligned}\n\\] Now consider the following second moment of \\(\\bM(\\bX)\\bY_i\\) conditional on \\(\\bX_i=\\bX\\): \\[\n\\begin{aligned}\n    & \\E[\\bY_i'\\bM(\\bX)'\\bM(\\bX)\\bY_i|\\bX_i=\\bX]\\\\\n    &  = \\E[\\bU_i'\\bM(\\bX)\\bU_i|\\bX_i=\\bX] \\\\\n    & = \\E\\left[ \\mathrm{tr}(\\bU_i'\\bM(\\bX)\\bU_i)|\\bX_i=\\bX\\right] \\\\\n    & = \\sigma^2(\\bX)(T-p).\n\\end{aligned}\n\\] The details of the trace argument are standard and can be found in section 4.11 of Hansen (2022).\nWe conclude that \\(\\sigma^2(\\bX)\\) is identified as \\[\n    \\sigma^2(\\bX) = \\dfrac{1}{T-p} \\E[\\bY_i'\\bM(\\bX)\\bY_i|\\bX_i=\\bX].\n\\]\n\n\n\n\n\n\nNote that it is also possible to solve for variance parameters from Equation 7.3! This approach can be useful when residuals exhibit more general dependence.\n\n\n\n\n\n7.2.4 Variance of Coefficients\nWe are now in a position to identify \\(\\var(\\bbeta_i)\\). In principle, we can solve for \\(\\E[\\bbeta_i\\bbeta_i']\\) by suitably vectorizing the system in Equation 7.3. However, it is more convenient to go back to the individual estimators (6.2). For brevity, let \\(\\bH_i = (\\bX_i'\\bX_i)^{-1}\\bX_i'\\), so that\n\\[\n\\hat{\\bbeta}_i = (\\bX_i'\\bX_i)^{-1}\\bX_i'\\bY_i =  \\bbeta_i + \\bH_i\\bU_i.\n\\] As \\(\\E[\\bbeta_i\\bU_i']=0\\) by condition (7.2), the variance of the individual estimator can be decomposed as \\[\n\\begin{aligned}\n    & \\var(\\hat{\\bbeta}_i) = \\var\\left(\\bbeta_i + \\bH_i\\bU_i \\right)\\\\\n    & = \\var(\\bbeta_i) + \\var(\\bH_i\\bU_i)\\\\\n    %\n    &\n    = \\var(\\bbeta_i) + \\E\\left[\\bH_i\\bU_i\\bU_i'\\bH_i' \\right]\\\\\n    & =   \\var(\\bbeta_i) + \\E\\left[\\E\\left[\\bH_i\\bU_i\\bU_i'\\bH_i'|\\bX_i \\right]\\right]\\\\\n    & =   \\var(\\bbeta_i) + \\E\\left[\\bH_i\\E\\left[\\bU_i\\bU_i'|\\bX_i \\right]\\bH_i'\\right]\\\\\n    & = \\var(\\bbeta_i) + \\E\\left[\\sigma(\\bX_i)\\bH_i\\bH_i'\\right].\n\\end{aligned}\n\\]\nObserve that the variance \\(\\var(\\hat{\\bbeta}_i)\\) of individual estimators \\(\\hat{\\bbeta}_i\\) (not \\(\\bbeta_i\\)!) is identified as \\[\n    \\var(\\hat{\\bbeta}_i) = \\var\\left(   (\\bX_i'\\bX_i)^{-1}\\bX_i\\bY_i  \\right).\n\\]\nSubstituting Equation 7.4 and rearranging, we obtain the following explicit expression for \\(\\var(\\bbeta_i)\\): \\[\n\\begin{aligned}  \n     \\var(\\bbeta_i)  &  =   \\var(\\hat{\\bbeta}_i) -   \\E\\left[\\sigma(\\bX_i)\\bH_i\\bH_i'\\right]\\\\\n     & =  \\var\\left(   (\\bX_i'\\bX_i)^{-1}\\bX_i\\bY_i  \\right)\\\\\n     & \\quad  -  \\dfrac{1}{T-p}\\E\\left[ \\E[\\bY_i'\\bM(\\bX_i)\\bY_i|\\bX_i]\\bH_i\\bH_i'\\right].\n\\end{aligned}\n\\tag{7.6}\\] Note that the right-hand side is a function of the distribution of \\((\\bY_i, \\bX_i)\\) — hence identification holds.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variance of Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-coefficient-variance.html#estimation",
    "href": "linear/linear-coefficient-variance.html#estimation",
    "title": "7  Variance of Heterogeneous Coefficients",
    "section": "7.3 Estimation",
    "text": "7.3 Estimation\nTo construct an estimator for \\(\\var(\\bbeta_i)\\) based on Equation 7.6, we replace the population expectations with their sample counterparts. Specifically, \\(\\var(\\hat{\\bbeta}_i)\\) is estimated using the sample variance of the individual estimators: \\[\n\\begin{aligned}\n    \\widehat{\\var(\\hat{\\bbeta}_i) } = \\dfrac{1}{N}\\sum_{i=1}^N \\left( \\hat{\\bbeta}_i - \\hat{\\bbeta}_{MG} \\right)\\left(\\hat{\\bbeta}_i-\\hat{\\bbeta}_{MG} \\right)'.\n\\end{aligned}\n\\] Likewise, we replace the expectation in the second term of Equation 7.6 with a sample average.\nThis process yields an estimator for the variance of \\(\\bbeta\\): \\[\n\\begin{aligned}\n    \\widehat{\\var(\\bbeta_i) } & = \\dfrac{1}{N}\\sum_{i=1}^N \\left( \\hat{\\bbeta}_i - \\hat{\\bbeta}_{MG} \\right)\\left(\\hat{\\bbeta}_i-\\hat{\\bbeta}_{MG} \\right)'\\\\\n    & \\quad - \\dfrac{1}{(T-p)N}\\sum_{i=1}^N \\bY_i'\\bM(\\bX_i)\\bY_i\\bH_i\\bH_i'.\n\\end{aligned}\n\\] Under standard regularity conditions, \\(\\widehat{\\var(\\bbeta_i) }\\) is consistent and (with some extra work) asymptotically normal, enabling inference on the variance of \\(\\bbeta_i\\).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variance of Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-coefficient-variance.html#higher-order-moments-and-moving-beyond",
    "href": "linear/linear-coefficient-variance.html#higher-order-moments-and-moving-beyond",
    "title": "7  Variance of Heterogeneous Coefficients",
    "section": "7.4 Higher-Order Moments and Moving Beyond",
    "text": "7.4 Higher-Order Moments and Moving Beyond\nIt turns out that a similar strategy can be used to identify the third and higher-order moments of \\(\\bbeta_i\\). See Arellano and Bonhomme (2012) for the details. The key ingredients of the identification argument for the \\(k\\)th moments of \\(\\bbeta_i\\) are:\n\n\\(k\\)th moments (or cumulants) of \\(\\bY_i\\) and \\(\\hat{\\bbeta}_i\\)\nRestrictions on the time series dynamics of \\(\\curl{U_{it}}_{t=1}^T\\)\nMultiplicative separability of the moments of \\(\\bbeta_i\\) and \\(\\bU_i\\) up to order \\(k\\).\n\nHowever, moments of \\(\\bbeta_i\\) may be also be obtained from the full distribution of \\(\\bbeta_i\\). Moreover, the distribution may also be used to compute quantiles of \\(\\bbeta_i\\) and further policy-relevant parameter. As such, this distribution is our maximal object of interest, and we now turn towards its identification.\n\n\nNext Section\nIn the next section, we review characteristic functions and deconvolution — key ingredients used in identification in the distribution of \\(\\bbeta_i\\).\n\n\n\n\nArellano, Manuel, and Stéphane Bonhomme. 2012. “Identifying distributional characteristics in random coefficients panel data models.” Review of Economic Studies 79 (3): 987–1020. https://doi.org/10.1093/restud/rdr045.\n\n\nHansen, Bruce. 2022. Econometrics. Princeton University Press.\n\n\nHeckman, James J., Jeffrey Smith, and Nancy Clements. 1997. “Making the Most out of Programme Evaluations and Social Experiments : Accounting for Heterogeneity in Programme Impacts.” Review of Economic Studies 64 (4): 487–535. https://doi.org/10.2307/2971729.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variance of Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-chf.html",
    "href": "linear/linear-chf.html",
    "title": "8  Interlude: Characteristic Functions and Deconvolution",
    "section": "",
    "text": "8.1 Characteristic Functions\nTo discuss the identification of the full distribution of the coefficients \\(\\bbeta_i\\) in model (2.7), we need to review two key concepts: characteristic functions and deconvolution. This brief section can be freely skipped if you are familiar with both.\nIf \\(\\bV\\) is a random \\(T\\)-vector, then the characteristic function \\(\\varphi_{\\bV}(s): \\R^T\\to \\C\\) of \\(\\bV\\) is defined as follows: \\[\n\\varphi_{\\bV}(\\bs) = \\E[\\exp(i\\bs'\\bV)].\n\\] See Durrett (2019) (or your favorite probability textbook) regarding general properties of characteristic functions.\nFor our purposes, we need the following three key properties:\nConditional characteristic functions may be defined analogously using conditional expectations in place of unconditional ones.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interlude: Characteristic Functions and Deconvolution</span>"
    ]
  },
  {
    "objectID": "linear/linear-chf.html#characteristic-functions",
    "href": "linear/linear-chf.html#characteristic-functions",
    "title": "8  Interlude: Characteristic Functions and Deconvolution",
    "section": "",
    "text": "The characteristic function uniquely determines the distribution.\nLet \\(\\bV, \\bU\\) be two independent random vectors. Then the characteristic function of their sum \\(\\bV+\\bU\\) is equal to the product of characteristic function of \\(\\bV\\) and \\(\\bU\\): \\[\n\\begin{aligned}\n     \\varphi_{\\bV+\\bU}(\\bs) & =  \\E\\left[e^{i\\bs'(\\bV+\\bU)}\\right] = \\E\\left[e^{i\\bs'\\bV}e^{i\\bs'\\bU} \\right]\\\\\n     & = \\E\\left[e^{i\\bs'\\bV}\\right]\\E\\left[e^{i\\bs'\\bU}\\right]\\\\\n     &  =  \\varphi_{\\bV}(\\bs) \\varphi_{\\bU}(\\bs).\n\\end{aligned}\n\\tag{8.1}\\]\nLet \\(\\bbeta\\) be a random \\(p\\)-vector and \\(\\bX\\) a matrix. Then \\[\n\\begin{aligned}\n     \\varphi_{\\bX\\bbeta}(\\bs) & = \\E\\left[\\exp(i\\bs'(\\bX\\bbeta)) \\right] \\\\\n     & = \\E\\left[\\exp(i(\\bX'\\bs)'\\bbeta) \\right]\\\\\n     &  = \\varphi_{\\bbeta}(\\bX'\\bs)\n\\end{aligned}\n\\tag{8.2}\\]",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interlude: Characteristic Functions and Deconvolution</span>"
    ]
  },
  {
    "objectID": "linear/linear-chf.html#deconvolution",
    "href": "linear/linear-chf.html#deconvolution",
    "title": "8  Interlude: Characteristic Functions and Deconvolution",
    "section": "8.2 Deconvolution",
    "text": "8.2 Deconvolution\nProperty (8.1) is particularly useful for statistical applications. It forms the basis of an estimation and identification approach known as deconvolution. This approach helps us identify the distribution of the coefficients \\(\\bbeta_i\\) in the next section.\nAt heart, deconvolution is simple. Suppose that we observe a random vector \\(\\bY\\). \\(\\bY\\) is generated as a sum of two independent vector \\(\\bV\\) and \\(\\bU\\). The distribution of \\(\\bU\\) is known, while the distribution of \\(\\bV\\) is the object of interest.\nBy property (8.1) the characteristic function of \\(\\bY\\) satisfies \\[\n    \\varphi_{\\bY}(\\bs)   =  \\varphi_{\\bV}(\\bs) \\varphi_{\\bU}(\\bs).\n\\] If \\(\\varphi_{\\bU}(\\bs)\\neq 0\\), we can divide and rearrange to obtain \\[\n\\varphi_{\\bV}(\\bs) = \\dfrac{\\varphi_{\\bY}(\\bs) }{\\varphi_{\\bU}(\\bs)}\n\\] By assumption, the distributions of \\(\\bY\\) and \\(\\bU\\) are known, and thus \\(\\varphi_{\\bY}(\\bs)\\) and \\(\\varphi_{\\bU}(\\bs)\\) are identified. It follows that the full \\(\\varphi_{\\bV}(\\cdot)\\) is also identified provided \\(\\varphi_{\\bU}(\\bs)\\neq 0\\) for all \\(\\bs\\) (or at least \\(\\varphi_{\\bU}(\\bs)= 0\\) for “not too many” \\(\\bs\\), see Evdokimov and White (2012)). The distribution of \\(\\bV\\) is identified since characteristic functions uniquely identify distributions.\nThis identification strategy is called deconvolution. The name of the procedure stems from the fact that the distribution of \\(\\bY\\) is the convolution of distributions of \\(\\bV\\) and \\(\\bU\\). Extracting the distribution of \\(\\bV\\) from the laws of \\(\\bY, \\bU\\) may be viewed as an inverse operation.\nObserve that the argument is nonparametric, as it imposes no parametric form assumptions on the distributions involved.\n\n\n\n\n\n\nIt is possible to relax the assumption that the distribution of \\(\\bU\\) is known by using mulitple observations. We discuss one approach in the following section. Another approach uses a second observation of \\(\\bY\\) using a result called Kotlarski’s lemma (Kotlarski 1967) (see Evdokimov and White (2012), Lewbel (2022) for extensions). Used with measurement error (see a review in Schennach 2016), nonparametric panel data models (Evdokimov 2010), and identification of systems of simultaneous equations (Lewbel, Schennach, and Zhang 2024).\n\n\n\n\n\nNext Section\nIn the next section, we discuss identification of the distribution of the coefficients \\(\\bbeta_i\\) in model (2.7) using the tools of this section.\n\n\n\n\nDurrett, Rick. 2019. Probability: Theory and Examples. Cambridge University Press. https://doi.org/10.1017/9781108591034.\n\n\nEvdokimov, Kirill. 2010. “Identification and Estimation of a Nonparametric Panel Data Model with Unobserved Heterogeneity.”\n\n\nEvdokimov, Kirill, and Halbert White. 2012. “Some Extensions of a Lemma of Kotlarski.” Econometric Theory 28 (4): 925–32. https://doi.org/10.1017/S0266466611000831.\n\n\nKotlarski, Ignacy. 1967. “On Characterizing the Gamma and the Normal Distribution.” Pacific Journal of Mathematics 20 (1): 69–76. https://doi.org/10.2140/pjm.1967.20.69.\n\n\nLewbel, Arthur. 2022. “Kotlarski with a Factor Loading.” Journal of Econometrics 229 (1): 176–79. https://doi.org/10.1016/j.jeconom.2020.12.012.\n\n\nLewbel, Arthur, Susanne M. Schennach, and Linqi Zhang. 2024. “Identification of a Triangular Two Equation System Without Instruments.” Journal of Business & Economic Statistics 42 (1): 14–25. https://doi.org/10.1080/07350015.2023.2166052.\n\n\nSchennach, Susanne M. 2016. “Recent Advances in the Measurement Error Literature.” Annual Review of Economics 8 (1): 341–77. https://doi.org/10.1146/annurev-economics-080315-015058.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Interlude: Characteristic Functions and Deconvolution</span>"
    ]
  },
  {
    "objectID": "linear/linear-distribution.html",
    "href": "linear/linear-distribution.html",
    "title": "9  Distribution of Heterogeneous Coefficients",
    "section": "",
    "text": "9.1 Model and Conditional Independence Assumption",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribution of Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-distribution.html#model-and-conditional-independence-assumption",
    "href": "linear/linear-distribution.html#model-and-conditional-independence-assumption",
    "title": "9  Distribution of Heterogeneous Coefficients",
    "section": "",
    "text": "9.1.1 Model\nWe are now in a position to obtain our final and strongest result in this block: identify the full distribution of \\(\\bbeta_i\\) in model (2.7): \\[\nY_{it}^{\\bx} = \\bbeta_i'\\bx + U_{it}.\n\\] As in section 7, we impose strict exogeneity \\[\n\\E[\\bU_i|\\bbeta_i, \\bX_i] = 0.\n\\] We also assume the existence of the first two moments of \\(\\bbeta_i, \\bX_i\\), and \\(\\bU_i\\).\nAs before, the number \\(T\\) of unit-level observations is assumed to exceed the number \\(p\\) of covariates. We treat \\(T\\) as fixed, and consider large-\\(N\\) identification and estimation arguments.\n\n\n9.1.2 Conditional Independence of \\(\\bbeta_i\\) and \\(\\bU_i\\)\nThe key to our identification strategy is the following assumption of conditional independence between \\(\\bbeta_i\\) and \\(\\bU_i\\): \\[\n\\bbeta_i \\independent \\curl{U_{it}}_{t=1}^T |\\bX_i\n\\tag{9.1}\\] To understand assumption (9.1), consider a production function example. Let \\(U_{it}\\) be the measurement error in the output value \\(Y_{it}\\). The variance of \\(U_{it}​\\) may depend on the scale of the firm (captured by capital), leading to dependence between \\(\\bX_i\\) and \\(\\bU_i\\). However, it is plausible that the firm size captures all information about firm technology relevant for measurement error in the value of the firm’s output. In this case, the assumption appears reasonable.\n\n\n9.1.3 Implication of Conditional Independence\nGiven the conditional independence assumption (9.1), both \\(\\bY_i\\) and the individual estimators \\(\\hat{\\bbeta}_i\\) are sums of two conditionally independent vectors. Specifically, conditionally on \\(\\curl{\\bX_i=\\bX}\\):\n\n\\(\\bY_i\\) is the sum of \\(\\bX\\bbeta_i\\) and \\(\\bU_i\\)\n\\(\\hat{\\bbeta}_i\\) is the sum of \\(\\bbeta_i\\) and \\((\\bX'\\bX)^{-1}\\bX\\bU_i\\)\n\nWe can write the conditional characteristic function of \\(\\bY_i\\) given \\(\\bX_i=\\bX\\) using properties (8.1) and (8.2) as: \\[\n\\begin{aligned}\n    \\varphi_{\\bY_i|\\bX_i}(\\bs|\\bX) & = \\varphi_{\\bX'\\bbeta_i|\\bX_i}(\\bs|\\bX)\\varphi_{\\bU_i|\\bX_i}(\\bs|\\bX) \\\\\n    & = \\varphi_{\\bbeta_i|\\bX_i}(\\bX'\\bs|\\bX)\\varphi_{\\bU_i|\\bX_i}(\\bs|\\bX).\n\\end{aligned}\n\\tag{9.2}\\] Similarly, the conditional characteristic function of \\(\\hat{\\bbeta}_i\\) given \\(\\bX_i=\\bX\\) satisfies \\[\n\\begin{aligned}\n    \\varphi_{\\hat{\\bbeta}_i|\\bX_i}(\\bs|\\bX) & = \\varphi_{\\bbeta_i|\\bX_i}(\\bs|\\bX) \\varphi_{\\bH_i\\bU_i|\\bX_i}(\\bs|\\bX) \\\\\n    &  = \\varphi_{\\bbeta_i|\\bX_i}(\\bs|\\bX) \\varphi_{\\bU_i|\\bX_i}(\\bX(\\bX'\\bX)^{-1}\\bs|\\bX),\n\\end{aligned}\n\\tag{9.3}\\] where we again define \\(\\bH_i = (\\bX_i'\\bX_i)^{-1}\\bX_i\\).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribution of Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-distribution.html#identification-of-the-distribution",
    "href": "linear/linear-distribution.html#identification-of-the-distribution",
    "title": "9  Distribution of Heterogeneous Coefficients",
    "section": "9.2 Identification of the Distribution",
    "text": "9.2 Identification of the Distribution\n\n9.2.1 Overall Strategy\nTo identify the distribution, we proceed similarly to how we identified the variance. The steps are:\n\nIdentify \\(\\varphi_{\\bU_i|\\bX_i}(\\cdot|\\bX)\\) from Equation 9.2.\nApply deconvolution to Equation 9.3 to recover \\(\\varphi_{\\bbeta_i|\\bX_i}(\\bs|\\bX)\\).\nInvert the characteristic function of \\(\\bbeta_i\\) to obtain the distribution.\n\n\n\n9.2.2 Equation in Hessians of Characteristic Functions\nWe start by rewriting Equation 9.2 in a more useful form. The characteristic functions in (9.2) are twice differentiable under our moment assumptions. Taking logarithms (see here) and differentiating twice yields \\[\n\\begin{aligned}\n    & \\dfrac{\\partial^2 \\log(   \\varphi_{\\bY_i|\\bX_i}(\\bs|\\bX))}{\\partial\\bs\\partial\\bs'} \\\\\n    & = \\bX \\dfrac{\\partial^2 \\log( \\varphi_{\\bbeta_i|\\bX_i}(\\bX'\\bs|\\bX)) }{\\partial \\bs\\partial\\bs'} \\bX' +  \\dfrac{\\partial^2 \\log(\\varphi_{\\bU_i|\\bX_i}(\\bs|\\bX))}{\\partial \\bs\\partial\\bs'} .\n\\end{aligned}\n\\tag{9.4}\\]\nThis equation is similar to the expression (7.3) we obtained for variance. It decomposes the characteristic function of the data into contributions from the coefficients \\(\\bbeta_i\\) and the residuals \\(\\bU_i\\). Unlike the variance expression, system (9.4) is a functional equation parametrized by \\(\\bs\\).\n\n\n9.2.3 Imposing Structure on the Error Term\nOur goal is to solve for the second term in the linear system (9.4). However, like system (7.3), system (9.4) is underdetermined. Accordingly, we need to impose additional assumptions to disentangle the \\(\\bU_i\\) component from the \\(\\bbeta_i\\) one.\nIn these notes, we consider a simple assumption that strengthens our temporal homoskedasticity assumption (7.4). Specifically, we assume that \\(U_{it}\\) is IID across \\(i\\) and \\(t\\) conditional on \\(\\bX_i=\\bX\\). This assumption implies that all \\(U_{it}\\) have the same characteristic function for all \\(i\\) and \\(t\\): \\[\n    \\varphi_{U_{i1}|\\bX_i}(s|\\bX) = \\cdots = \\varphi_{U_{iT}|\\bX_i}(s|\\bX).\n\\tag{9.5}\\] We label the common function \\(\\varphi_{U|\\bX_i}(s|\\bX)\\).\nThe characteristic function of the \\(T\\)-vector \\(\\bU_i\\) can be written as \\[\n\\begin{aligned}\n    \\varphi_{\\bU_i|\\bX_i}(\\bs|\\bX) & = \\prod_{j=1}^T \\varphi_{U|\\bX_i}(s_j|\\bX), \\\\\n    \\bs & = (s_1, s_2, \\dots, s_T).\n\\end{aligned}\n\\] Taking logarithms turns the product into a sum: \\[\n    \\log\\left(\\varphi_{\\bU_i|\\bX_i}(\\bs|\\bX)\\right) = \\sum_{j=1}^T \\log(\\varphi_{U|\\bX_i}(s_j|\\bX)).\n\\] The Hessian of this function with respect to \\(\\bs\\) is diagonal: \\[\n\\begin{aligned}\n    \\dfrac{\\partial^2 \\log(\\varphi_{\\bU_i|\\bX_i}(\\bs|\\bX))}{\\partial \\bs\\partial\\bs'}  & = \\diag\\curl{\\bphi(\\bs)},\\\\\n\\end{aligned}\n\\] where \\[\n    \\bphi(\\bs)  =  \\left(\\dfrac{d^2\\log(\\varphi_{U|\\bX_i}(s_1|\\bX))}{ds_1^2}, \\dots,   \\dfrac{d^2\\log(\\varphi_{U|\\bX_i}(s_T|\\bX))}{ds_T^2}\\right).\n\\] To summarize, assumption (9.5) reduces the unknown \\(T\\times T\\) matrix \\(\\frac{\\partial^2 \\log(\\varphi_{\\bU_i|\\bX_i}(\\bs|\\bX))}{\\partial \\bs\\partial\\bs'}\\) to an unknown \\(T\\)-vector \\(\\bphi(\\bs)\\). There are now sufficiently many equations to cover all the remaining unknown components, provided standard rank conditions hold.\n\n\n9.2.4 Solving for the Distribution of Residuals\nTo solve for \\(\\bphi(\\bs)\\), we return to (9.4). We first put it into more familiar tall form (one line, one equation) using the vectorization operator. Applying the vectorization operator yields \\[\n\\begin{aligned}\n    & \\vecc\\left(\\dfrac{\\partial^2 \\log(    \\varphi_{\\bY_i|\\bX_i}(\\bs|\\bX))}{\\partial\\bs\\partial\\bs'}\\right) \\\\\n    &  = (\\bX \\otimes \\bX) \\vecc\\left(\\dfrac{\\partial^2 \\log( \\varphi_{\\bbeta_i|\\bX_i}(\\bX'\\bs|\\bX)) }{\\partial \\bs\\partial\\bs'}\\right) +   \\bA\\bphi(\\bs),\n\\end{aligned}\n\\tag{9.6}\\] where an explicit formula for \\(\\bA\\) can be found here.\nNow we premultiply system (9.6) by \\(\\bM(\\bX\\otimes \\bX)\\) where \\(\\bM(\\cdot)\\) is defined in (7.5): \\[\n\\begin{aligned}\n    & \\bM(\\bX\\otimes \\bX)   \\vecc\\left(\\dfrac{\\partial^2 \\log(  \\varphi_{\\bY_i|\\bX_i}(\\bs|\\bX))}{\\partial\\bs\\partial\\bs'}\\right)\n    \\\\ & = \\bM(\\bX\\otimes \\bX)\\bA\\bphi(\\bs)\n\\end{aligned}\n\\tag{9.7}\\] We can solve this system for \\(\\bphi(\\bs)\\) provided \\(\\rank(\\bM(\\bX\\otimes \\bX)\\bA)=T\\). Indeed, this rank condition holds in this case, as shown by Arellano and Bonhomme (2012). As both \\(\\bM(\\bX\\otimes\\bX)\\) and \\(\\varphi_{\\bY_i|\\bX_i}(\\cdot|\\cdot)\\) are identified, we conclude that \\(\\bphi(\\bs)\\) is also identified.\nThe characteristic function of \\(\\bU_i\\) is now straighforward to recover from \\(\\bphi(\\bs)\\) by integrating twice with respect to \\(\\bs\\). As \\(\\bphi(\\bs)\\) encodes second derivatives, we need two initial values. These initial values are provided by the properties of the characteristic function and the assumption of strict exogeneity: \\[\n\\begin{aligned}\n    \\dfrac{\\partial \\log(\\varphi_{\\bU_i|\\bX_i}(0|\\bX))}{\\partial\\bs'} &  = \\E[\\bU_i|\\bX_i=\\bX] = 0,\\\\\n    \\log(\\varphi_{\\bU_i|\\bX_i}(0|\\bX))  & = 0.\n\\end{aligned}\n\\] This completes the first identification step.\n\n\n9.2.5 Identifying the Distribution of Coefficients\nFor the second step — identification of \\(\\varphi_{\\bbeta_i|\\bX_i}\\) — we return to Equation 9.3. We make an additional assumption: \\[\n\\varphi_{\\bU_i|\\bX_i}(\\bs|\\bX)\\neq 0 \\text{ for all }\\bs.\n\\] This assumption allows us to divide by the characteristic function of \\(\\bU_i\\) in Equation 9.3 and obtain \\[\n\\varphi_{\\bbeta_i|\\bX_i}(\\bs|\\bX) = \\dfrac{\\varphi_{\\hat{\\bbeta}_i|\\bX_i}(\\bs|\\bX)  }{\\varphi_{\\bU_i|\\bX_i}(\\bX(\\bX'\\bX)^{-1}\\bs|\\bX)}.\n\\tag{9.8}\\]\nFinally, the density or cumulative distribution functions of the coefficients may be recovered using inversion formulae. For continuously distributed coefficients, the conditional density is: \\[\nf_{\\bbeta_i|\\bX_i}(\\bb|\\bX) = \\dfrac{1}{(2\\pi)^n} \\int_{\\R^p} \\exp(-i\\bs'\\bb)\\varphi_{\\bbeta_i|\\bX_i}(\\bs|\\bX)d\\bs.\n\\] Last, we can recover the unconditional distribution of the coefficients since we know the marginal distribution of \\(\\bX_i\\). For example, if \\(f_{\\bX_i}\\) is the marginal density, the unconditional density of \\(\\bbeta_i\\) is obtained by simply integrating \\(\\bX_i\\) out as \\[\nf_{\\bbeta_i}(\\bb) = \\int f_{\\bbeta_i|\\bX_i}(\\bb|\\bX)f_{\\bX_i}(\\bX)d\\bX.\n\\]",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribution of Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-distribution.html#estimation",
    "href": "linear/linear-distribution.html#estimation",
    "title": "9  Distribution of Heterogeneous Coefficients",
    "section": "9.3 Estimation",
    "text": "9.3 Estimation\n\n9.3.1 With Discrete Covariates\nFor estimation, we discuss the conceptually simpler case where \\(\\bX_i\\) has finite support. In such a setting, there is a non-zero probability that \\(\\bX_i\\) takes each value in its support.\nEstimation proceeds in three steps:\n\nEstimation of \\(\\varphi_{\\hat{\\bbeta}_i|\\bX_i}(\\cdot|\\bX)\\).\nEstimation of \\(\\varphi_{\\bU_i|\\bX_i}(\\cdot|\\bX)\\).\nCombining the estimators of the first two steps using Equation 9.8 and inverting the resulting estimated characteristic function.\n\nThe characteristic function of the individual estimators \\(\\hat{\\bbeta}_i\\) can be estimated with the empirical characteristic function on the sample of units with \\(\\bX_i=\\bX\\): \\[\n\\hat{\\varphi}_{\\hat{\\bbeta}_i|\\bX_i}(\\bs|\\bX) = \\dfrac{1 }{\\sum_{i=1}^N \\I\\curl{\\bX_i=\\bX} }\\sum_{i=1}^N \\I\\curl{\\bX_i=\\bX} \\exp\\left( i\\bs'\\hat{\\bbeta}_i \\right).\n\\]\nAs for the characteristic function of \\(\\bU_i\\), it can be estimated from a sample version of Equation 9.7. We replace the characteristic function of the data with its empirical counterpart: \\[\n\\hat{\\varphi}_{\\bY_i|\\bX_i}(\\bs|\\bX) = \\dfrac{1 }{\\sum_{i=1}^N \\I\\curl{\\bX_i=\\bX} }\\sum_{i=1}^N \\I\\curl{\\bX_i=\\bX} \\exp\\left( i\\bs'\\bY_i \\right).\n\\]\n\n\n9.3.2 With Continuous Covariates\nIf \\(\\bX_i\\) is continuously distributed, it is instead necessary to estimate the characteristic functions using techniques such as kernel regression. Evdokimov (2010) studies such conditional deconvolution estimators and their asymptotic properties. Inference may be conducted using the results of Kato, Sasaki, and Ura (2021).\n\n\n\n\n\n\nSuch nonparametric estimators may perform poorly due to the curse of dimensionality if \\(T\\) and \\(p\\) are not small. In the next block we discuss some assumptions that can reduce the dimensionality of the problem and be applied in this context.\n\n\n\n\n\nNext Section\nIn the next section, we briefly conclude the block and discuss some further results on heterogeneous linear models.\n\n\n\n\nArellano, Manuel, and Stéphane Bonhomme. 2012. “Identifying distributional characteristics in random coefficients panel data models.” Review of Economic Studies 79 (3): 987–1020. https://doi.org/10.1093/restud/rdr045.\n\n\nEvdokimov, Kirill. 2010. “Identification and Estimation of a Nonparametric Panel Data Model with Unobserved Heterogeneity.”\n\n\nKato, Kengo, Yuya Sasaki, and Takuya Ura. 2021. “Robust Inference in Deconvolution.” Quantitative Economics 12 (1): 109–42. https://doi.org/10.3982/QE1643.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Distribution of Heterogeneous Coefficients</span>"
    ]
  },
  {
    "objectID": "linear/linear-conclusion.html",
    "href": "linear/linear-conclusion.html",
    "title": "10  Conclusion: Linear Models",
    "section": "",
    "text": "10.1 Summary\nIn this block, we have examined the linear panel data model (2.7) with time-invariant heterogeneous coefficients. Our primary focus was on identifying the moments (mean, variance, etc.) and the full distribution of these coefficients:",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion: Linear Models</span>"
    ]
  },
  {
    "objectID": "linear/linear-conclusion.html#summary",
    "href": "linear/linear-conclusion.html#summary",
    "title": "10  Conclusion: Linear Models",
    "section": "",
    "text": "For the mean:\n\nWe discovered that traditional within and IV estimators are generally inconsistent under coefficient heterogeneity.\nTo address this, we introduced the mean group estimator, which is robust to any dependence structure between the coefficients and the covariates Pesaran and Smith (1995).\n\nFor the variance and distribution, we employed a constructive approach from Arellano and Bonhomme (2012), which relies on assumptions about the idiosyncratic error components. As with the mean, the variance and distribution are identified without restricting the dependence between the coefficients and the covariates.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion: Linear Models</span>"
    ]
  },
  {
    "objectID": "linear/linear-conclusion.html#some-further-results-on-heterogeneous-linear-models",
    "href": "linear/linear-conclusion.html#some-further-results-on-heterogeneous-linear-models",
    "title": "10  Conclusion: Linear Models",
    "section": "10.2 Some Further Results on Heterogeneous Linear Models",
    "text": "10.2 Some Further Results on Heterogeneous Linear Models\nWe have barely scratched the surface of the literature on linear models with unobserved heterogeneity is extensive. If you are interested, here is a selection of some further results:\n\nFurther results in model (2.7): For example, it is possible to identify the average effect even when the number of periods equals the number of covariates (Graham and Powell (2012)). Additionally, endogeneity can be permitted in the equation (Laage (2024)).\nTime-varying coefficients: Models (2.4) with time-varying heterogeneous coefficients have also received some recent attention, particularly in the “grouped fixed effect” literature. Notable papers include Bonhomme and Manresa (2015) and Lumsdaine, Okui, and Wang (2023) (also see Bonhomme, Lamadon, and Manresa (2022)).\nUnobserved Factors: Models with unobserved factors allow for common shocks to impact individuals differently and provide a parsimonious way to include cross-sectional dependence. Key references include Pesaran (2006) and Bai (2009).\nCross-Sectional Data: Some identification is possible with cross-sectional data, typically assuming that coefficients are independent from the covariates (a randomized experiment framework). See Hoderlein, Klemelä, and Mammen (2010) and Masten (2018) for identifying the full distribution of coefficients in single-equation and system of equations frameworks, respectively.\nDeconvolution Applications: Deconvolution is a versatile technique with broad applications. For examples, see Bonhomme and Robin (2009) and Bonhomme and Robin (2010).\n\n\n\nNext Section\nIn the next section we will move beyond linearity and start our discussion of nonparametric models with unobserved heterogeneity.\n\n\n\n\nArellano, Manuel, and Stéphane Bonhomme. 2012. “Identifying distributional characteristics in random coefficients panel data models.” Review of Economic Studies 79 (3): 987–1020. https://doi.org/10.1093/restud/rdr045.\n\n\nBai, Jushan. 2009. “Panel Data Models With Interactive Fixed Effects.” Econometrica 77 (4): 1229–79. https://doi.org/10.3982/ECTA6135.\n\n\nBonhomme, Stéphane, Thibaut Lamadon, and Elena Manresa. 2022. “Discretizing Unobserved Heterogeneity.” Econometrica 90 (2): 625–43. https://doi.org/10.3982/ECTA15238.\n\n\nBonhomme, Stéphane, and Elena Manresa. 2015. “Grouped Patterns of Heterogeneity in Panel Data.” Econometrica 83 (3): 1147–84. https://doi.org/10.3982/ecta11319.\n\n\nBonhomme, Stéphane, and Jean-Marc Robin. 2009. “Assessing the Equalizing Force of Mobility Using Short Panels: France, 1990–2000.” The Review of Economic Studies 76 (1): 63–92. https://doi.org/10.1111/j.1467-937X.2008.00521.x.\n\n\n———. 2010. “Generalized Non-Parametric Deconvolution with an Application to Earnings Dynamics.” Review of Economic Studies 77 (2): 491–533. https://doi.org/10.1111/j.1467-937X.2009.00577.x.\n\n\nGraham, Bryan S, and James L Powell. 2012. “Identification and Estimation of Average Partial Effects in \"Irregular\" Correlated Random Coefficient Panel Data Models.” Econometrica 80 (5): 2105–52. https://doi.org/10.3982/ecta8220.\n\n\nHoderlein, Stefan, Jussi Klemelä, and Enno Mammen. 2010. “Analyzing the Random Coefficient Model Nonparametrically.” Econometric Theory 26 (03): 804–37. https://doi.org/10.1017/S0266466609990119.\n\n\nLaage, Louise. 2024. “A Correlated Random Coefficient Panel Model With Time-Varying Endogeneity.” Journal of Econometrics 242 (2): 105804. https://doi.org/10.1016/j.jeconom.2024.105804.\n\n\nLumsdaine, Robin L., Ryo Okui, and Wendun Wang. 2023. “Estimation of Panel Group Structure Models With Structural Breaks in Group Memberships and Coefficients.” Journal of Econometrics 233 (1): 45–65. https://doi.org/10.1016/j.jeconom.2022.01.001.\n\n\nMasten, Matthew A. 2018. “Random Coefficients on Endogenous Variables in Simultaneous Equations Models.” Review of Economic Studies 85 (2): 1193–1250. https://doi.org/10.1093/restud/rdx047.\n\n\nPesaran, M. Hashem. 2006. “Estimation and Inference in Large Heterogeneous Panels with a Multifactor Error Structure.” Econometrica 74 (4): 967–1012. https://doi.org/10.1111/j.1468-0262.2006.00692.x.\n\n\nPesaran, M. Hashem, and Ron P. Smith. 1995. “Estimating long-run relationships from dynamic heterogeneous panels.” Journal of Econometrics 6061: 473–77.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion: Linear Models</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-introduction.html",
    "href": "nonparametric/nonparametric-introduction.html",
    "title": "11  Introduction to Nonparametric Models with Unobserved Heterogeneity",
    "section": "",
    "text": "11.1 Towards Nonparametric Models",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Nonparametric Models with Unobserved Heterogeneity</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-introduction.html#towards-nonparametric-models",
    "href": "nonparametric/nonparametric-introduction.html#towards-nonparametric-models",
    "title": "11  Introduction to Nonparametric Models with Unobserved Heterogeneity",
    "section": "",
    "text": "11.1.1 Motivation\nWe began these notes by studying linear models with heterogeneous coefficients (2.7), a familiar and flexible starting point. As discussed in section 2, such models are widely used and arise naturally in empirical work.\nBut linearity is not innocent. Except for binary-only covariates, the assumption often lacks theoretical support and may be at odds with the data. Many economic settings suggest richer structures: preferences with satiation, production with non-constant returns to scale, or outcomes bounded by construction. Furthermore, differences between individuals may not be compressible into a finite-dimensional vector of heterogeneous coefficients. In such cases, linear models may be severely misspecified and lead to incorrect conclusions.\nThis motivates a shift. In this block, we move beyond linearity and consider nonparametric models with unobserved heterogeneity — a class that allows for far greater flexibility in how outcomes respond to both observed and unobserved variation. These models present new challenges but also offer a more powerful framework for accounting for unobserved differences.\n\n\n11.1.2 Nonparametric Models\nNonparametric models address functional form concerns directly. Rather than imposing a specific shape on the relationship between \\(Y\\) and the treatments \\(\\bX\\), we assume only that this relationship is governed by an unknown function \\(\\phi\\) of observed and unobserved variables. This leads to the following general potential outcome models:\n\nIn cross-sectional settings: \\[\n  Y_i^{\\bx} = \\phi(\\bx, A_i), \\quad {}_{i=1,\\dots, N},\n\\tag{11.1}\\] where \\(\\bX_{i}\\) includes the observed variables and \\(A_i\\) includes the unobserved components.\nIn panel data settings \\[\nY_{it}^{\\bx} = \\phi(\\bx, A_i, U_{it}), \\quad {}_{i=1,\\dots, N}^{t= 1, \\dots, T},\n\\tag{11.2}\\] where both \\(A_i\\) and \\(U_{it}\\) are not observed.\n\nModels (11.1) and (11.2) parallel and generalize models (2.3) and (2.4).\nIn both cases the nature of \\((A_i, U_{it})\\) is not restricted a priori. These unobserved components may include both finite-dimensional vectors (such as unobserved variables or coefficients) and infinite-dimensional objects (such as utility functions). In such a fully unrestricted setting, we can equivalently represent (11.1) and (11.2) as \\[\n    Y_i^{\\bx} = \\phi_i(\\bx), \\quad \\quad  Y_{it}^{\\bx} = \\phi_{it}(\\bx),\n\\] respectively.\n\n\n11.1.3 Object of Interest\nAs in the linear case, possible objects of interest include:\n\nThe full structural function \\(\\phi(\\cdot, \\cdot)\\) or \\(\\phi(\\cdot, \\cdot, \\cdot)\\). This function fully traces out the potential outcomes \\(Y^{\\bx}\\) for all individuals. This corresponds to the problem of identifying individual treatment effects.\nSome distributional features of “treatment effects” — changes in outcomes due to variation in \\(\\bX_{it}\\), conditional on unobserved heterogeneity. In the context of model (11.2), these effects are given by \\[\n  \\begin{aligned}\n      &   \\phi(\\bx_2, A_i, U_{it}) - \\phi(\\bx_1, A_i, U_{it}),\\\\\n      &   \\partial_{\\bx} \\phi(\\bx_0, A_i, U_{it}),\n  \\end{aligned}\n\\tag{11.3}\\] where \\(\\bx_0, \\bx_1, \\bx_2\\) are some possible values for \\(\\bX_{it}\\), and the marginal effect is considered if \\(\\phi\\) is suitably differentiable in \\(\\bx\\). The distributional feature of interest may include average effects, variances, higher-order moments, or the full distribution.\n\n\n\n11.1.4 Common Issue\nUnfortunately, models (11.1) and (11.2) require further assumptions to be useful, as discussed in section 1. Without assumptions, we cannot hope to identify counterfactual objects of interest, not even average effects.\n\n\n\n\n\n\nIn a strict sense, one may point out that (11.1) and (11.2) are not even models in the sense of offering testable predictions or supporting counterfactual analysis. (11.1) and (11.2) are just statements that \\(\\bX\\) and \\(Y\\) are related through some function that may differ with \\(i\\) and \\(t\\). Such a statement is vacuously true. For example, without further assumptions one may take \\(\\phi(x, a) = a\\) and \\(A_i = Y_i\\) in (11.1).\n\n\n\nTypically, such assumptions fall into two categories:\n\nAssumptions on the joint distribution of the observed and the unobserved components, including on the nature of \\((A_i, U_{it})\\).\nAssumptions on how unobserved components enter the equation.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Nonparametric Models with Unobserved Heterogeneity</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-introduction.html#models-of-this-block",
    "href": "nonparametric/nonparametric-introduction.html#models-of-this-block",
    "title": "11  Introduction to Nonparametric Models with Unobserved Heterogeneity",
    "section": "11.2 Models of This Block",
    "text": "11.2 Models of This Block\nTo make progress, we focus on two general but tractable versions of the general nonparametric panel model (11.2) in these notes. In both cases, we assume that the outcome \\(Y_{it}\\) is continuous and that \\(T=2\\).\n\nFirst Model\nWe begin with the following model: \\[\n    Y_{it}^{x} = \\phi(x, A_i, U_{it}), \\quad {}_{i=1, \\dots, N}^{t=1, 2}.\n\\tag{11.4}\\] where for simplicity we assume that \\(x\\) is scalar. The realized treatment \\(X_{it}\\) is assumed to be continuously distributed, and \\(\\phi\\) is continuous in \\(x\\) for all values of \\((A_i, U_{it})\\). As always, the realized outcomes \\(Y_{it}\\) satisfy \\[\nY_{it} = Y_{it}^{X_{it}} =  \\phi(x, A_i, U_{it}), \\quad {}_{i=1, \\dots, N}^{t=1, 2}.\n\\]\nWe consider a version of (11.4) that is very general in terms of unobserved variables \\((A_{i}, U_{it})\\). In particular, we do not restrict\n\nThe dimension and the form of \\(A_i\\) and \\(U_{it}\\);\nHow the outcome depends on \\((A_{i}, U_{it})\\);\nThe dependence structure between \\((A_i, U_{it})\\) and \\(X_{it}\\).\n\nAt the same time, we impose an assumption of stationarity on \\(U_{it}\\). Its distribution is stable over time conditional on observed and unobserved covariates, allowing us to isolate changes in \\(Y_{it}\\) attributable to variation in \\(X_{it}\\).\n\n\nSecond Model\nAfter reaching the (probable) limits of identification with (11.4), we consider a different flavor of model (11.2), where the time-varying unobserved component \\(U_{it}\\) is scalar and affects the outcome \\(Y_{it}\\) additively: \\[\nY_{it}^{x} = \\phi(x, A_i) + U_{it}, \\quad {}_{i=1, \\dots, N}^{t=1, 2}\n\\tag{11.5}\\] In contrast to model (11.4), we do not assume that \\(U_{it}\\) is stationary. Models (11.4) and (11.5) are hence non-nested. We continue to allow \\(A_i\\) to have unrestricted dimensionality and structure. It may also have a complex dependence structure with \\(X_{it}\\).",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Nonparametric Models with Unobserved Heterogeneity</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-introduction.html#plan-for-this-block",
    "href": "nonparametric/nonparametric-introduction.html#plan-for-this-block",
    "title": "11  Introduction to Nonparametric Models with Unobserved Heterogeneity",
    "section": "11.3 Plan for This Block",
    "text": "11.3 Plan for This Block\nIn this block, we focus on models (11.4)-(11.5) and consider identification of some distributional features of treatment effects (11.3). Specifically,\n\nAverage treatment and marginal effects for model (11.4):\n\nShow that identifying average effects is more complicated than considering averages of the outcome directly.\nDiscuss heterogeneity bias, a form and consequence of confounding.\nShow how stationarity assumptions on \\(U_{it}\\) allow us to identify the average effects for a population of stayers — units with \\(X_{i1}=X_{i2}\\) — without any further assumptions.\nConsider two generalizations of the identification result: beyond the population of stayers and allowing some non-stationarity in the structural function.\n\nVariance of treatment and marginal effects in model (11.5): identify the variance of effects (11.3) by requiring that \\(U_{it}\\) cannot depend on future values of \\(\\bX_{it}\\)\n\nIn the next block, we also revisit model (11.4) through the lens of quantile regression.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Nonparametric Models with Unobserved Heterogeneity</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-introduction.html#a-brief-classification-of-nonparametric-models",
    "href": "nonparametric/nonparametric-introduction.html#a-brief-classification-of-nonparametric-models",
    "title": "11  Introduction to Nonparametric Models with Unobserved Heterogeneity",
    "section": "11.4 A Brief Classification of Nonparametric Models",
    "text": "11.4 A Brief Classification of Nonparametric Models\nIn these notes we primarily focus on the general and powerful models (11.4)-(11.5). However, much work has gone into analyzing other special instances of (11.1) and (11.2). Before moving on to identification of average effects, we offer a brief taxonomy of nonparametric models with unobserved heterogeneity with some essential references. We organize the literature by the types of assumptions made:\n\nHow unobserved heterogeneity enters in the system:\n\nFully non-separable models (Altonji and Matzkin 2005; Hoderlein and Mammen 2007, 2007; Chernozhukov et al. 2015).\nPartially additive separable panel models: (Chen and Ai 2003; Evdokimov and White 2012; Blundell, Horowitz, and Parey 2012; Morozov 2023).\nFully separable, additively (Henderson, Carroll, and Li 2008; Boneva, Linton, and Vogt 2015; Lee and Robinson 2015; Henderson and Soberon 2024) or multiplicatively (Beckert and Blundell 2008).\n\nWhat assumptions are imposed on the form of unobserved components: restrictions on dimension such as scalarity (Matzkin 2003; Altonji and Matzkin 2005; Imbens and Newey 2009) or no restrictions (Hoderlein and Mammen 2007, 2007; Chernozhukov et al. 2015)]\nWhat assumptions are imposed on dependence structure between observed and unobserved components: full or partial independence restrictions (Hoderlein and Mammen 2007; Imbens and Newey 2009) vs. no independence restrictions (Hoderlein and White 2012; Chernozhukov et al. 2015).\nThe nature of the outcome variable: continuous-outcome models (Chernozhukov et al. 2015) or discrete-outcome models (Manski 1987; Chesher and Rosen 2017, 2020).\n\n\n\nNext Section\nIn the next section, we begin our analysis of average effects in model (11.4) and discuss why identification is more complex that analyzing average outcomes.\n\n\n\n\nAltonji, Joseph G., and Rosa L. Matzkin. 2005. “Cross Section and Panel Data Estimators for Nonseparable Models with Endogenous Regerssors.” Econometrica 73 (4): 1053–1102. https://doi.org/10.1111/j.1468-0262.2005.00609.x.\n\n\nBeckert, Walter, and Richard Blundell. 2008. “Heterogeneity and the Non-Parametric Analysis of Consumer Choice: Conditions for Invertibility.” The Review of Economic Studies 75 (4): 1069–80. https://doi.org/10.1111/j.1467-937X.2008.00500.x.\n\n\nBlundell, Richard, Joel L. Horowitz, and Matthias Parey. 2012. “Measuring The Price Responsiveness of Gasoline Demand: Economic Shape Restrictions and Nonparametric Demand Estimation.” Quantitative Economics 3 (1): 29–51. https://doi.org/10.3982/qe91.\n\n\nBoneva, Lena, Oliver Linton, and Michael Vogt. 2015. “A Semiparametric Model for Heterogeneous Panel Data with Fixed Effects.” Journal of Econometrics, Heterogeneity in Panel Data and in Nonparametric Analysis in honor of Professor Cheng Hsiao, 188 (2): 327–45. https://doi.org/10.1016/j.jeconom.2015.03.003.\n\n\nChen, Xiaohong, and Chunrong Ai. 2003. “Efficient Estimation of Models with Conditional Moment Restrictions Containing Unknown Functions.” Econometrica 71 (6): 1795–1843.\n\n\nChernozhukov, Victor, Iván Fernández-Val, Stefan Hoderlein, Hajo Holzmann, and Whitney Newey. 2015. “Nonparametric Identification in Panels Using Quantiles.” Journal of Econometrics 188 (2): 378–92. https://doi.org/10.1016/j.jeconom.2015.03.006.\n\n\nChesher, Andrew, and Adam M. Rosen. 2017. “Generalized Instrumental Variable Models.” Econometrica 85 (3): 959–89. https://doi.org/10.3982/ecta12223.\n\n\n———. 2020. Generalized Instrumental Variable Models, Methods, and Applications. Vol. 7. Elsevier B.V. https://doi.org/10.1016/bs.hoe.2019.11.001.\n\n\nEvdokimov, Kirill, and Halbert White. 2012. “Some Extensions of a Lemma of Kotlarski.” Econometric Theory 28 (4): 925–32. https://doi.org/10.1017/S0266466611000831.\n\n\nHenderson, Daniel J., Raymond J. Carroll, and Qi Li. 2008. “Nonparametric Estimation and Testing of Fixed Effects Panel Data Models.” Journal of Econometrics 144 (1): 257–75. https://doi.org/10.1016/j.jeconom.2008.01.005.\n\n\nHenderson, Daniel J., and Alexandra Soberon. 2024. “Nonparametric Models with Fixed Effects.” In The Econometrics of Multi-Dimensional Panels: Theory and Applications, edited by Laszlo Matyas, 285–323. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-49849-7_9.\n\n\nHoderlein, Stefan, and Enno Mammen. 2007. “Identification of Marginal Effects in Nonseparable Models without Monotonicity.” Econometrica 75 (5): 1513–18. https://doi.org/10.1111/j.1468-0262.2007.00801.x.\n\n\nHoderlein, Stefan, and Halbert White. 2012. “Nonparametric Identification in Nonseparable Panel Data Models with Generalized Fixed Effects.” Journal of Econometrics 168 (2): 300–314. https://doi.org/10.1016/j.jeconom.2012.01.033.\n\n\nImbens, Guido W., and Whitney K. Newey. 2009. “Identification and Estimation of Triangular Simultaneous Equations Models Without Additivity.” Econometrica 77 (5): 1481–1512. https://doi.org/10.3982/ecta7108.\n\n\nLee, Jungyoon, and Peter M. Robinson. 2015. “Panel Nonparametric Regression with Fixed Effects.” Journal of Econometrics, Heterogeneity in Panel Data and in Nonparametric Analysis in honor of Professor Cheng Hsiao, 188 (2): 346–62. https://doi.org/10.1016/j.jeconom.2015.03.004.\n\n\nManski, Charles F. 1987. “Semiparametric Analysis of Random Effects Linear Models from Binary Panel Data.” Econometrica 55 (2): 357. https://doi.org/10.2307/1913240.\n\n\nMatzkin, Rosa L. 2003. “Nonparametric Estimation of Nonadditive Random Functions.” Econometrica 71 (5): 1339–75. https://doi.org/10.1111/1468-0262.00452.\n\n\nMorozov, Vladislav. 2023. “Estimating the Moments and the Distribution of Heterogeneous Marginal Effects Using Panel Data.”",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Introduction to Nonparametric Models with Unobserved Heterogeneity</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-heterogeneity-bias.html",
    "href": "nonparametric/nonparametric-heterogeneity-bias.html",
    "title": "12  Average Effects and Heterogeneity Bias",
    "section": "",
    "text": "12.1 Model and Object of Interest",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Average Effects and Heterogeneity Bias</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-heterogeneity-bias.html#model-and-object-of-interest",
    "href": "nonparametric/nonparametric-heterogeneity-bias.html#model-and-object-of-interest",
    "title": "12  Average Effects and Heterogeneity Bias",
    "section": "",
    "text": "12.1.1 Model\nAs noted in the previous section, we first focus our attention on the fully nonseparable panel data model (11.4): \\[\nY_{it}^{x} = \\phi(x, A_i, U_{it}), \\quad {}_{i=1, \\dots, N}^{t=1, 2},\n\\tag{12.1}\\] where \\(\\phi(x, a, u)\\) is differentiable in \\(x\\) for each possible value \\((a, u)\\).\nModel (12.1) is an extremely general nonparametric model. We make essentially no assumptions on the form of \\((A_i, U_{it})\\) or \\(\\phi\\) (beyond differentiability). In particular, \\((A_i, U_{it})\\) may be finite- or infinite-dimensional.\nTo illustrate the identification arguments clearly, we make two simplifying assumptions throughout:\n\n\\(X_{it}\\) is scalar.\nThere are only two observations per each unit.\n\nBoth of these assumptions are easy to relax at the price of more complex notation.\n\n\n12.1.2 Object of Interest\nIn studying economic models with continuous treatments or inputs, marginal effects provide interpretable measures of how units respond to small changes in covariates. Accordingly, our key object of interest is the average marginal effect: \\[\n    \\E\\left[\\partial_x Y^x_{it}|X_{it}=x, \\dots \\right] = \\E[\\partial_x \\phi(x, A_i, U_{it})|X_{it}=x, \\dots],\n\\tag{12.2}\\] where \\(x\\) is some fixed point, and the conditioning set defines some population of interest in terms of \\(X_{it}\\) and other variables. Such marginal effects are standard objects of interest in models with continuous covariates and outcomes (Hoderlein and Mammen 2007; Hoderlein and White 2012; Chernozhukov et al. 2015).\nThe parameter (12.2) has a standard causal interpretation. We consider the population of units with \\(X_{it}=x\\), along with other characteristics captured by the conditioning set. For these units, we exogenously change their \\(X_{it}\\) infinitesimally. Effect (12.2) reports the average change in outcomes for this fixed population of units.\n\n\n\n\n\n\nOne may also consider average treatment effects \\(\\E[\\partial_x \\phi(x_2, A_i, U_{it}) - \\partial_x \\phi(x_1, A_i, U_{it})|\\dots]\\). We do not discuss such effects explicitly, but all the results we derive for marginal effects have direct counterparts for average effects of non-marginal changes in \\(x\\).",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Average Effects and Heterogeneity Bias</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-heterogeneity-bias.html#heterogeneity-bias-and-issue-with-average-outcomes",
    "href": "nonparametric/nonparametric-heterogeneity-bias.html#heterogeneity-bias-and-issue-with-average-outcomes",
    "title": "12  Average Effects and Heterogeneity Bias",
    "section": "12.2 Heterogeneity Bias and Issue with Average Outcomes",
    "text": "12.2 Heterogeneity Bias and Issue with Average Outcomes\n\n12.2.1 Intuitive Approach\nOne may think that it is easy to identify effect (12.2) even from cross-sectional data. It is tempting to consider the derivative of the conditional expectation of \\(Y\\) given \\(X_{it}\\). However, as we now see, this approach fails in the presence of non-random treatment assignment — typical in observational settings.\nConsider the conditional average of \\(Y_{it}\\) for the units with \\(X_{it}=x\\): \\[\n\\begin{aligned}\n    \\E[Y_{it}|X_{it}=x]  & = \\E[\\phi(X_{it}, V_{it})|X_{it}=x]  \\\\\n    & = \\E[\\phi(x, V_{it})|X_{it}=x].\n\\end{aligned}  \n\\] where we label \\(V_{it}=(A_i, U_{it})\\) for brevity. The expectation is with respect to the conditional law of \\(V_{it}\\) given \\(X_{it}=x\\).\nOne might hope that differentiating the conditional expectation with respect to \\(x\\) recovers the average marginal effect (12.2): \\[\n\\partial_x \\E[Y_{it}|X_{it}=x] \\overset{??}{=} \\E[\\partial_x \\phi(x, V_{it})|X_{it}=x]\n\\]\n\n\n12.2.2 Heterogeneity Bias\nHowever, this reasoning fails when covariates may be correlated with unobserved factors affecting the outcome.\nTo make this intuition precise, let \\(f_{V_{it}|X_{it}}(v|x)\\) be the conditional density of \\(V_{it}\\) given \\(X_{it}=x\\) with respect to some dominating measure \\(\\mu\\) that does not depend on \\(x\\) (its existence is an assumption, but not a particularly important one right now, see section 13). With this notation, the above expectation can be written as \\[\n\\E[Y_{it}|X_{it}=x]  = \\int \\phi(x, v)f_{V_{it}|X_{it}}(v|x)\\mu(dv).\n\\] We assume that \\(f_{V_{it}|X_{it}}(v|x)\\) is differentiable in the conditioning argument \\(x\\): there is smooth dependence between \\(X_{it}\\) and \\(V_{it}\\) in a distributional sense. Intuitively, the shares of the unobserved “types” \\(v\\) very smoothly with \\(x\\).\nAssuming we can swap the integral and the derivative, we obtain the following expression for the derivative \\[\n\\begin{aligned}\n    & \\partial_x \\E[Y_{it}|X_{it}=x] \\\\\n    &  = \\int  \\partial_x \\left[\\phi(x, v)f_{V_{it}|X_{it}}(v|x) \\right]\\mu(dv)\\\\\n    & = \\int \\partial_x \\phi(x, v)f_{V_{it}|X_{it}}(v|x)\\mu(dv) + \\int  \\phi(x, v)\\partial_x f_{V_{it}X_{it}}(v|x)\\mu(dv)\\\\\n    & = \\E[\\partial_x \\phi(x, V_{it})|X_{it}=x] + \\int  \\phi(x, v)\\partial_x f_{V_{it}|X_{it}}(v|x)\\mu(dv).  \n\\end{aligned}\n\\tag{12.3}\\]\nEquation 12.3 shows that, as we change \\(x\\), the conditional expectation \\(\\E[Y_{it}|X_{it}=x]\\) changes due to two factors:\n\nChange in the function \\(\\phi\\). This term is an average marginal effect — the parameter of interest.\nChanges in the conditional distribution of \\((A_i, U_{it})\\) given \\(X_{it}\\) — the source of the second term, typically called the heterogeneity bias (Chamberlain 1982; see also Graham and Powell 2012 for a more explicit representation).\n\n\n\n12.2.3 Source of Bias and Conditions for no Bias\nSo why does the heterogeneity bias appear? Fundamentally, it arises when the treatment \\(X_{it}\\) is not assigned exogenously but is instead chosen by economic agents based on information about their potential outcomes \\(\\phi(x, A_i, U_{it})\\). For example, consumers choose products based on knowledge of their preferences \\(A_i\\) and transitory taste shocks and current prices \\(U_{it}\\). Similarly, firms decide on input levels based on firm-specific productivity and technology components encapsulated in \\((A_i, U_{it})\\).\nAs a result, units with different values of \\(X_{it}\\) may systematically differ in their unobserved characteristics. When we compute the derivative \\(\\partial_x \\E[Y_{it} | X_{it} = x]\\), we are not isolating the effect of changing \\(x\\) for a fixed unit. Instead, we are comparing different populations—those for whom \\(X_{it} = x\\) versus those for whom \\(X_{it} = x + \\varepsilon\\). Each such population potentially has a different distribution of \\((A_i, U_{it})\\).\nThis violates the ceteris paribus logic required for causal interpretation of marginal effects. In particular, the expression \\(\\partial_x \\E[Y_{it} | X_{it} = x]\\) combines the genuine marginal effect \\(\\E[\\partial_x Y_{it}^x|X_{it}=x]\\) with a compositional shift due to variation in the distribution of unobservables across \\(x\\).\nThe bias term does vanish if the conditional distribution of \\((A_i, U_{it})\\) given \\(X_{it}\\) does not vary with \\(x\\). That is, a sufficient condition for identification of average marginal effects from average outcomes is \\[\n\\partial_x f_{V_{it}|X_{it}}(v|x) = 0\n\\]\nThis condition implies that \\(X_{it}\\) is independent of \\((A_i, U_{it})\\) — a random assignment assumption reflective of a randomized controlled trial. In that case, \\(\\partial_x \\E[Y_{it} | X_{it} = x]\\) equals the average partial derivative \\(\\E[\\partial_x \\phi(x, A_i, U_{it}) | X_{it} = x]\\) and hence recovers the target marginal effect.\nIn observational data, this assumption typically fails, and the heterogeneity bias must be explicitly addressed.\n\n\n\n\n\n\nNote that it is also possible to consider the case where \\((A_i, U_{it})\\) and \\(X_{it}\\) are independent only conditionally on some additional observed variable \\(Z_{it}\\) (ignorability/unconfoundedness assumption). The results are then conditional on \\(Z_{it}\\). See Altonji and Matzkin (2005) for this flavor of results in continuous settings.\n\n\n\n\n\nNext Section\nIn the next section we show how one can actually identify average marginal effects for certain subpopulations by assuming that \\(U_{it}\\) is stationary.\n\n\n\n\nAltonji, Joseph G., and Rosa L. Matzkin. 2005. “Cross Section and Panel Data Estimators for Nonseparable Models with Endogenous Regerssors.” Econometrica 73 (4): 1053–1102. https://doi.org/10.1111/j.1468-0262.2005.00609.x.\n\n\nChamberlain, Gary. 1982. “Multivariate Regression Models for Panel Data.” Journal of Econometrics 18 (1): 5–46. https://doi.org/10.1016/0304-4076(82)90094-X.\n\n\nChernozhukov, Victor, Iván Fernández-Val, Stefan Hoderlein, Hajo Holzmann, and Whitney Newey. 2015. “Nonparametric Identification in Panels Using Quantiles.” Journal of Econometrics 188 (2): 378–92. https://doi.org/10.1016/j.jeconom.2015.03.006.\n\n\nGraham, Bryan S, and James L Powell. 2012. “Identification and Estimation of Average Partial Effects in \"Irregular\" Correlated Random Coefficient Panel Data Models.” Econometrica 80 (5): 2105–52. https://doi.org/10.3982/ecta8220.\n\n\nHoderlein, Stefan, and Enno Mammen. 2007. “Identification of Marginal Effects in Nonseparable Models without Monotonicity.” Econometrica 75 (5): 1513–18. https://doi.org/10.1111/j.1468-0262.2007.00801.x.\n\n\nHoderlein, Stefan, and Halbert White. 2012. “Nonparametric Identification in Nonseparable Panel Data Models with Generalized Fixed Effects.” Journal of Econometrics 168 (2): 300–314. https://doi.org/10.1016/j.jeconom.2012.01.033.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Average Effects and Heterogeneity Bias</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-average-identification.html",
    "href": "nonparametric/nonparametric-average-identification.html",
    "title": "13  Nonparametric Identification of Average Effects",
    "section": "",
    "text": "13.1 A Finite Difference Perspective on Heterogeneity Bias\nThe previous section highlighted how heterogeneity bias prevents identification of causal marginal effects using cross-sectional averages. We now show how panel data, together with smoothness and stationarity assumptions, can overcome this issue.\nTo motivate our identification strategy for average marginal effects, it is helpful to recall the definition of a derivative. Recall that the derivative of a function \\(g\\) is just the limit of a sequence of finite differences: \\[\n    g'(x) = \\lim\\limits_{h\\to 0}\\dfrac{g(x+h)-g(x)}{h}.\n\\]\nIf we take \\(g(x)=\\E[Y_{it}|X_{it}=x]\\), the above finite difference is given by \\[\n\\begin{aligned}\n    & \\dfrac{\\E[Y_{it}|X_{it} = x+h] - \\E[Y_{it} |X_{it}=x] }{h} \\\\\n    & =  \\dfrac{\\E[\\phi(x+h, A_i, U_{it})|X_{it}=x+h] - \\E[\\phi(x,  A_i, U_{it})|X_{it}=x] }{h}.\n\\end{aligned}\n\\tag{13.1}\\] Notice that the conditioning sets on the two expectations are different. Consequently, the conditional distributions of \\((A_i, U_{it})\\) given \\(X_{it}\\) are different between the two expectations. In other words, we cannot hold the distribution of \\((A_i, U_{it})\\) fixed as we vary \\(x\\) in \\(\\E[Y_{it}|X_{it}]\\). This violates the ceteris paribus logic required for interpreting \\(\\partial_x \\E[Y_{it} | X_{it} = x]\\) as a causal effect, as mentioned in the previous section.\nThe average marginal effect can itself be restated using a finite difference as \\[\n\\begin{aligned}\n& \\E[\\partial_x Y_{it}^x|\\cdots]\\\\\n& = \\E[\\partial_x\\phi(x+h, A_i, U_{it}|\\cdots] \\\\\n&  = \\E\\left[\\lim_{h\\to 0}  \\dfrac{ \\phi(x+h, A_i, U_{it}) - \\phi(x, A_i, U_{it})    }{h} \\Bigg| \\cdots \\right].\n\\end{aligned}\n\\tag{13.2}\\] In this view, the failure of the naive approach of the previous section stems from the fact that we cannot interchange finite differences and expectations when using cross-sectional data in Equation 13.1.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Identification of Average Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-average-identification.html#identifying-average-marginal-effects-for-stayers",
    "href": "nonparametric/nonparametric-average-identification.html#identifying-average-marginal-effects-for-stayers",
    "title": "13  Nonparametric Identification of Average Effects",
    "section": "13.2 Identifying Average Marginal Effects for Stayers",
    "text": "13.2 Identifying Average Marginal Effects for Stayers\n\n13.2.1 Stationarity: Time as an Instrument\nPanel data offers a natural way to address this failure of ceteris paribus reasoning. By comparing outcomes within the same unit, we can hold \\(A_i\\) constant.\nAt the same time, contrasting outcomes within the same unit necessarily means contrasting outcomes from different periods — and hence dealing with variation in \\(U_{it}\\).\nAs it turns out, to handle such comparisons it is sufficient to assume that \\(U_{it}\\) is conditionally stationary. Formally, we assume that the distribution \\(U_{i1}\\) conditional on \\((X_{i1}, X_{i2}, A_i)\\) is equal to the conditional distribution of \\(U_{i2}\\): \\[\nU_{i1}|(X_{i1}, X_{i2}, A_i) \\overset{d}{=} U_{i2}|(X_{i1}, X_{i2}, A_i) .\n\\tag{13.3}\\] Under this assumption, the distribution of the potential outcomes \\(Y_{it}^x=\\phi(x, A_i, U_{it})\\) does not depend on time \\(t\\) any more given \\((X_{i1}, X_{i2})\\). Intuitively, one may think that time is randomly assigned to observations; an interpretation called “time as an instrument” (Chernozhukov et al. 2013). The variation in \\(U_{it}\\) over time becomes non-systemic and we can use the time dimension to isolate causal effects.\n\n\n\n\n\n\nThe stationarity assumption actually imposes a time-invariance assumption on the structural function \\(\\phi\\). To see why, note that stationarity means that \\(U_{it}\\) cannot contain changing deterministic variables, including time \\(t\\). In general, if \\(t\\) is part of \\(U_{it}\\), then the function could change arbitrarily between periods. There is effectively no value in panel data if such arbitrary changes are possible. The stationarity assumption rules out such situations.\n\n\n\n\n\n13.2.2 Two Key Analysis Steps\nWe base our panel data identification argument on the following expectations involving \\(\\phi\\) \\[\n    \\E\\left[ \\dfrac{\\phi(x+h, A_i, U_{i2}) - \\phi(x, A_i, U_{i1})}{h}\\Bigg|\\cdots \\right].\n\\tag{13.4}\\] Expectation (13.4) reflects the idea sketched out above:\n\nThe same \\(A_i\\) appears in both \\(\\phi\\) terms.\nThe \\(U_{it}\\) terms change over time.\n\nThe latter point means that (13.4) does not involve a genuine finite difference in contrast to Equation 13.2. As a consequence, there are two key conceptual pieces in our analysis:\n\nConvergence: showing that expectation (13.4) does converge to the average marginal effect as \\(h\\to 0\\), despite not involving the “correct” finite difference.\nIdentification: showing that the limit of expectation (13.4) is identified as \\(h\\to 0\\), at least for some subpopulation of interest.\n\n\n\n13.2.3 Identifying Moments of Finite Difference\nWe begin with the identification step, showing how the expression in (13.4) maps onto observable quantities. To identify the limit of expectation (13.4) as \\(h\\to 0\\), it is sufficient to identify (13.4) for all \\(h&gt;0\\) in some neighborhood of 0. To do so, consider the population of units with \\(\\curl{ X_{i1} = x, X_{i2}  = x+h}\\). By model (12.1), the realized outcomes of these units satisfy \\[\n\\begin{aligned}\n    Y_{i2} & = \\phi(x+h, A_i, U_{i2}),\\\\\n    Y_{i1} & = \\phi(x, A_i, U_{i1}).\n\\end{aligned}\n\\] Accordingly, \\[\n\\begin{aligned}\n    &   \\E\\left[\\dfrac{Y_{i2}-Y_{i1}}{h}\\Bigg|X_{i1} = x, X_{i2} = x+h \\right] \\\\\n    & = \\E\\left[\\dfrac{\\phi(x+h, A_i, U_{i2}) - \\phi(x, A_i, U_{i1})}{h}\\Bigg|X_{i1} = x, X_{i2} = x+h  \\right].\n\\end{aligned}\n\\tag{13.5}\\] We conclude that (13.4) is identified for all \\(h&gt;0\\) small enough, provided that the joint density of \\((X_{i1}, X_{i2})\\) is positive in some neighborhood of \\((x, x)\\),\nThis shows that we can observe the difference in outcomes directly in panel data for near-stayers — units whose covariates change only slightly over time (\\(X_{i1}=x, X_{i2}=x+h\\) for small \\(h&gt;0\\); see Sasaki and Ura (2021) and the references cited therein). It also determines the population for which we are able to identify the parameter of interest. We return to this topic below.\n\n\n13.2.4 Average Differences and Almost Marginal Effects\nWith identification of (13.4) in hand, we now analyze the behavior of this expression as \\(h\\to 0\\) to ensure that it converges to a marginal effect. Establishing convergence requires some continuity assumptions on the relationship between \\(X_{it}\\), \\(A_i\\) and \\(U_{it}\\). To motivate their form, we write out (13.4) as an integral, similarly to how we proceeded in the previous section. To that end, define the following conditional densities:\n\n\\(f_{A_i, U_{i1}, U_{i2}|X_{i1}, X_{i2}}(a, u_1, u_2|x_1, x_2)\\) be the conditional density of \\((A_i, U_{i1}, U_{i2})\\) given \\(\\curl{X_{i1} =x_1, X_{i2}=x_2}\\);\n\\(f_{U_{i1}, U_{i2}|X_{i1}, X_{i2}, A_i}(u_1, u_2|x_1, x_2, a)\\) be the conditional density of \\((U_{i1}, U_{i2})\\) given \\(\\curl{X_{i1} =x_1, X_{i2}=x_2, A_i=a}\\);\n\\(f_{A_i|X_{i1}, X_{i2}}(a|x_1, x_2)\\) be the conditional density of \\(A_i\\) given \\(\\curl{X_{i1}=x_1, X_{i2}=x_2}\\)\n\\(f_{U_{it}|X_{i1}, X_{i2}, A_i}(u|x_1, x_2, a)\\) be the conditional density of \\(U_{it}\\) given \\(\\curl{X_{i1} =x_1, X_{i2}=x_2, A_i=a}\\).\n\\(f_{A_i, U_{it}|X_{i1}, X_{i2}}(a, u|x_1, x_2)\\) be the conditional density of \\((A_i, U_{it})\\) given \\(\\curl{X_{i1} =x_1, X_{i2}=x_2}\\).\n\nObserve that the latter two densities do not depend on \\(t\\) by assumption of stationarity! This property is crucial to reducing (13.4) to an integral of a finite difference in \\(\\phi\\).\nThroughout, we assume that the above densities are taken with respect to some overall dominating measure \\(\\mu\\) that does not depend on \\((x_1, x_2)\\).\n\n\n\n\n\n\nRegarding the existence of \\(\\mu\\)\n\n\n\n\n\nIntuitively, the assumption that \\(\\mu\\) does not depend on \\((x_1, x_2)\\) means that the type of distribution of \\((A_i, U_{it})\\) does not depend with \\((x_1, x_2)\\). In the simplest case, imagine that \\(A_i\\) and \\(U_{it}\\) are just random scalars. The assumption states that the following two statements cannot be true at the same time:\n\nFor some \\((x_1, x_2)\\) there is only a finite number of possible values of \\((A_i, U_{it})\\) (with \\(\\mu\\) being the counting measure).\nFor some \\((x_1, x_2)\\) there is a continuum of possible values of \\((A_i, U_{it})\\) with no atoms (with \\(\\mu\\) being the Lebesgue measure).\n\n\n\n\nWith this notation, we can represent (13.4) as \\[\n\\begin{aligned}\n    &   \\E\\left[\\dfrac{\\phi(x+h, A_i, U_{i2}) - \\phi(x, A_i, U_{i1})}{h}\\Bigg|X_{i1} = x, X_{i2} = x+h  \\right]\\\\\n    %\n    & = \\int \\left[\\phi(x+h, a, u_2) - \\phi(x, a, u_1) \\right] \\\\\n    & \\hspace{2cm}\\times f_{A_i, U_{i1}, U_{i2}|X_{i1}, X_{i2}}(a, u_1, u_2|x,x+h) \\mu(da, du_1, du_2)\\\\\n    %\n    & = \\int \\left[\\phi(x+h, a, u_2) - \\phi(x, a, u_1) \\right]\n    \\\\\n    & \\hspace{2cm}\\times   f_{U_{i1}, U_{i2}|X_{i1}, X_{i2}, A_i}(u_1, u_2|x, x+h, a)\n        \\\\\n    & \\hspace{2cm}\\times f_{A_i|X_{i1}, X_{i2}}(a|x, x+h) \\mu(da, du_1, du_2)\\\\\n    %\n    %       & = \\int \\left[\\phi(x+h, a, u_2) - \\phi(x, a, u_1) \\right]\n    % \\\\\n    % & \\hspace{2cm}\\times   f_{U_{i1}, U_{i2}|X_{i1}, X_{i2}, A_i}(u_1, u_2|x, x+h, a)\n    %       \\\\\n    % & \\hspace{2cm}\\times f_{A_i|X_{i1}, X_{i2}}(a|x, x+h) \\mu(da, du_1, du_2).\n\\end{aligned}\n\\] Now we make four key observations:\n\nWe can split the integral into two integrals: one involving \\(\\phi(x+h, a, u_2)\\) and one involving \\(\\phi(x, a, u_1)\\).\n\\(\\phi(x+h, a, u_2)\\) does not depend on \\(u_1\\), and so \\(u_{1}\\) is just integrated out: \\[\n\\begin{aligned}\n& \\int \\phi(x+h, a, u_2)  f_{A_i|X_{i1}, X_{i2}}(a|x, x+h)\n\\\\\n& \\hspace{2cm}\\times   f_{U_{i1}, U_{i2}|X_{i1}, X_{i2}, A_i}(u_1, u_2|x, x+h, a) \\mu(da, du_1, du_2)\n     \\\\\n& = \\int  \\phi(x+h, a, u_2) f_{A_i|X_{i1}, X_{i2}}(a|x, x+h)\n\\\\\n& \\hspace{2cm}\\times   f_{U_{i2}|X_{i1}, X_{i2}, A_i}(u_2|x, x+h, a) \\mu(da, du_2).\n\\end{aligned}\n\\]\nThe conditional density of \\(U_{i2}\\) is equal to the time-invariant density \\(f_{U_{it}|X_{i1}, X_{i2}, A_i}\\): \\[\n\\begin{aligned}\n& = \\int  \\phi(x+h, a, u_2) f_{A_i|X_{i1}, X_{i2}}(a|x, x+h)\n\\\\\n& \\hspace{2cm}\\times   f_{U_{i2}|X_{i1}, X_{i2}, A_i}(u_2|x, x+h, a) \\mu(da, du_2)\\\\\n& = \\int  \\phi(x+h, a, u) f_{A_i|X_{i1}, X_{i2}}(a|x, x+h)\n\\\\\n& \\hspace{2cm}\\times   f_{U_{it}|X_{i1}, X_{i2}, A_i}(u|x, x+h, a) \\mu(da, du).\n\\end{aligned}\n\\]\nA symmetric argument applies to the integral with \\(\\phi(x, a, u_1)\\).\n\nWe conclude that we can represent (13.4) as \\[\n\\begin{aligned}\n&   \\E\\left[\\dfrac{\\phi(x+h, A_i, U_{i2}) - \\phi(x, A_i, U_{i1})}{h}\\Bigg|X_{i1} = x, X_{i2} = x+h  \\right]\\\\\n& = \\int \\left[ \\phi(x+h, a, u)  - \\phi(x, a, u) \\right]f_{A_i, U_{it}|X_{i1}, X_{i2}, A_i}(a, u|x, x+h) \\mu(da, du)\n\\end{aligned}\n\\]\nThe achievement of the above representation is that we managed to obtain a genuine finite difference involving \\(\\phi\\)! Only \\(x\\) changes under \\(\\phi\\), while the same \\(a\\) and \\(u\\) appear in both terms. We can now apply the mean value theorem to this finite difference as \\[\n\\phi(x+h, a, u) = \\phi(x, a, u) + h\\partial_x\\phi(\\tilde{x}, a, u),\n\\tag{13.6}\\] where \\(\\tilde{x}\\) is a point between \\(x\\) and \\(x+h\\); \\(\\tilde{x}\\) possibly depends on \\(a\\) and \\(u\\).\nSubstituting Equation 13.6 into the above representation for (13.4) we conclude that \\[\n\\begin{aligned}\n& \\E\\left[\\dfrac{\\phi(x+h, A_i, U_{i2}) - \\phi(x, A_i, U_{i1})}{h}\\Bigg|X_{i1} = x, X_{i2} = x+h  \\right]\\\\\n& = \\int \\partial_x\\phi(\\tilde{x}, a, u)f_{A_i, U_{it}|X_{i1}, X_{i2}}(a, u|x, x+h) \\mu(da, du).\n\\end{aligned}\n\\tag{13.7}\\]\n\n\n13.2.5 Distributional Continuity in Unobservables\nIn expectation form, we can write the integral as \\[\n\\E[\\partial_x \\phi(\\tilde{x}, A_i, U_{it})|X_{i1}=x, X_{i2}=x+h].\n\\] This object is not quite an average marginal effect — the function \\(\\phi\\) is evaluated at a potentially random point \\(\\tilde{x}=\\tilde{x}(A_i, U_{it})\\).\nTo resolve this issue, we need to enforce \\(\\tilde{x} = x\\). Since \\(\\tilde{x} \\in [x-h, x+h]\\), this result can be achieved by taking \\(h\\to 0\\) in Equation 13.7, while preserving the expectation interpretation of the result.\nTo ensure this limiting behavior is well-defined, we impose our second key assumption to allow taking \\(h\\to 0\\). We assume that the density \\(f_{A_i, U_{it}|X_{i1}, X_{i2}}(a, u|x_1, x_2)\\) is continuous in \\((x_1, x_2)\\). In words, we assume that the distribution of unobserved components varies smoothly as we vary the treatments.\nWith this assumption, the integrand in Equation 13.7 converges as \\(h\\to 0\\) \\[\n\\begin{aligned}\n& \\partial_x\\phi(\\tilde{x}, a, u)f_{A_i, U_{it}|X_{i1}, X_{i2}}(a, u|x, x+h) \\\\\n& \\to \\partial_x\\phi(x, a, u)f_{A_i, U_{it}|X_{i1}, X_{i2}}(a, u|x, x).\n\\end{aligned}\n\\] Under mild dominance conditions, this convergence also applies to the integrals: \\[\n\\begin{aligned}\n& \\int \\partial_x\\phi(\\tilde{x}, a, u)f_{A_i, U_{it}|X_{i1}, X_{i2}}(a, u|x, x+h) \\mu(da, du) \\\\\n& \\to \\int \\partial_x\\phi(x, a, u)f_{A_i, U_{it}|X_{i1}, X_{i2}}(a, u|x, x) \\\\\n& = \\E\\left[ \\partial_x\\phi(x, a, u)|X_{i1}=X_{i2}=x \\right],\n\\end{aligned}\n\\tag{13.8}\\] finally yielding an average marginal effect.\n\n\n13.2.6 Result Statement\nCombining equations (13.5), (13.7), and (13.8), we obtain our identification result: \\[\n\\begin{aligned}\n& \\E[\\partial_x Y_{it}^x | X_{i1} = X_{i2}= x]\\\\\n& = \\E\\left[ \\partial_x\\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x \\right] \\\\\n& = \\lim_{h\\to 0} \\left[ \\dfrac{Y_{i2}-Y_{i1}}{h}\\Bigg|X_{i1}=x, X_{i2} = x+h \\right].\n\\end{aligned}\n\\tag{13.9}\\] This result, derived by Hoderlein and White (2012) and Chernozhukov et al. (2015), establishes nonparametric identification of average marginal effects for stayers at x — units with identical values of \\(X_{i1}\\) and \\(X_{i2}\\).\nTo obtain this result, we relied on two key assumptions:\n\nConditional stationarity of \\(U_{it}\\): used to convert expected change in outcomes into an expected finite difference in \\(\\phi\\)\nContinuity of the conditional distribution of \\((A_i, U_{it})\\) in the conditioning variables.\n\nWhat makes Equation 13.9 especially powerful is that it is compatible with potentially infinite-dimensional unobserved heterogeneity without independence assumptions. We can nonparametrically identify meaningful causal parameters without even needing to know the dimension and the form of the unobserved components \\((A_i, U_{it})\\). Moreover, we did not restrict the dependence between \\(X_{it}\\) and \\((A_{i}, U_{it})\\).\nThis is a remarkably flexible framework. For example,\n\nIn a consumption context, individuals may have arbitrarily different preferences.\n\nIn a production setting, firms may differ in their entire technology sets.\n\nAs long as there is smoothness in the distribution of unobservables and comparability over time (i.e., conditional stationarity), the average marginal effect for stayers is identified.\nIn the next section, we refine Equation 13.9 to provide an even more explicit identification result and discuss estimation.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Identification of Average Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-average-identification.html#interpretation-and-stayers",
    "href": "nonparametric/nonparametric-average-identification.html#interpretation-and-stayers",
    "title": "13  Nonparametric Identification of Average Effects",
    "section": "13.3 Interpretation and Stayers",
    "text": "13.3 Interpretation and Stayers\nBefore proceeding, we briefly discuss the population for which Equation 13.9 identifies marginal effects. Stayers represent an important subpopulation in nonparametric panel data analysis, dating back to the seminal work of Chamberlain (1982).\nStayers matter for two main reasons, one positive, one negative:\n\nEmpirical relevance: In many microdata settings, stayers and near-stayers comprise a substantial share or an outright majority of the population. See, for example, the evidence and discussion in Sasaki and Ura (2021).\nTheoretical necessity: Stayers are typically the only population for which identification is possible without meaningful restrictions on the dependence between \\(X_{it}\\) and \\((A_i, U_{it})\\). As shown by the constructive counterexamples in Cooprider, Hoderlein, and Meister (2022), identification generally fails for all distributional features of marginal effects for non-stayers in models (11.4) and (11.5).\n\n\n\nNext Section\nIn the next section, we refine Equation 13.9 into a limit-free expression and discuss practical estimation of the average marginal effect.\n\n\n\n\nChamberlain, Gary. 1982. “Multivariate Regression Models for Panel Data.” Journal of Econometrics 18 (1): 5–46. https://doi.org/10.1016/0304-4076(82)90094-X.\n\n\nChernozhukov, Victor, Iván Fernández-Val, Jinyong Hahn, and Whitney K. Newey. 2013. “Average and Quantile Effects in Nonseparable Panel Models.” Econometrica 81 (2): 535–80. https://doi.org/10.3982/ecta8405.\n\n\nChernozhukov, Victor, Iván Fernández-Val, Stefan Hoderlein, Hajo Holzmann, and Whitney Newey. 2015. “Nonparametric Identification in Panels Using Quantiles.” Journal of Econometrics 188 (2): 378–92. https://doi.org/10.1016/j.jeconom.2015.03.006.\n\n\nCooprider, Joseph, Stefan Hoderlein, and Alexander Meister. 2022. “A Panel Data Estimator for the Distribution and Quantiles of Marginal Effects in Nonlinear Structural Models with an Application to the Demand for Junk Food.” https://doi.org/10.2139/ssrn.3545485.\n\n\nHoderlein, Stefan, and Halbert White. 2012. “Nonparametric Identification in Nonseparable Panel Data Models with Generalized Fixed Effects.” Journal of Econometrics 168 (2): 300–314. https://doi.org/10.1016/j.jeconom.2012.01.033.\n\n\nSasaki, Yuya, and Takuya Ura. 2021. “Slow Movers in Panel Data,” 1–40. https://arxiv.org/abs/2110.12041.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Identification of Average Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-average-estimation.html",
    "href": "nonparametric/nonparametric-average-estimation.html",
    "title": "14  Nonparametric Estimation of Average Effects",
    "section": "",
    "text": "14.1 An Estimable Form of the Average Marginal Effect\nIn the previous section, we have identified the average marginal effect for stayers in model (11.4) in terms of a limit of average change in outcomes across periods (Equation 13.9): \\[\n\\begin{aligned}\n& \\E\\left[ \\partial_x\\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x \\right] \\\\\n& = \\lim_{h\\to 0} \\left[ \\dfrac{Y_{i2}-Y_{i1}}{h}\\Bigg|X_{i1}=x, X_{i2} = x+h \\right].\n\\end{aligned}\n\\] This representation is convenient for identification and explicitly shows that the source of identification is the within variation of near-stayers.\nHowever, the limit-based representation poses challenges for estimation, which would benefit from a more explicit representation. To provide such a representation, we define the following function: \\[\n    g(x_1, x_2) = \\E\\left[Y_{i2}- Y_{i1}|X_{i1}=x_1, X_{i2} = x_2 \\right] .\n\\] This function captures how the average difference in outcomes depends on the treatment values in both periods. \\(g(\\cdot, \\cdot)\\) plays a central role in constructing our estimator.\nObserve that \\(g(x, x) = 0\\), since \\[\n\\begin{aligned}\ng(x, x)  & = \\E\\left[Y_{i2}- Y_{i1}|X_{i1}=X_{i2} = x \\right] \\\\\n& = \\E\\left[\\phi(x, A_i, U_{i2}) - \\phi(x, A_i, U_{i1})|X_{i1}=X_{i2} = x \\right]\\\\\n& = \\E\\left[\\phi(x, A_i, U_{it}) - \\phi(x, A_i, U_{it})|X_{i1}=X_{i2} = x \\right]\\\\\n& = 0,\n\\end{aligned}\n\\] where we use the stationarity of \\(U_{it}\\) (and hence of \\(\\phi(x, A_i, U_{it})\\)) in the last two lines.\nThus, by the definition of partial derivatives it holds that \\[\n\\begin{aligned}\n& \\E\\left[\\dfrac{Y_{i2}-Y_{i1}}{h}\\Bigg|X_{i1}=x, X_{i2}= x+h \\right] \\\\\n& = \\dfrac{g(x, x+h)}{h} = \\dfrac{g(x, x+h) - g(x, x)}{h} \\\\\n& \\to \\partial_{x_2} g(x, x),\n\\end{aligned}\n\\tag{14.1}\\] where \\(\\partial_{x_2}\\) means the partial derivative with respect to the second argument.\nBy combining Equation 14.1 with Equation 13.9, we obtain the following representation for the average marginal effects of stayers: \\[\n\\begin{aligned}\n    & \\E[\\partial_x\\phi (x, A_i, U_{it})|X_{i1}=X_{i2}=x] \\\\\n    & = \\partial_{x_2} \\E\\left[Y_{i2}- Y_{i1}|X_{i1}=x_1, X_{i2} = x_2 \\right]\\Big|_{(x_1, x_2)=(x, x)}.\n\\end{aligned}\n\\tag{14.2}\\]\nThe practical importance of Equation 14.2 is that it reduces the problem of estimating average marginal effects to the problem of estimating a derivative of a conditional function — a standard nonparametric regression problem.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Nonparametric Estimation of Average Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-average-estimation.html#a-primer-on-multivariate-local-polynomial-estimation",
    "href": "nonparametric/nonparametric-average-estimation.html#a-primer-on-multivariate-local-polynomial-estimation",
    "title": "14  Nonparametric Estimation of Average Effects",
    "section": "14.2 A Primer on Multivariate Local Polynomial Estimation",
    "text": "14.2 A Primer on Multivariate Local Polynomial Estimation\n\n14.2.1 Idea\nIn principle, many nonparametric regression methods can estimate derivatives. Of these, local polynomial (LP) regression provides a particularly convenient approach. LP estimators are available in closed form, can estimate derivatives directly, and have favorable asymptotic properties.\nWe now provide the essential idea for a local quadratic estimator for a bivariate regression function. However, it is straightforward to extend the argument to any higher order polynomial and any number of variables. See 2.5.2 in Li and Racine (2007) and Fan and Gijbels (1996) for some standard references.\nTo motivate the approach, let \\(g(\\cdot, \\cdot)\\) be a smooth function of two variables, and suppose that we see observations \\((Y_i, X_{i1}, X_{i2})\\) such that \\[\n    \\E[Y_i|X_{i1}, X_{i2}] =  g(X_{i1}, X_{i2}), \\quad i=1, \\dots, N.\n\\] Let \\((x_1, x_2)\\) be some fixed point.\nThe bivariate Taylor’s theorem tell us that, if \\((X_{i1}, X_{i2})\\) is “close” to \\((x_1, x_2)\\), then (expanding up to the second order) \\[\n\\begin{aligned}\n    &   g(X_{i1}, X_{i2}) \\\\\n    &  \\approx g(x_1, x_2) \\\\\n    & \\quad + \\partial_{x_1} g(x_1, x_2) (X_{i1}-x_1) + \\partial_{x_2} g(x_1, x_2)(X_{i2} - x_2) \\\\\n    & \\quad + \\dfrac{1}{2} \\partial_{x_1}^2 g(x_1, x_2)(X_{i1}-x_1)^2 +  \\dfrac{1}{2} \\partial_{x_2}^2 g(x_1, x_2)(X_{i2}-x_2)^2 \\\\\n    & \\quad  + \\partial_{x_1}\\partial_{x_2} g(x_1, x_2) (X_{i1}-x_1)(X_{i2}-x_2) \\\\\n    & = \\bZ_i(x_1, x_2)'\\bbeta(x_1, x_2)\n\\end{aligned}\n\\] where \\[\n\\begin{aligned}\n    \\bZ_i(x_1, x_2) & = \\begin{pmatrix}\n        1\\\\\n        X_{i1}- x_1\\\\\n        X_{i2} - x_2\\\\\n        (X_{i1}-x_1)^2/2\\\\\n        (X_{i1}- x_1)(X_{i2}-x_2) \\\\\n        (X_{i2} - x_2)^2/2\n    \\end{pmatrix}, \\\\\n     \\bbeta(x_1, x_2) & = \\begin{pmatrix}\n        g(x_1, x_2)\\\\\n        \\partial_{x_1} g(x_1, x_2)\\\\\n        \\partial_{x_2} g(x_1, x_2)\\\\\n        \\partial^2_{x_1} g(x_1, x_2)\\\\\n        \\partial_{x_1}\\partial_{x_2} g(x_1, x_2)\\\\\n        \\partial^2_{x_2} g(x_1, x_2)\n    \\end{pmatrix}\n\\end{aligned}\n\\] This expansion suggests that we can estimate the leading derivatives \\(\\bbeta(x_1, x_2)\\) by regressing \\(Y_i\\) on \\(\\bZ_i(x_1, x_2)\\) using weighted least squares. Observations which are closer to the target point \\((x_1, x_2)\\) should receive a higher weight.\n\n\n14.2.2 Estimator\nWe formalize the idea of closeness using a bivariate kernel function \\(K(\\cdot, \\cdot)\\) that measures the distance between data points and the target point \\((x_1, x_2)\\). In principle, we can take any function \\(K\\) that satisfies the following properties: \\[\n\\begin{aligned}\nK(u_1, u_2) & \\geq 0,  \\\\\n\\iint K(u_1, u_2)du_1du_2 & =1, \\\\\n  \\int u_j K(u_1, u_2)du_j & =0.\n\\end{aligned}\n\\] A common approach is to take \\(K\\) to be a product of some univariate density functions \\(K_1\\): \\[\nK(x_1, x_2) = K_1(x_1) K(x_2),\n\\] where \\(K_1\\) may be the probability density function of the standard Gaussian distribution, for example.\nTo estimate the derivative coefficients in \\(\\bbeta(x_1, x_2)\\), we perform a weighted least squares regression of \\(Y_i\\) on the “covariates”/basis functions \\(\\bZ_i(x_1, x_2)\\), using kernel weights that reflect proximity to the target point. The resulting estimator is: \\[\n\\begin{aligned}\n    \\hat{\\bbeta}(x_1, x_2) &  = \\left( \\sum_{i=1}^N K\\left(\\dfrac{X_{i1}-x_1}{s}, \\dfrac{X_{i2}-x_2}{s} \\right) \\bZ_i(x_1, x_2)\\bZ_i(x_1, x_2)'\\right)^{-1}\\\\\n    & \\hspace{0.9cm} \\times \\sum_{i=1}^N K\\left(\\dfrac{X_{i1}-x_1}{s}, \\dfrac{X_{i2}-x_2}{s} \\right)  \\bZ_i(x_1, x_2) Y_i,\n\\end{aligned}\n\\tag{14.3}\\] where \\(s&gt;0\\) is the smoothing parameter (bandwidth). As usual with kernel estimators, larger values of \\(s\\) correspond to stronger smoothing — the estimator considers points in a larger neighborhood of \\((x_1, x_2)\\).\nOne may generalize the approach to consider local polynomials of general order \\(p\\). Taking higher \\(p\\) permits estimation of higher-order derivatives.\n\n\n\n\n\n\nWhich order of local polynomials should one use? The standard advice is to take the degree \\(p\\) of the polynomial to be one higher than the highest derivatives of interest. For example, if we are interested in the regression function itself (zeroth derivative), it is most common to use local linear regression (first degree polynomial). In our case, we are interested in the first derivative. According to this rule, we should run local quadratic regression, as described above.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Nonparametric Estimation of Average Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-average-estimation.html#estimating-average-marginal-effects",
    "href": "nonparametric/nonparametric-average-estimation.html#estimating-average-marginal-effects",
    "title": "14  Nonparametric Estimation of Average Effects",
    "section": "14.3 Estimating Average Marginal Effects",
    "text": "14.3 Estimating Average Marginal Effects\n\n14.3.1 Estimator\nEquipped with the local polynomial estimator, we can now estimate the average marginal effect of stayers. To do so, we use \\(Y_{i2}-Y_{i1}\\) as the dependent variable in Equation 14.3 and select the target point as \\((x_1, x_2)=(x, x)\\). The third coordinate of \\(\\hat{\\bbeta}(x, x)\\) is an estimator for the partial derivative of \\(\\E[Y_{i2}-Y_{i1}|X_{i1}=x_1, X_{i2}=x_2]\\) with respect to \\(x_2\\), evaluated at \\((x, x)\\) — precisely the average marginal effect of interest by Equation 14.2: \\[\n\\widehat{ \\E}\\left[ \\partial_x\\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x \\right] = \\hat{\\bbeta}_3(x, x).\n\\]\n\n\n14.3.2 Asymptotic Properties\nOur estimator inherits all the desirable properties of local polynomial estimators, including consistency and asymptotic normality (see theorem 2.10 in Li and Racine 2007; Masry 1996a, 1996b). In particular, by suitably undersmoothing the local polynomial estimator (taking \\(s\\) smaller than the MSE-optimal value), one can conduct inference on the target average effect of interest.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Nonparametric Estimation of Average Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-average-estimation.html#another-estimator",
    "href": "nonparametric/nonparametric-average-estimation.html#another-estimator",
    "title": "14  Nonparametric Estimation of Average Effects",
    "section": "14.4 Another Estimator",
    "text": "14.4 Another Estimator\nBefore we move on to generalizations of the identification result, it is worth discussing Equation 14.2 somewhat more. The preceding identification and estimation arguments were based on the one-sided finite difference \\((\\phi(x+h, a, u)-\\phi(x, a, u))\\). While the one-sided finite difference yields a valid estimator, symmetric (central) differences often provide more accurate approximations to derivatives..\nBy suitably adjusting the above argument, one can indeed obtain an identification argument based on central differences: \\[\n\\begin{aligned}\n    & \\E[\\partial_x \\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x]\n    \\\\\n    & = \\dfrac{1}{2}  \\dfrac{d}{dh}\\Big(  \\E\\left[Y_{i2}- Y_{i1}|X_{i1}=x-h,  X_{i2} = x+h \\right]\\Big)\\Big|_{h=0}\\\\\n    & = \\dfrac{1}{2}\\Bigg(  \\partial_{x_2}  \\E[Y_{i2}-Y_{i1}|X_{i1}=x_1, X_{i2}=x_2] \\\\\n    & \\hspace{2cm} -\\partial_{x_1}  \\E[Y_{i2}-Y_{i1}|X_{i1}=x_1, X_{i2}=x_2]     \\Bigg)\\Bigg|_{(x_1, x_2)=(x, x)}.\n\\end{aligned}\n\\] A symmetric estimator would then be based on the difference based on the difference between the estimated partial derivatives with respect to \\(x_2\\) and \\(x_1\\), i.e., the third and second elements of \\(\\hat{\\bbeta}(x, x)\\).\n\n\nNext Section\nIn the next section, we somewhat relax the stationarity assumption on the model by allowing location-scale shifts in the structural function.\n\n\n\n\nFan, Jianqing, and Irene Gijbels. 1996. Local Polynomial Modelling and Its Applications. Springer New York.\n\n\nLi, Qi, and Jeffrey Scott Racine. 2007. Nonparametric Econometrics: Theory and Practice. Princeton University Press.\n\n\nMasry, Elias. 1996a. “Multivariate Local Polynomial Regression for Time Series: Uniform Strong Consistency and Rates.” Journal of Time Series Analysis 17 (6): 571–99. https://doi.org/10.1111/j.1467-9892.1996.tb00294.x.\n\n\n———. 1996b. “Multivariate Regression Estimation: Local Polynomial Fitting for Time Series.” Stochastic Processes and Their Applications 65 (1): 3575–81. https://doi.org/10.1016/S0304-4149(96)00095-6.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Nonparametric Estimation of Average Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-location-scale.html",
    "href": "nonparametric/nonparametric-location-scale.html",
    "title": "15  Relaxing Stationarity with Nonparametric Time Effects",
    "section": "",
    "text": "15.1 A More General Model",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Relaxing Stationarity with Nonparametric Time Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-location-scale.html#a-more-general-model",
    "href": "nonparametric/nonparametric-location-scale.html#a-more-general-model",
    "title": "15  Relaxing Stationarity with Nonparametric Time Effects",
    "section": "",
    "text": "15.1.1 The Drawbacks of Stationarity\nThe stationarity assumption (13.3) is crucial to the identification argument of section (13). It allows us to connect the average change in the realized outcomes \\(Y_{it}\\) to finite differences of the structural function \\(\\phi(\\cdot, \\cdot, \\cdot)\\). In turn, this connection leads to our key identification result for average marginal effects: \\[\n\\begin{aligned}\n    & \\E[\\partial^x Y_{it}^{x}|X_{i1} = X_{i2} = x]\\\\\n    & = \\E[\\partial_x\\phi (x, A_i, U_{it})|X_{i1}=X_{i2}=x] \\\\\n    & = \\partial_{x_2} \\E\\left[Y_{i2}- Y_{i1}|X_{i1}=x_1, X_{i2} = x_2 \\right]\\Big|_{(x_1, x_2)=(x, x)}.\n\\end{aligned}\n\\]\nWhile useful for identification, assumption (13.3) imposes that the function \\(\\phi(\\cdot, \\cdot, \\cdot)\\) cannot change across periods. Such time invariance may be reasonable if consecutive observations are not separated by long periods of time. In contrast, it may be untenable if the interval between observations is large or there are meaningful changes in the overall “context” in which the units operate.\n\n\n15.1.2 Including Location-Scale Time Effects\nHowever, it is possible to accommodate some changes over time while preserving our identification results. In particular, it is possible to accommodate flexible location-scale effects that depend on the observables nonparametrically.\nSpecifically, we now consider the following extension of the model (11.4), discussed by Chernozhukov et al. (2015): \\[\n\\begin{aligned}\n    Y_{i1}^{x_1}  &= \\phi(x_1, A_i, U_{it}),\\\\\n    Y_{i2}^{x_2} & = \\mu(x_2) + \\sigma(x_2) \\phi(x_2, A_i, U_{i2}).\n\\end{aligned}\n\\tag{15.1}\\] The functions \\(\\mu(\\cdot)\\) and \\(\\sigma(\\cdot)\\) may be viewed as flexible nonparametric location-scale time effects. We assume throughout that \\(\\sigma(\\cdot)\\neq 0\\).\nAlthough we allow the function \\(\\phi\\) to change, we retain the conditional stationarity assumption (13.3): \\[\nU_{i1}|(X_{i1}, X_{i2}, A_i) \\overset{d}{=} U_{i2}|(X_{i1}, X_{i2}, A_i).\n\\]\nThe key objects of interest are the average marginal effects of \\(x\\) for stayers in periods \\(t=1\\): \\[\n\\begin{aligned}\n  \\E[\\partial_x Y_{i1}^x|X_{i1}=X_{i2} =x] & = \\E\\left[ \\partial_x \\phi(x, A_{i}, U_{it}) |X_{i1}=X_{i2} =x\\right],\n\\end{aligned} \\tag{15.2}\\] and \\(t=2\\): \\[\n\\begin{aligned}\n& \\E[\\partial_x Y_{i2}^x|X_{i1}=X_{i2} =x]\\\\\n  & = \\E\\left[ \\partial_x \\left(\\mu(x) + \\sigma(x)\\phi(x, A_{i}, U_{it})  \\right)|X_{i1}=X_{i2} =x\\right]\n\\end{aligned}\n\\tag{15.3}\\] Observe that there is now time variation in the average effects, unlike under model (13.3).\n\n\n15.1.3 Discussion\nModel (15.1) preserves the two attractive features of the simpler model (11.4):\n\n\\((A_i, U_{it})\\) can take any form, have any dimension, and affect the potential outcomes arbitrarily.\nThere are no restrictions on the dependence between the treatment \\(X_{it}\\) and potential outcomes.\n\nIntuitively, the connection between models (11.4) and (15.1) mirrors familiar binary-treatment settings. The time-invariant model (11.4) corresponds to a nonparametric event-study design with a continuous treatment (see chapter 17 of Huntington-Klein (2025) regarding event studies with no trends in the outcome). The location-scale extension (15.1), by contrast, generalizes this to a nonparametric difference-in-differences framework, accommodating nonparametric trends in the outcomes.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Relaxing Stationarity with Nonparametric Time Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-location-scale.html#identification",
    "href": "nonparametric/nonparametric-location-scale.html#identification",
    "title": "15  Relaxing Stationarity with Nonparametric Time Effects",
    "section": "15.2 Identification",
    "text": "15.2 Identification\nWe now turn to identification of the average marginal effects (15.2) and (15.3). Identification proceeds in three steps:\n\nIdentifying the scale function \\(\\sigma(\\cdot)\\).\nIdentifying the location function \\(\\mu(\\cdot)\\).\nIdentifying the average value \\(\\E[\\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x]\\) of \\(\\phi\\) and the average value of its derivative \\(\\E[\\partial_x \\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x]\\).\n\nTogether, these components are sufficient to identify both (15.2) and (15.3), since the second period effect can be represented as \\[\n\\begin{aligned}\n& \\E\\left[ \\partial_x \\left(\\mu(x) + \\sigma(x)\\phi(x, A_{i}, U_{it}) \\right)|X_{i1}=X_{i2} =x\\right] \\\\\n& = \\partial_x \\mu(x) \\\\\n& \\quad + \\sigma(x) \\E[\\partial_x \\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x] \\\\\n& \\quad + \\left(\\partial_x \\sigma(x)\\right) \\E[\\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x]\n\\end{aligned}\n\\]\n\n15.2.1 Scale Effect\nWe begin by identifying the scale function \\(\\sigma(\\cdot)\\). The scale is directly connect to the variance of the second period realized outcome for the subpopulation of stayers as \\[\n\\begin{aligned}\n    & \\var(Y_{i2}|X_{i1}=X_{i2}=x) \\\\\n    & = \\sigma^2(x)\\var(\\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x).\n\\end{aligned}\n\\] As before, the notation \\(U_{it}\\) under \\(\\phi\\) emphasizes stationarity of \\(U_{it}\\).\nAt the same time, the conditional variance of \\(\\phi(x, A_i, U_{it})\\) for stayers is directly obtained from the variance of the first period realized outcome: \\[\n\\begin{aligned}\n    & \\var\\left(Y_{i1}|X_{i1}=X_{i2}=x \\right) \\\\\n    & = \\var(\\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x).\n\\end{aligned}\n\\]\nCombining the two expression yields an explicit formula for the scale effect, provided \\(\\var\\left(Y_{i1}|X_{i1}=X_{i2}=x \\right)\\neq 0\\): \\[\n    \\sigma^2(x) = \\dfrac{   \\var(Y_{i2}|X_{i1}=X_{i2}=x) }{ \\var(Y_{i1}|X_{i1}=X_{i2}=x) }.\n\\]\n\n\n15.2.2 Location Effect and Average of \\(\\phi\\)\nTo obtain the location effect \\(\\mu(\\cdot)\\) and \\(\\E[\\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x]\\), we instead look at the averages of the realized outcomes for stayers. By model (15.1) it holds that \\[\n\\begin{aligned}\n    \\E[Y_{i1}|X_{i1}=X_{i2}=x] & = \\E[\\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x], \\\\\n    \\E[Y_{i2}|X_{i1}=X_{i2}=x] & = \\mu(x) + \\sigma(x)\\E[\\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x].\n\\end{aligned}\n\\] Identification for \\(\\E[\\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x]\\) is thus immediate. To obtain \\(\\mu(\\cdot)\\), we rearrange and multiply the first line by \\(\\sigma(x)\\): \\[\n\\begin{aligned}\n    \\mu(x) & = \\E[Y_{i2}|X_{i1}=X_{i2}=x] - \\sigma(x)\\E[Y_{i1}|X_{i1}=X_{i2}=x] \\\\\n    & = \\E[Y_{i2}|X_{i1}=X_{i2}=x] \\\\\n    & \\quad - \\sqrt{\\dfrac{ \\var(Y_{i1}|X_{i1}=X_{i2}=x) }{ \\var(Y_{i1}|X_{i1}=X_{i2}=x) }}\\E[Y_{i2}|X_{i1}=X_{i2}=x] .\n\\end{aligned}\n\\]\n\n\n15.2.3 Average Derivative of \\(\\phi\\)\nFinally, having identified \\(\\mu(\\cdot)\\) and \\(\\sigma(\\cdot)\\), we can now proceed to identify \\(\\E[\\partial_x \\phi(x, A_i, U_{it})|X_{i1}=X_{i2}=x]\\). To do so, we define new synthetic outcomes that eliminate the location-scale effects, reducing the problem to the stationary case (11.4): \\[\n\\begin{aligned}\nZ_{i1} & = Y_{i1}, \\\\\nZ_{i2} & = \\dfrac{Y_{i2}- \\mu(X_{i2})}{\\sigma(X_{i2})}.\n\\end{aligned}\n\\]\nThe new variables \\((Z_{i1}, Z_{it})\\) satisfy two key properties:\n\nThe distribution of \\((Z_{i1}, Z_{i2}, X_{i1}, X_{i2})\\) is identified.\nThe variables follow \\[\nZ_{it} = \\phi(X_{it}, A_i, U_{it}).\n\\]\n\nIn other words, \\((Z_{i1}, Z_{i2})\\) follow model (11.4).\nBy applying the results of sections 13-14 we conclude that \\[\n\\begin{aligned}\n    & \\E[\\partial_x\\phi (x, A_i, U_{it})|X_{i1}=X_{i2}=x] \\\\\n    & = \\partial_{x_2} \\E\\left[ Z_{i2}- Z_{i1}|X_{i1}=x_1, X_{i2} = x_2 \\right]\\Big|_{(x_1, x_2)=(x, x)}.\n\\end{aligned}\n\\]\nCombining the above results together yields overall identification for average marginal effects (15.2) and (15.3).",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Relaxing Stationarity with Nonparametric Time Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-location-scale.html#estimation",
    "href": "nonparametric/nonparametric-location-scale.html#estimation",
    "title": "15  Relaxing Stationarity with Nonparametric Time Effects",
    "section": "15.3 Estimation",
    "text": "15.3 Estimation\nEstimation of (15.2) and (15.3) is now straightforward. As in section 14, we have shown that the objects of interest can be expressed in terms of explicit functions of conditional expectations of \\((Y_{i1}, Y_{i2})\\) given \\((X_{i1}, X_{i2})\\). This expectations can be replaced with local polynomial estimators, following the logic of section 14. The resulting estimator for the average marginal effects of interest will be consistent and asymptotically normal by the delta method, as local polynomial estimators are consistent and jointly asymptotically normal.\nFor inference, the simplest approach is to use the bootstrap rather than use the expression for the variance implied by the delta method. Accordingly, the simplest way to conduct valid inference is by using bootstrap and recomputing all conditional expectations. See Chernozhukov et al. (2015) regarding bootstrap inference, and also alternative estimation using global methods (as opposed to the local polynomial approach).\n\n\nNext Section\nIn the next section, we extend our identification results for average marginal effects beyond the population of stayers using restrictions on the dependence between \\((A_i, U_{it})\\) and \\(X_{it}\\).\n\n\n\n\nChernozhukov, Victor, Iván Fernández-Val, Stefan Hoderlein, Hajo Holzmann, and Whitney Newey. 2015. “Nonparametric Identification in Panels Using Quantiles.” Journal of Econometrics 188 (2): 378–92. https://doi.org/10.1016/j.jeconom.2015.03.006.\n\n\nHuntington-Klein, Nick. 2025. The Effect: An Introduction to Research Design and Causality. S.l.: Chapman and Hall/CRC.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Relaxing Stationarity with Nonparametric Time Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-cre.html",
    "href": "nonparametric/nonparametric-cre.html",
    "title": "16  Beyond Stayers Using Index Restrictions",
    "section": "",
    "text": "16.1 Index Restrictions",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Stayers Using Index Restrictions</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-cre.html#index-restrictions",
    "href": "nonparametric/nonparametric-cre.html#index-restrictions",
    "title": "16  Beyond Stayers Using Index Restrictions",
    "section": "",
    "text": "16.1.1 Regarding Stayers\nAll the results of sections 13-15 for the average marginal effect in models (11.4) and (15.1) are limited to the subpopulation of stayers (units \\(X_{i1}=X_{i2}\\)).\nThis restriction is a direct consequence of imposing no structure on how \\((X_{i1}, X_{i2})\\) relate to the unobserved components \\((A_i, U_{it})\\). Without restrictions, the dependence between treatments and unobservables may be arbitrarily complex, and Cooprider, Hoderlein, and Meister (2022) shows that no identification is possible for non-stayers if \\((A_i, U_{it})\\) is multidimensional.\n\n\n16.1.2 Index Restrictions\nTo extend identification beyond stayers, we must restrict the dependence structure between treatments \\((X_{i1}, X_{i2})\\) and unobserved components \\((A_i, U_{it})\\). In this section, we look at a popular class of restrictions known as index restrictions. Broadly, index restrictions reduce the dimensionality of the dependence structure between the treatments and the unobserved components by assuming that \\((A_i, U_{it})\\) depends on \\((X_{i1}, X_{i2})\\) only through a low-dimensional index. They were introduced and studied by Altonji and Matzkin (2005) (see also Bester and Hansen (2009) and Liu, Poirier, and Shiu (2024)).\nAs a simple example, consider again model (11.4), with the potential outcome in period \\(t\\) determined as \\[\nY_{it}^x = \\phi(x, A_i, U_{it}), \\quad t=1, 2.\n\\]\nWe now make an assumption about how \\((X_{i1}, X_{i2})\\) and \\((A_i, U_{i1}, U_{i2})\\) relate to each other. Specifically, we assume that the joint distribution of \\((A_i, U_{i1}, U_{i2})\\) only depends on \\((X_{i1}, X_{i2})\\) through the sum \\(X_{i1}+X_{i2}\\): \\[\n\\begin{aligned}\n    & f_{A_i, U_{i1}, U_{i2}|X_{i1}, X_{i2}}(a, u_1, u_2|x_1, x_2) \\\\\n    & = f_{A_i, U_{i1}, U_{i2}|X_{i1}+ X_{i2}}(a, u_1, u_2|x_1+x_2).        \n\\end{aligned}\n\\tag{16.1}\\] In other words, the \\(X_{i1}+X_{i2}\\) plays the role of an index or a sufficient statistic that captures all the relevant dimensions of the dependence structure between the treatments and the unobserved components. Economically, restriction (16.1) arises when agents respond to aggregate exposure (see the example below).\nMore generally, in a setting with \\(T\\) time periods one may assume that the joint distribution of potential outcomes \\((Y_{i1}^{x_1}, \\dots, Y_{iT}^{x_T})\\) only depends on the treatments \\((X_{i1}, \\dots, X_{iT})\\) through \\(k&lt;T\\) index functions \\(g_1(\\cdot), \\dots, g_k(\\cdot)\\):\n\\[\\begin{aligned}\n     & (Y_{i1}^{x_1}, \\dots, Y_{iT}^{x_T})|X_{i1}, \\dots, X_{iT} \\\\\n     & \\overset{d}{=} (Y_{i1}^{x_1}, \\dots, Y_{iT}^{x_T})|g_1(X_{i1}, \\dots, X_{iT}), \\dots, g_k(X_{i1}, \\dots, X_{iT}).\n\\end{aligned}\n\\tag{16.2}\\] The index functions \\(g_{\\cdot}(\\cdot)\\) are typically assumed known (averages, variances, etc.), though there are some results with unknown indidex \\(g(\\cdot)\\) (Bester and Hansen 2009).\n\n\n\n\n\n\nIn panel data terminology, such index restrictions are sometimes called “correlated random effects” (CRE) assumptions. CRE settings lie between fixed effects settings that do not restrict the dependence structure at all and random effects settings which assume full unconditional independence.\n\n\n\n\n\n16.1.3 Intuition\nTo get some intuition for index restrictions, let us consider a consumption example. Suppose that \\(X_{it}\\) is the income in period \\(t\\), \\(Y_{it}\\) is the expenditure on a given consumption category (say, entertainment). The components \\((A_i, U_{it})\\) represent preferences, prices and other unobserved variables.\nIf consumers smooth consumption based on total income \\(X_{i1} + X_{i2}\\) (e.g., due to savings or credit), then deviations in \\((X_{i1}, X_{i2})\\) are independent of \\((A_i, U_{it})\\) conditional on the total income. Here, \\(X_{i1} + X_{i2}\\) is the index; it plays the role of a sufficient statistic for consumption decisions.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Stayers Using Index Restrictions</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-cre.html#average-marginal-effects-beyond-stayers",
    "href": "nonparametric/nonparametric-cre.html#average-marginal-effects-beyond-stayers",
    "title": "16  Beyond Stayers Using Index Restrictions",
    "section": "16.2 Average Marginal Effects Beyond Stayers",
    "text": "16.2 Average Marginal Effects Beyond Stayers\n\n16.2.1 Example Identification Argument\nIndex restrictions in the spirit of equations (16.1) and (16.2) can be used to obtain more general identification results. Broadly, they exploit the idea that units with the same index value are “comparable” in terms of unobserved heterogeneity, even if their individual treatments differ.\nFor example, under (16.1), two units with \\((X_{i1}, X_{i2}) = (x_1, x_2)\\) and \\((X_{i1}, X_{i2}) = (x_2, x_1)\\) share the same index \\(x_1 + x_2\\) and thus the same distribution of \\((A_i, U_{it})\\). This allows us to borrow information from stayers (where \\(X_{i1} = X_{i2}\\)) to identify effects for non-stayers with the same index.\nMore formally, assumption (16.1) implies that the average marginal effect of a change of \\(x\\) is the same for all units whose covariates are the same on average across time: \\[\n\\begin{aligned}\n& \\E\\left[ \\partial_x Y_{it}^x |X_{i1}=x_1, X_{i2}= x \\right] \\\\\n&  = \\E[\\partial_x \\phi(x, A_i, U_{it})|X_{i1}= x_1, X_{i2}=x_2] \\\\\n& =\\E\\left[\\partial_x \\phi(x, A_i, U_{it})\\Bigg|\\dfrac{X_{i1}+X_{i2}}{2}=\\dfrac{x_1+x_2}{2}\\right].\n\\end{aligned}\n\\]\nIn particular, the above equality holds if we consider the stayers on one of the sides of the equations: \\[\n\\begin{aligned}\n    & \\E[\\partial_x \\phi(x, A_i, U_{it})| X_{i1}=X_{i2}=x] \\\\\n    &  =\\E\\left[\\partial_x \\phi(x, A_i, U_{it})\\Big|\\dfrac{X_{i1}+X_{i2}}{2}=x\\right]\n\\end{aligned}\n\\tag{16.3}\\] Provided there exist stayers at \\(x\\), the argument of section 13-14 identifies \\(\\E[\\partial_x \\phi(x, A_i, U_{it})| X_{i1}=X_{i2}=x]\\).\nThus, if stayers exist at \\(x\\), we can identify effects for all units with \\((X_{i1} + X_{i2})/2 = x\\). The diagram below visually illustrates the identified set:\n\n\n\nIdentified set under index restrictions. The diagonal (stayers) anchors identification. The shaded region includes all \\((x_1, x_2)\\) pairs where \\((x_1 + x_2)/2\\) matches a stayer’s average treatment, enabling extrapolation via the index restriction.\n\n\nSummarizing, identification of the average marginal effect with an index restriction proceeds in two steps. First, we identify the average marginal effects for the subpopulation of stayers as before (the diagonal on the diagram). Second, we extrapolate these effects to subpopulations that share the same values of the index as the stayers.\n\n\n16.2.2 Estimation\nIndex restrictions open some new possibilities for estimation. One now faces a choice between two possible approaches:\n\nUsing only stayer data.\nPooling data and performing estimation in the index space.\n\nIn the first scenario, one construct estimators for the average effects for stayers using the data for stayers, exactly as in section (14). Then those estimators are automatically consistent and asymptotically normal estimators for average marginal effects in virtue of Equation 16.3.\nUnder the second scenario, one instead regresses \\(Y_{i2}-Y_{i1}\\) directly on the index \\(X_{i1} + X_{i2}\\). The key connection to data is obtained by combining equations (16.3) and (14.2). This approach is naturally more efficient, as it relies on more data. However, a potential risk is that even the average marginal effects for stayers may be estimated inconsistently if the index restriction is\n\n\nNext Section\nIn the next section, we go beyond average effects and discuss identification of variance in models with unrestricted unobserved heterogeneity.\n\n\n\n\nAltonji, Joseph G., and Rosa L. Matzkin. 2005. “Cross Section and Panel Data Estimators for Nonseparable Models with Endogenous Regerssors.” Econometrica 73 (4): 1053–1102. https://doi.org/10.1111/j.1468-0262.2005.00609.x.\n\n\nBester, C Alan, and Christian Hansen. 2009. “Identification of Marginal Effects in a Nonparametric Correlated Random Effects Model.” Journal of Business and Economic Statistics 27 (2): 235–50. https://doi.org/10.1198/jbes.2009.0017.\n\n\nCooprider, Joseph, Stefan Hoderlein, and Alexander Meister. 2022. “A Panel Data Estimator for the Distribution and Quantiles of Marginal Effects in Nonlinear Structural Models with an Application to the Demand for Junk Food.” https://doi.org/10.2139/ssrn.3545485.\n\n\nLiu, Laura, Alexandre Poirier, and Ji-Liang Shiu. 2024. “Identification and Estimation of Partial Effects in Nonlinear Semiparametric Panel Models.” Journal of Econometrics, September, 105860. https://doi.org/10.1016/j.jeconom.2024.105860.",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Beyond Stayers Using Index Restrictions</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-variance.html",
    "href": "nonparametric/nonparametric-variance.html",
    "title": "17  Variance of Marginal Effects",
    "section": "",
    "text": "17.1 Issues in Fully Nonseparable Model\nIn the preceding sections, we focused on average marginal effects. However, as noted in our discussion of linear models, higher-order features — variances, other moments, and distributions — are also important and useful in policy analysis.\nUnfortunately, the finite-difference approach used for average effects does not extend to variances in the fully nonseparable model (11.4). Below, we formalize this limitation and then show that variance is identified under additive separability (11.5).",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variance of Marginal Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-variance.html#issues-in-fully-nonseparable-model",
    "href": "nonparametric/nonparametric-variance.html#issues-in-fully-nonseparable-model",
    "title": "17  Variance of Marginal Effects",
    "section": "",
    "text": "17.1.1 Setting and Parameter of Interest\nWe start our discussion by returning to model (11.4): \\[\nY_{it}^{x} = \\phi(x, A_i, U_{it}), \\quad t=1, 2.\n\\] As before, we assume that \\(U_{it}\\) is conditionally stationary given \\((X_{i1}, X_{i2}, A_i)\\). We do not restrict the dependence structure between the covariates and the unobserved components, again focusing on stayers.\nWe are interested in identifying and estimating the variance of marginal effects for stayers: \\[\n     \\var\\left(\\partial_x \\phi(x, A_i, U_{it}) |X_{i1} = X_{i2}= x\\right).\n\\] This variance captures how much variation there is in a response to an infinitesimal change in \\(x\\).\n\n\n17.1.2 Issue with Finite Difference-Based Approach\nUnfortunately, our previous approach based on the change in outcomes across periods does not extend to second moments. To see why, first note that the convergence argument of section 13 can be extended to the square of finite differences as \\[\n\\begin{aligned}\n& \\E\\left[ \\left(\\partial_x \\phi(x, A_i, U_{it})  \\right)^2|X_{i1}=X_{i2} = x \\right]  \\\\\n&  = \\lim\\limits_{h\\to 0} \\E\\left[ \\left(\\dfrac{\\phi(x+h, A_i, U_{it}) - \\phi(x, A_i, U_{it})}{h}\\right)^2 \\Bigg| X_{i1} = x, X_{i2} = x+h  \\right].\n\\end{aligned}\n\\tag{17.1}\\]\nThe problem is that to identify the limit, we need to observe the second moment of finite differences for all \\(h&gt;0\\) small enough (or at least for a sequence of \\(h\\) convergent to 0). Previously, we identified the average finite difference by considering the average change in outcomes for near-stayers. Attempting the same approach leads to the representation: \\[\n\\begin{aligned}\n    & \\E\\left[ \\left(\\dfrac{Y_{i2}-Y_{i1}}{h}\\right)^2\\Bigg|X_{i1} = x, X_{i2} = x+h \\right] \\\\\n    & =\\E \\left[ \\left(  \\dfrac{\\phi(x+h, A_i, U_{i2}) - \\phi(x, A_i, U_{i1})}{h}\\right)^2\\Bigg|X_{i1} = x, X_{i2} = x+h    \\right]\n\\end{aligned}\n\\]\nUnfortunately, this expectation is in general not the expectation we are interested in, and there is an irreducible contamination term driving a wedge between the two expectations: \\[\n\\begin{aligned}\n    & \\E\\left[ \\left(\\dfrac{Y_{i2}-Y_{i1}}{h}\\right)^2\\Bigg|X_{i1} = x, X_{i2} = x+h \\right] \\\\\n    & \\quad - \\E\\left[ \\left(\\dfrac{\\phi(x+h, A_i, U_{it}) - \\phi(x, A_i, U_{it})}{h}\\right)^2 \\Bigg| X_{i1} = x, X_{i2} = x+h  \\right]\\\\\n    & = 2\\E\\left[ \\dfrac{\\phi(x, A_i, U_{i1})\\left(\\phi(x, A_i, U_{i2}) -\\phi(x, A_i, U_{i1})\\right)  }{h^2}   \\Bigg|X_{i1} = x, X_{i2} = x+h \\right].\n\\end{aligned}\n\\tag{17.2}\\]\nThe existence of a difference in Equation 17.2 means that our previous approach does not generalize to variances, without an obvious alternative path. In general, the expression Equation 17.2 is non-zero unless \\(U_{it}\\) is perfectly correlated over time (and thus part of \\(A_i\\)). Intuitively, this failure is driven by the same reason why the variance of treatment effect is generally not identified: different drivers may affect different potential outcomes preventing identification of correlation.\n\n\n\n\n\n\nThe above argument does not show lack of identification — only that a particular approach does not work. To formally prove lack of identification, one must construct a pair of data-generating process which generate the same distributions for \\((Y_{i1}, Y_{i2}, X_{i1}, X_{i2})\\) but different values of \\(\\var\\left(\\partial_x \\phi(x, A_i, U_{it}) |X_{i1} = X_{i2}= x\\right)\\). Constructing such a counterexample is an open question and an achievement in itself, as it would establish the limits of identification with model (11.4).",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variance of Marginal Effects</span>"
    ]
  },
  {
    "objectID": "nonparametric/nonparametric-variance.html#identifying-variance-in-models-with-additively-separable-u_it",
    "href": "nonparametric/nonparametric-variance.html#identifying-variance-in-models-with-additively-separable-u_it",
    "title": "17  Variance of Marginal Effects",
    "section": "17.2 Identifying Variance in Models with Additively Separable \\(U_{it}\\)",
    "text": "17.2 Identifying Variance in Models with Additively Separable \\(U_{it}\\)\n\n17.2.1 Model and Parameter of Interest\nThe finite-difference approach fails in the fully nonseparable model because the \\(U_{it}\\) distort the second moment and enter nonseparably under the unknown \\(\\phi\\), without an obvious way to correct for their presence. However, additive separability (model (11.5)) resolves this issue: \\[\n    Y_{it}^x = \\phi(x, A_i)  + U_{it}, \\quad i=1, \\dots, N, \\quad t=1, 2.\n\\]\nHere, \\(U_{it}\\) enters outside the structural function and the marginal effect \\(\\partial_x \\phi(x, A_i)\\). This allows us to adapt the finite-difference argument to identify the variance of marginal effects (and higher-order features), as shown by Morozov (2023).\nWe make the following assumption:\n\nMean exogeneity: \\[  \n   \\E[U_{it}|X_{i1}, X_{i2}, A_i] =0.\n\\tag{17.3}\\]\nUncorrelated \\(U_{it}\\): \\(U_{i1}\\) and \\(U_{i2}\\) are uncorrelated conditional on \\((X_{i1}, X_{i2})\\).\n\nImportantly, we do not need the assumption of stationarity on \\(U_{it}\\) anymore and hence drop it. If \\(U_{it}\\) is restricted to be stationary, model (11.5) is a special case of model (11.4), though the two are not nested otherwise.\nThe key difference between models (11.4) and (11.5) lies in their treatment of \\(U_{it}\\). The nonseparable model (11.4) permits \\(U_{it}\\) to be of any form and dimension. In contrast, model (11.5) restricts \\(U_{it}\\) to be an additive scalar, and only the time-invariant component is left fully unrestricted.\nAs above, the parameter of interest is the variance of marginal effects for stayers: \\[\n     \\var(\\partial_x \\phi(x, A_i)|X_{i1}=X_{i2}=x).\n\\] By additive separability, the distribution of \\(U_{it}\\) does not influence the marginal effects anymore, and hence \\(U_{it}\\) does not appear in the expression.\n\n\n17.2.2 Identification Strategy\nAs we now show, focusing on moments of differences in outcomes does yield identification in model (11.5). Our identification strategy uses the additivity to disentangle the variation in \\(\\phi(x, A_i)\\) from the variation in \\(U_{it}\\). We proceed in three steps:\n\nDecompose the second moment of outcome differences into the second moment of finite differences of \\(\\phi\\) and the second moments of \\(U_{it}\\).\nIdentify the second moments of \\(U_{it}\\): use the additive structure and conditional independence assumptions to identify \\(\\E[U_{it}^2|\\cdot]\\).\nCombine results and take limits: subtract the \\(U_{it}\\) terms (from step 2) from the observed second moment (step 1) and take \\(h \\to 0\\) to recover the target variance.\n\nThis approach mirrors the logic for average effects but requires additional corrections for the second moments of \\(U_{it}\\).\n\n\n17.2.3 Identification: Expansion\nTo start, we return to the second moment of difference of outcomes for near-stayers: \\[\n\\begin{aligned}\n& \\E\\left[ \\left(\\dfrac{Y_{i2}-Y_{i1}}{h}\\right)^2\\Bigg|X_{i1} = x, X_{i2} = x+h \\right] \\\\\n& =\\E \\left[ \\left(  \\dfrac{\\phi(x+h, A_i) - \\phi(x, A_i)}{h}  + \\dfrac{U_{i2} -U_{i1}}{h} \\right)^2\\Bigg|X_{i1} = x, X_{i2} = x+h    \\right]\\\\\n& = \\E \\left[ \\left(  \\dfrac{\\phi(x+h, A_i) - \\phi(x, A_i)}{h}\\right)^2\\Bigg|X_{i1} = x, X_{i2} = x+h    \\right] \\\\\n& \\quad  + \\E \\left[  \\left(\\dfrac{U_{i2} -U_{i1}}{h} \\right)^2\\Bigg|X_{i1} = x, X_{i2} = x+h    \\right]\\\\\n& \\quad +2 \\E\\left[   \\dfrac{  (U_{i2} -U_{i1})(\\phi(x+h, A_i) - \\phi(x, A_i))  }{h^2}\\Bigg|X_{i1} = x, X_{i2} = x+h   \\right]\\\\\n& = \\E \\left[ \\left(  \\dfrac{\\phi(x+h, A_i) - \\phi(x, A_i)}{h}\\right)^2\\Bigg|X_{i1} = x, X_{i2} = x+h    \\right]\\\\\n& \\quad +  h^{-2}\\left( \\E[U_{i2}^2|X_{i1}=x, X_{i2}=x+h] +  \\E[U_{i1}^2|X_{i1}=x, X_{i2}=x+h]\\right),  \n\\end{aligned}\n\\tag{17.4}\\]\nwhere we have used\n\nMean exogeneity: \\(\\E[U_{it} \\mid X_{i1}, X_{i2}, A_i] = 0\\) to conclude that the cross-term of \\(U_{it}\\) and \\(\\phi(x, A_i)\\) is zero.\nUncorrelatedness to conclude that that \\(\\E[U_{i1}U_{i2}\\mid X_{i1}, X_{i2}] = 0\\).\n\nThus, the observed second moment decomposes into:\n\nThe target (second moment of finite differences of \\(\\phi\\)),\nNoise terms (second moments of \\(U_{it}\\)), which we address next.\n\n\n\n17.2.4 Identification of Second Moments of \\(U_{it}\\)\nIn general, it is not obvious how to identify the second moments of \\(U_{it}\\) in Equation 17.4. The key challenge is that \\(\\E[U_{i2}^2|X_{i1}=x, X_{i2}=x+h]\\) takes the whole history of \\(X_{it}\\) into account. The only source of information about these moments are exactly the near-stayers. However, we are already using the second-moment information of the near-stayers in Equation 17.4.\nTo proceed, we reduce the dimensionality of the conditioning set. Specifically, we assume that the second moment of \\(U_{it}\\) only depends on the contemporaneous value of \\(X_{it}\\), but not on the values of \\(X_{is}\\) for \\(s\\neq t\\). Formally, we assume that \\[\n\\begin{aligned}\n\\E[U_{i1}^2|X_{i1}=x_1, X_{i2}=x_2] & = \\E[U_{i1}^2|X_{i1}=x_1],\\\\\n\\E[U_{i2}^2|X_{i1}=x_1, X_{i2}=x_2] & = \\E[U_{i2}^2|X_{i2}=x_2].\n\\end{aligned}\n\\] Intuitively, this is a “static heteroskedasticity” (or static variance) assumption that rules out dynamic dependencies in the variances of \\(U_{it}\\).\nTo identify \\(\\E[U_{i1}^2 \\mid X_{i1}=x, X_{i2}=x+h]\\), consider the stayer population \\(X_{i1}\\) \\(= X_{i2}\\) \\(= x\\). Their outcomes satisfy: \\[\n\\begin{aligned}\nY_{i1} & = \\phi(x, A_i) + U_{i1}, \\\\\nY_{i2} & = \\phi(x, A_i) + U_{i2}.\n\\end{aligned}\n\\] Subtracting and multiplying by \\(Y_{i1}\\) yields: \\[\nY_{i1}(Y_{i1} - Y_{i2}) = \\phi(x, A_i)(U_{i1} - U_{i2}) - U_{i1}U_{i2} + U_{i1}^2.\n\\] Taking expectations and applying mean exogeneity and uncorrelatedness yields \\[\n\\E[Y_{i1}(Y_{i1} - Y_{i2}) \\mid X_{i1} = X_{i2} = x] = \\E[U_{i1}^2 \\mid X_{i1} = X_{i2} = x].\n\\] By the static variance assumption, this equals \\(\\E[U_{i1}^2 \\mid X_{i1}=x, X_{i2}=x+h]\\) for any \\(h\\). Thus: \\[\n\\E[U_{i1}^2 \\mid X_{i1}=x, X_{i2}=x+h] = \\E[Y_{i1}(Y_{i1} - Y_{i2}) \\mid X_{i1} = X_{i2} = x].\n\\]\nWhy do we multiply by \\(Y_{i1}\\)? This trick isolates \\(U_{i1}^2\\) by exploiting:\n\n\\(Y_{i1} - Y_{i2} = U_{i1} - U_{i2}\\) (since \\(\\phi(x, A_i)\\) cancels out for stayers).\n\\(Y_{i1} = \\phi(x, A_i) + U_{i1}\\), so \\(Y_{i1}(U_{i1} - U_{i2})\\) expands to include \\(U_{i1}^2\\).\n\nA symmetric argument identifies \\(\\E[U_{i2}^2 \\mid X_{i1}=x, X_{i2}=x+h]\\) using \\(Y_{i2}(Y_{i2} - Y_{i1})\\).\n\n\n17.2.5 Identification of Variance of Marginal Effects\nWe now combine the results to identify the second moment of marginal effects. Recall from 17.4 that: \\[\n\\begin{aligned}\n& \\E\\left[\\left(\\frac{Y_{i2}-Y_{i1}}{h}\\right)^2 \\Bigg| X_{i1}=x, X_{i2}=x+h\\right] \\\\\n& = \\E\\left[\\left(\\frac{\\phi(x+h, A_i) - \\phi(x, A_i)}{h}\\right)^2 \\Bigg| X_{i1}=x, X_{i2}=x+h\\right] \\\\\n& \\quad + h^{-2}\\left(\\E[U_{i2}^2 \\mid X_1=x] + \\E[U_{i1}^2 \\mid X_2=x+h]\\right).\n\\end{aligned}\n\\]\nSubtracting the identified second moments of \\(U_{it}\\) now yields an explicit expression for the second moment of the finite difference of \\(\\phi\\): \\[\n\\E\\left[\\left(\\frac{\\phi(x+h, A_i) - \\phi(x, A_i)}{h}\\right)^2 \\Bigg| X_{i1}=x, X_{i2}=x+h\\right] = D(h),\n\\]\nwhere the function \\(D(h)\\) is defined as\n\\[\n\\begin{aligned}\n     D(h) & =  \\E\\left[ (Y_{i2}-Y_{i1})^2|X_{i1} = x, X_{i2} = x+h  \n\\right] \\\\\n&  \\hspace{1cm} - \\E[   Y_{i1}( Y_{i1} - Y_{i2}) |X_{i1} = X_{i2} = x] \\\\\n& \\hspace{1cm} -\\E[ Y_{i2}( Y_{i2} - Y_{i1}) |X_{i1} = X_{i2} = x+h].\n\\end{aligned}\n\\]\nBy Equation 17.1, the second moment of marginal effects is identified as \\[\n\\begin{aligned}\n    &  \\E\\left[ (\\partial_x \\phi(x, A_i))^2|X_{i1}= X_{i2}=x \\right]    \\\\\n    & = \\lim\\limits_{h\\to 0} h^{-2} D(h)\n\\end{aligned}\n\\tag{17.5}\\]\nFinally, the variance follows by subtracting the squared average marginal effect (identified in section 13): \\[\n\\begin{aligned}\n& \\var(\\partial_x \\phi(x, A_i) \\mid X_{i1}=X_{i2}=x) \\\\\n& = \\E\\left[(\\partial_x \\phi(x, A_i))^2 \\mid X_{i1}=X_{i2}=x\\right] - \\left(\\E\\left[\\partial_x \\phi(x, A_i) \\mid X_{i1}=X_{i2}=x\\right]\\right)^2.\n\\end{aligned}\n\\]\n\n\n17.2.6 A More Explicit Representation and Higher-Order Moments and Distributions\nThe expression for the second moment in 17.5 is again inconvenient for estimation as in involves a limit. However, Morozov (2023) shows that a more explicit characterization is possible under some further smoothness assumptions. Specifically, the second moment can be represented as: \\[\n\\E\\left[(\\partial_x \\phi(x, A_i))^2 \\mid X_{i1}=X_{i2}=x\\right] = \\frac{1}{2} D''(0),\n\\] where \\(D''(0)\\) is the second derivative of \\(D(h)\\) at \\(h=0\\). This allows estimation via local cubic regression.\nAs a final comment, we note that the above argument can be generalized to higher-order moments and even be used to identify the full distribution of marginal effects for stayers. For recovery of moments, one needs a full conditional independence assumptions as in Arellano and Bonhomme (2012) (section Chapter 9), giving the model a convolution structure.\n\n\nNext Section\nIn the next section, we change our focus to cross-sectional data and switch our focus to approaches targeting quantile and distributional treatment effects.\n\n\n\n\nArellano, Manuel, and Stéphane Bonhomme. 2012. “Identifying distributional characteristics in random coefficients panel data models.” Review of Economic Studies 79 (3): 987–1020. https://doi.org/10.1093/restud/rdr045.\n\n\nMorozov, Vladislav. 2023. “Estimating the Moments and the Distribution of Heterogeneous Marginal Effects Using Panel Data.”",
    "crumbs": [
      "Nonparametrics and Heterogeneity",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Variance of Marginal Effects</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-dte-intro.html",
    "href": "qte-dte/qte-dte-intro.html",
    "title": "18  Defining Quantile and Distributional Treatment Effects",
    "section": "",
    "text": "18.1 Towards Cross-Sectional Data\nIn the previous block we have discussed that one can learn average treatment effects, the variance of treatment effects, and even the full distribution of treatment effects in settings that permit:\nHowever, these results come at an important price: they require panel data with (at least approximate) stationarity. Such data may often not be available. Many datasets provide only one observation per unit, or have cross-sections spaced too far apart in time to justify stationarity assumptions. This motivates a key question: what distributional features of treatment effects can we learn from cross-sectional data alone?\nUnfortunately, the above success is generally hard to replicate in cross-sectional data. Some identification of distributional features beyond averages is certainly possible, but it typically requires monotonicity assumptions (Matzkin 2003, 2007). However, as Hoderlein and Mammen (2007) argue, monotonicity assumptions are often unrealistic. They require that a single scalar unobservable (e.g., “ability”) monotonically determines all potential outcomes, a restriction that clashes with most economic models. For example, earnings may depend on both ability and risk aversion (a multidimensional unobservable), or responses to treatments may be nonmonotonic (e.g., intermediate doses of a policy having larger effects than high doses).\nThe literature’s response has been to refocus on distributional treatment effects (DTEs) and quantile treatment effects (QTEs). These do not require monotonicity and can often be point-identified from cross-sectional data, even with multidimensional unobservables. At the same time, this identification has a price — QTEs and DTEs are not the quantiles or the distributions of treatment effects, a point we discuss in this section.\nIn this block, we:",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Defining Quantile and Distributional Treatment Effects</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-dte-intro.html#towards-cross-sectional-data",
    "href": "qte-dte/qte-dte-intro.html#towards-cross-sectional-data",
    "title": "18  Defining Quantile and Distributional Treatment Effects",
    "section": "",
    "text": "Multiple unobserved components (including functional-valued differences),\nNonparametric dependence of potential outcomes on treatments and unobservables.\n\n\n\n\n\n\nDefine QTEs and DTEs, along with discussing their interpretations.\nDiscuss their identification under various form of unconfoundedness.\nIntroduce quantile and distributional regression as estimation tools.\nConnect these methods to the nonseparable panel models from the previous block.",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Defining Quantile and Distributional Treatment Effects</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-dte-intro.html#defining-qtes-and-dtes",
    "href": "qte-dte/qte-dte-intro.html#defining-qtes-and-dtes",
    "title": "18  Defining Quantile and Distributional Treatment Effects",
    "section": "18.2 Defining QTEs and DTEs",
    "text": "18.2 Defining QTEs and DTEs\n\n18.2.1 Potential Outcome Framework\nTo formalize the setting of this block and define the new parameters of interest, we go back to the cross-sectional potential outcome model of Equation 1.1.\nLet \\(X_i\\) be some treatment. \\(X_i\\) may be scalar- or vector-valued. We are interested in the effect of \\(X_i\\) on some outcome \\(Y_i\\). To each value \\(x\\) of \\(X_i\\) we associate a potential outcome \\(Y^x_i\\), determined as \\[\nY_i^x = \\phi(x, A_i).\n\\] Like in the previous block, \\(\\phi\\) is an unknown function and \\(A_i\\) is some unobserved component, potentially infinite-dimensional in nature. In this block, our identification results mostly directly involve \\(Y_{i}^x\\), rather than the \\(\\phi(\\cdot, A_i)\\) representation.\nAs before, we maintain the Stable Unit Treatment Value Assumption (SUTVA) throughout, ruling out spillovers or general equilibrium effects (see Warning 18.2 for caveats).\nThe key role in this section is played by the distribution of potential outcomes, where by “distribution” we mean either the cumulative distribution function \\(F_{Y^x}(\\cdot)\\) of \\(Y^x_i\\) or its quantile function \\(Q_{Y^x}(\\cdot)\\).\n\n\n18.2.2 QTEs and DTEs\nThe new distributional parameters of interest are built up by contrasting the distributions of \\(Y^x_i\\) for different values of \\(x\\).\nThe most fundamental parameters of interest are the unconditional quantile and distributional treatment effects (QTE and DTE) of switching from \\(x_1\\) to \\(x_2\\), which are defined as \\[\n\\begin{aligned}\n    QTE(x_1, x_2, \\tau ) &  =Q_{Y^{x_2}}(\\tau) -    Q_{Y^{x_1}}(\\tau) ,\\\\\n        DTE(x_1, x_2, y) & = F_{Y^{x_2}}(y) - F_{Y^{x_1}}(y).\n\\end{aligned}\n\\tag{18.1}\\] where \\(x_1, x_2\\) are some values of the treatment and \\(\\tau\\in [0, 1]\\).\nIf the treatment is continuous, we may further consider derivatives of \\(Q_{Y^x}(\\tau)\\) and \\(F_{Y^x}(y)\\) with respect to \\(x\\), yielding marginal QTEs and DTEs.\nWe may also condition on the realized treatment values \\(X_i\\) or on other control variables \\(W_i\\). For the first scenario, let \\(F_{Y^{x_2}|X}(y|x_1)\\) be the CDF of \\(Y^{x_2}\\) for the units that actually receive \\(X=x_1\\); similarly for the quantile function \\(Q_{Y^{x_2}|X}(\\tau|x_1)\\). The QTEs and DTEs of switching from \\(x_1\\) to \\(x_2\\) for the population of units with realized treatment value \\(x_3\\) are defined as \\[\n\\begin{align}\n    QTE(x_1, x_2, \\tau|x_3) &  =Q_{Y^{x_2}|X}(\\tau|x_3) -   Q_{Y^{x_1}|X}(\\tau|x_3) ,\\\\\n    DTE(x_1, x_2, y|x_3) & = F_{Y^{x_2}|X}(y|x_3) - F_{Y^{x_1}|X}(y|x_3).\n\\end{align}\n\\tag{18.2}\\] If \\(X_i\\) is a scalar binary treatment, the quantile effects are also known as the quantile treatment effects for the treated or the control group, depending on whether \\(x_3=1\\) or \\(x_3=0\\) (shortened to QTT and QTC, respectively).\nFor the second scenario, let \\(W_i\\) be some other covariates or control variables. One may consider the QTEs and DTEs within the \\(w\\)-stratum of \\(W_i\\): \\[\n\\begin{align}\n    QTE(x_1, x_2, \\tau|w) & = Q_{Y^{x_2}|W}(\\tau|w) - Q_{Y^{x_1}|W}(\\tau|w), \\\\\n        DTE(x_1, x_2, y|w) & = F_{Y^{x_2}|W}(y|w) - F_{Y^{x_1}|W}(y|w).\n\\end{align}\n\\tag{18.3}\\]\nThe above QTEs and DTEs may be viewed as the core parameters of interest in the literature on distributional effects. However, one may also consider Gini-like and other transformations of the marginal distributions of potential outcomes, see section 2.1 in Chernozhukov, Fernández-Val, and Melly (2013) for some further details.\n\n\n\n\n\n\nWarning 18.1\n\n\n\nThere is an important difference between ATEs and QTEs when dealing with conditional and unconditional effects. When working with average effects, one may integrate the conditional average treatment effects given \\(W_i\\) to obtain the ATE. This connection no longer holds for QTEs, and so the choice between conditional and unconditional QTEs leads to different interpretations.\nTo understand the difference between conditional and unconditional quantiles, consider the following example. Let \\(W_i\\) be the person’s education level, and \\(Y_i\\) be earnings. The 10th percentile of the \\(Y_i\\) for college-educated people may lie fairly high in the overall (marginal) distribution of \\(Y_i\\). See Powell (2020) for more discussion and further references.\n\n\n\n\n18.2.3 Interpretations\nWhat are the QTEs and the DTEs?\n\nGeometric Interpretation\nThe first and simpler interpretation is purely geometric. As shown on Figure 18.1, DTEs and QTEs can be viewed as measures of the distance in the distributions of potential outcomes:\n\nThe DTEs measure the vertical difference between the CDFs of the potential outcomes.\nthe QTE corresponds to the vertical distance between quantile functions (or equivalently, the horizontal distance between CDFs, since quantile functions are inverses of CDFs).\n\n\n\n\n\n\n\nFigure 18.1: Visual representation of distributional treatment effects (DTEs) and quantile treatement effects (QTEs). Depicted: \\(DTE(3)\\) — difference of CDFs of potential outcomes distributions at \\(y=3\\) and \\(QTE(0.5)\\) — difference of medians of potential outcome distributions\n\n\n\n\n\nCausal Interpretation\nThe second and deeper interpretation of QTEs and DTEs is causal. This interpretation requires SUTVA and typically goes as follows (Athey and Imbens 2017). The QTE and DTE of a change from \\(x_1\\) to \\(x_2\\) describe how the entire outcome distribution would shift if all units moved from treatment value \\(x_1\\) to value \\(x_2\\). Under SUTVA, the resulting distribution of outcomes will be exactly given by the marginal distribution of the corresponding potential outcomes. For example, a QTE of +5 at \\(\\tau=0.25\\) means the 25th percentile of earnings would rise by 5 units under the treatment change.\n\n\n\n\n\n\nWarning 18.2\n\n\n\nSUTVA is critical in the above interpretation, as it rules out general equilibrium effects of such a universal shift in treatment.\nIn many settings such an assumption may be unrealistic. However, as these lecture notes are written, analysis of “global” QTEs appears to only be a nascent field.\n\n\nBetween the QTE and the DTE, the QTE is slightly easier to interpret, as QTEs are expressed in the same units as the outcome variable. In contrast, the DTEs are slightly more complicated to interpret, as they are expressed in terms of changes in CDFs.",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Defining Quantile and Distributional Treatment Effects</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-dte-intro.html#qtes-vs-quantiles-of-treatment-effects",
    "href": "qte-dte/qte-dte-intro.html#qtes-vs-quantiles-of-treatment-effects",
    "title": "18  Defining Quantile and Distributional Treatment Effects",
    "section": "18.3 QTEs vs Quantiles of Treatment Effects",
    "text": "18.3 QTEs vs Quantiles of Treatment Effects\n\n18.3.1 QTEs vs. Quantiles of Treatment Effects\nAn important aspect of QTEs and DTEs is that those objects are not equal to the quantiles or distributions of treatment effects outside of some special cases. For example, for quantiles it is usually the case that \\[\n     Q_{Y^{x_2}}(\\tau) -    Q_{Y^{x_1}}(\\tau)  \\neq Q_{Y^{x_2}- Y^{x_1}}(\\tau).\n\\] Similarly, the DTE in general is only loosely related to the distribution of treatment effects. In this sense, calling QTEs and DTEs “treatment effects” may be somewhat misleading, and one should always be careful in practice with the interpretations assigned to these objects.\n\n\n18.3.2 Bounds for Distribution and Quantiles of Treatment Effects\nThe focus on QTEs and DTEs is driven by the fact the actual distribution of quantile effects is only partially identified even in experimental settings (outside of restrictive settings). Even the best experiments usually allow us to identify at most the marginal distributions \\(F_{Y^1}\\) and \\(F_{Y^0}\\), but not the joint distribution of the two potential outcomes. The following bounds are the most that can be said about the distribution \\(F_{Y^1-Y^0}(\\cdot)\\) and about the quantiles of \\(Y^1_i-Y_i^0\\): \\[\n\\begin{aligned}\n    F^L(y) & \\leq F_{Y^1-Y^0}(y)\\leq F^U(y),\\\\\n    Q^L(\\tau) & \\leq Q_{Y^1-Y^0}(\\tau) \\leq Q^U(\\tau),\n\\end{aligned}\n\\] for \\[\n\\begin{aligned}\n    F^L(y) & = \\sup_w \\max\\curl{(F_1(w) - F_0(w-y), 0},\\\\\n    F^U(y) & = 1+ \\inf_w\\min\\curl{F_1(w) - F_0(w-y), 0 },\\\\\n            Q^L(\\tau) & = \\sup_{w\\in [0, \\tau]}\\left( F_X^{-1}(w) + F_Y^{-1}(\\tau-w) \\right), \\\\\n             Q^U(\\tau) & = \\inf_{w\\in [\\tau, 1]} \\left(F_X^{-1}(w)  + F^{-1}_Y(1+\\tau-w)\\right). \\\\\n\\end{aligned}\n\\] See Makarov (1981) and Williamson and Downs (1990) regarding these bounds. They are known to be pointwise sharp without further assumptions, though Firpo and Ridder (2019) describe some refinements in case of looking at multiple values of \\(y\\) or \\(\\tau\\). Fan and Park (2010) discuss inference on such bounds in practice.\nUltimately, the switch to the less satisfactory QTE and DTE parameters is the price that we pay for working in cross-sectional settings and not restricting the form of potential outcomes.\n\n\n18.3.3 The Comonotonic Case\nQTEs do correspond to the quantile of the treatment effect in the special case of perfect rank correlation (comonotonicity, rank preservation) between potential outcomes (Doksum 1974). Consider the case of a binary treatment \\(x=0, 1\\). Under comonotonicity, potential outcomes share a common rank structure. Formally, they admit a Skorokhod representation \\[\n    (Y^1_i, Y^0_i) = \\left(F_{Y^1}^{-1}(U_i), F_{Y^0}^{-1}(U_i)\\right).\n\\] where \\(U_i\\) is a common Uniform[0, 1] random variable.\nIntuitively, under comonotonicity each individual has the same rank in all of the potential outcome distributions. For example, if a person would be in the 80th percentile of earnings after a training program, they would be in the 80th percentile of earnings without training too. The same preservation of rank would hold for all units.\nIn this special case it is indeed true that the difference in quantiles is equal to the quantile of the difference (the treatment effect). Moreover, the above representation also implies that the potential outcomes can be determined as \\[\n    Y^1_i = F_{Y^1}^{-1}\\left( F_{Y^0} (Y^0_i) \\right).\n\\]\nComonotonicity is an example of the monotonicity assumptions discussed at the beginning of the section. Accordingly, it is likely unrealistic in practice. For example, imagine that \\(x\\) are different possible career paths (e.g. ballet dance and astronaut) and that \\(Y_i^x\\) are earnings of person \\(i\\) in career \\(x\\). Comonotonicity would mean that the person is equally good or bad at both ballet and being an astronaut (relative to the population), which seems like an implausibly strong assumption in most labor markets.\n\n\nNext Section\nIn the next section, we start our discussion of identification of QTEs and DTEs with a simple randomized control trial setting with (unconditional) unconfoundedness.\n\n\n\n\nAthey, Susan, and G Imbens. 2017. “The Econometrics of Randomized Experiments.” In Handbook of Economic Field Experiments, 1:73–140. North-Holland. https://doi.org/10.1016/bs.hefe.2016.10.003.\n\n\nChernozhukov, Victor, Iván Fernández-Val, and Blaise Melly. 2013. “Inference on Counterfactual Distributions.” Econometrica 81 (6): 2205–68. https://doi.org/10.3982/ecta10582.\n\n\nDoksum, Kjell. 1974. “Empirical Probability Plots and Statistical Inference for Nonlinear Models in the Two-Sample Case.” The Annals of Statistics 2 (2). https://doi.org/10.1214/aos/1176342662.\n\n\nFan, Yanqin, and Sang Soo Park. 2010. “Sharp bounds on the distribution of treatment effects and their statistical inference.” Econometric Theory 26 (3): 931–51. https://doi.org/10.1017/S0266466609990168.\n\n\nFirpo, Sergio, and Geert Ridder. 2019. “Partial identification of the treatment effect distribution and its functionals.” Journal of Econometrics 213 (1): 210–34. https://doi.org/10.1016/j.jeconom.2019.04.012.\n\n\nHoderlein, Stefan, and Enno Mammen. 2007. “Identification of Marginal Effects in Nonseparable Models without Monotonicity.” Econometrica 75 (5): 1513–18. https://doi.org/10.1111/j.1468-0262.2007.00801.x.\n\n\nMakarov, G. D. 1981. “Estimates for the Distribution Function of a Sum of Two Random Variables When the Marginal Distributions are Fixed.” Theory of Probability Amd Its Applications 26 (4): 803–6. https://doi.org/10.1137/1126086.\n\n\nMatzkin, Rosa L. 2003. “Nonparametric Estimation of Nonadditive Random Functions.” Econometrica 71 (5): 1339–75. https://doi.org/10.1111/1468-0262.00452.\n\n\n———. 2007. “Nonparametric identification.” In Handbook of Econometrics, Vol. 6B, edited by James J Heckman and Edward E B T - Handbook of Econometrics Leamer, 6:5307–68. Elsevier. https://doi.org/https://doi.org/10.1016/S1573-4412(07)06073-4.\n\n\nPowell, David. 2020. “Quantile Treatment Effects in the Presence of Covariates.” Review of Economics and Statistics 102 (5): 994–1005. https://doi.org/10.1162/rest_a_00858.\n\n\nWilliamson, Robert C., and Tom Downs. 1990. “Probabilistic Arithmetic. I. Numerical Methods for Calculating Convolutions and Dependency Bounds.” International Journal of Approximate Reasoning 4 (2): 89–158. https://doi.org/10.1016/0888-613X(90)90022-T.",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Defining Quantile and Distributional Treatment Effects</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-dte-id-rct.html",
    "href": "qte-dte/qte-dte-id-rct.html",
    "title": "19  Nonparametric Identification of QTEs and DTEs in Simple RCTs",
    "section": "",
    "text": "19.1 Introduction\nHaving set the stage, we now turn to identifying quantile and distributional treatment effects introduced in section 18.\nWe structure our discussion in two steps:\nOur approach remains fully nonparametric: we impose no functional form restrictions on how potential outcomes depend on treatments or unobserved heterogeneity.",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Nonparametric Identification of QTEs and DTEs in Simple RCTs</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-dte-id-rct.html#introduction",
    "href": "qte-dte/qte-dte-id-rct.html#introduction",
    "title": "19  Nonparametric Identification of QTEs and DTEs in Simple RCTs",
    "section": "",
    "text": "This section: we consider identification in a simple (pure) randomized control settings where the realized treatment \\(X_i\\) is fully independent from potential outcomes \\(Y_i^x\\).\nThe next two sections: we generalize the arguments to a setting with (conditional) unconfoundedness, where independence may hold only conditionally on some variables \\(W_i\\).\n\n\n\n\n\n\n\nIdentification under endogeneity is also possible, although outside the scope of this course; see Chernozhukov and Hansen (2005) and chapter 17 in Koenker et al. (2017).",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Nonparametric Identification of QTEs and DTEs in Simple RCTs</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-dte-id-rct.html#setting",
    "href": "qte-dte/qte-dte-id-rct.html#setting",
    "title": "19  Nonparametric Identification of QTEs and DTEs in Simple RCTs",
    "section": "19.2 Setting",
    "text": "19.2 Setting\n\n19.2.1 Assumption: Unconditional Unconfoundedness\nIn this section we consider the simplest setting in which the treatment \\(X_i\\) is marginally independent from the collection of potential outcomes indexed by treatment values \\(x\\): \\[\n    \\curl{Y^x_i}_{x} \\independent X_i.\n\\tag{19.1}\\]\nThere may potentially be some covariates \\(W_i\\), which are also marginally independent of \\(X_i\\): \\[\n    W_i \\independent X_i.\n\\tag{19.2}\\]\nThis setting is represented by the single-world intervention graph on Figure 19.1, where the realized treatment node \\(X_i\\) is independent from the potential outcome \\(Y_i^x\\) (see chapter 7 of Hernan and Robins (2024) for a textbook introduction to single-world intervention graphs).\n\n\n\n\n\n\nFigure 19.1: graphical representation of setting Equation 19.1. Note that the arrow is going from \\(x\\) to \\(Y^x_i\\), not from \\(X_i\\) to \\(Y^x_i\\)\n\n\n\nIntuitively, the setting of Equation 19.1 is a simple randomized controlled trial where the treatment is assigned independently of potential outcomes and other individual characteristics (unconditional unconfoundedness or a random effects setting, in panel data terminology).\nAs always, the realized outcome \\(Y_i\\) is equal to \\(Y^x_i\\) if \\(X_i=x\\) (i.e., we observe the potential outcome corresponding to the treatment actually received). The treatment \\(X_i\\) may be discrete- or continuous-valued. For continuous treatments, we assume standard regularity conditions (e.g., existence of densities) to ensure that conditioning on \\(\\curl{X_i=x}\\) is well-defined.\n\n\n19.2.2 Parameters of Interest\nIn the setting of Equation 19.1, interest lies in two kinds of QTEs and DTEs:\n\nUnconditional QTE and DTE of Equation 18.1: \\[\n\\begin{aligned}\n  QTE(x_1, x_2, \\tau ) &  =Q_{Y^{x_2}}(\\tau) -    Q_{Y^{x_1}}(\\tau) ,\\\\\n      DTE(x_1, x_2, y) & = F_{Y^{x_2}}(y) - F_{Y^{x_1}}(y).\n\\end{aligned}\n\\]\nQTEs and DTEs conditional on some value of \\(W_i\\), defined in Equation 18.3: \\[\n\\begin{align}\n  QTE(x_1, x_2, \\tau|w) & = Q_{Y^{x_2}|W}(\\tau|w) - Q_{Y^{x_1}|W}(\\tau|w), \\\\\n      DTE(x_1, x_2, y|w) & = F_{Y^{x_2}|W}(y|w) - F_{Y^{x_1}|W}(y|w).\n\\end{align}\n\\]\n\nObserve that no new parameter is gained by conditioning on values of \\(X_i\\) — the independence restriction in Equation 19.1 implies that \\[\nF_{Y^{x_1}|X}(\\cdot|x_1) = F_{Y^{x_1}}(\\cdot)\n\\] for any \\((x_1, x_2)\\). Hence the QTEs and DTEs of Equation 18.2 are equal to the unconditional QTEs and DTEs in Equation 18.1.",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Nonparametric Identification of QTEs and DTEs in Simple RCTs</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-dte-id-rct.html#identification",
    "href": "qte-dte/qte-dte-id-rct.html#identification",
    "title": "19  Nonparametric Identification of QTEs and DTEs in Simple RCTs",
    "section": "19.3 Identification",
    "text": "19.3 Identification\nFor all parameters of interest, it is sufficient to identify the quantile or distribution functions of the potential outcomes.\n\n19.3.1 Unconditional CDF of Potential Outcomes\nWe begin with the CDF \\(F_{Y^x}(\\cdot)\\) of \\(Y^x_i\\). To start, observe that we can use the independence assumption (19.1) to write \\[\n    F_{Y^x}(y)  \\equiv \\E\\left[ \\I\\curl{Y^x_i\\leq y} \\right] = \\E\\left[ \\I\\curl{Y^x_i\\leq y} |X_i=x \\right].\n\\] Since \\(Y^x_i\\) is independent from \\(X_i\\), we may freely condition on event involving \\(X_i\\) without changing the expected value.\nNext, under the event \\(\\curl{X_i=x}\\) it holds that the realized outcome \\(Y_i\\) is exactly \\(Y^x_i\\), allowing us to conclude that \\[\n\\E\\left[ \\I\\curl{Y^x_i\\leq y} |X_i=x \\right]= \\E\\left[ \\I\\curl{Y_i\\leq y}|X_i=x \\right] \\equiv  F_{Y|X}(y|x),\n\\] where \\(F_{Y|X}(y|x)\\) is the CDF of the realized outcome \\(Y_i\\) in the subpopulation of units with \\(X_i=x\\).\nWe conclude that, as long as there exist units with \\(X_i=x\\) in population, the marginal CDF of \\(Y^x_i\\) is identified as \\[\nF_{Y^x}(y) = F_{Y|X}(y|x).\n\\]\n\n\n19.3.2 Unconditional Quantile Function of Potential Outcomes\nIn order to identify the quantiles \\(Q_{Y^x}(\\tau)\\) of potential outcomes, for each treatment value \\(x\\), we assume the CDF \\(F_{Y^x}(\\cdot)\\) is continuous and strictly increasing. Then the quantile function \\(Q_{Y^x}(\\tau)\\) is well-defined and uniquely characterized by the moment condition \\[\n  P\\left(Y^x_i\\leq  Q_{Y^x}(\\tau)  \\right) =     \\tau  \n\\] or, in other words, that \\[\n\\tau  =  \\E\\left[ \\I\\curl{Y^x_i\\leq Q_{Y^x}(\\tau)} \\right]\n\\]\nWe can now again apply the same conditioning argument to the above moment to conclude that \\[\n\\E\\left[ \\I\\curl{Y^x_i\\leq Q_{Y^x}(\\tau)} \\right] = \\E\\left[\\I\\curl{Y^x_i\\leq Q_{Y^x}(\\tau) }  |X_i=x   \\right].\n\\]\nFinally, we can again use that \\(Y^x_i=Y_i\\) under \\(X_i=x\\) to connect the above moment to observed variables: \\[\n\\E\\left[\\I\\curl{Y^x_i\\leq Q_{Y^x}(\\tau) }  |X_i=x   \\right] = \\E\\left[\\I\\curl{Y_i\\leq Q_{Y^x}(\\tau) }  |X_i=x   \\right].\n\\]\nCombining the two extreme of the above chain of equalities, we obtain that \\(Q_{Y^x}(\\tau)\\) must satisfy the moment condition: \\[\n\\tau = \\E\\left[\\I\\curl{Y_i\\leq Q_{Y^x}(\\tau) }  |X_i=x   \\right]\n\\] At the same time, under our continuity and strict monotonicity assumptions we know that \\(Q_Y(\\tau)\\) (the \\(\\tau\\)th quantile of the observed outcome \\(Y_i\\)) uniquely satisfies \\[\n\\tau = \\E\\left[\\I\\curl{Y_i\\leq Q_{Y}(\\tau) }  |X_i=x   \\right].\n\\]\nWe conclude that it must be the case that \\[\n     Q_{Y^x}(\\tau) = Q_{Y|X}(\\tau|x).\n\\] In other words, the \\(\\tau\\)th quantile of \\(Y^x_i\\) is exactly the \\(\\tau\\)th quantile of the observed outcome \\(Y_i\\) in the group with \\(X_i=x\\).\n\n\n19.3.3 CDF and Quantiles Conditional on Covariates\nFinally, we turn to identifying the conditional distribution function \\(F_{Y^x|W}(y|w)\\) and conditional quantile function \\(Q_{Y^x|W}(\\tau|w)\\) that appear in the conditional QTE and DTE of Equation 18.3.\nThe argument for conditional CDFs and quantiles mirrors that of the unconditional case and again relies on the independence restriction (19.1), along with the independence condition (19.2). For example, the identification argument for the CDF looks as follows: \\[\n\\begin{aligned}\nF_{Y^x|W}(y|w) & = \\E\\left[ \\I\\curl{Y^x_i\\leq y}|W_i=w \\right] \\\\\n& = \\E\\left[ \\I\\curl{Y^x_i\\leq y} |X_i=x, W_i=w \\right]\\\\\n& = \\E\\left[ \\I\\curl{Y_i\\leq y}|X_i=x, W_i=w \\right]\\\\\n& = F_{Y|X, W}(y|x, w),\n\\end{aligned}\n\\] where \\(F_{Y|X, W}(y|x, w)\\) is the CDF of the realized outcome \\(Y_i\\) in the subpopulation of units with \\(X_i=x\\) and \\(W_i=w\\). As before, the group of units with \\(X_i=x\\) identifies the distribution of potential outcomes under \\(x\\). The argument for the quantiles of \\(Y^x_i\\) is entirely analogous, and leads to the identifying expression \\[\n     Q_{Y^x|W}(\\tau|w) = Q_{Y|X, W}(\\tau|x, w).\n\\]\n\n\nNext Section\nIn the next section, we continue our discussion of identification and consider identification under the (conditional) unconfoundedness assumption that \\(Y_i^x\\independent X_i|W_i\\).\n\n\n\n\nChernozhukov, Victor, and Christian Hansen. 2005. “An IV Model of Quantile Treatment Effects.” Econometrica 73 (1): 245–61. https://doi.org/10.1111/j.1468-0262.2005.00570.x.\n\n\nHernan, Miguel A., and James M. Robins. 2024. Causal Inference: What If. First edition. Boca Raton: Taylor and Francis.\n\n\nKoenker, Roger, Victor Chernozhukov, Xuming He, and Limin Peng, eds. 2017. Handbook of Quantile Regression. 1st ed. Chapman and Hall/CRC. https://doi.org/10.1201/9781315120256.",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Nonparametric Identification of QTEs and DTEs in Simple RCTs</span>"
    ]
  },
  {
    "objectID": "qte-dte/dte-id-conf.html",
    "href": "qte-dte/dte-id-conf.html",
    "title": "20  Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness",
    "section": "",
    "text": "20.1 Setting\nIn the simple RCT setting of the previous section, identification of QTEs and DTEs is particularly simple. The required quantile and distribution functions can be directly obtained from conditional quantile and distribution functions of the realized outcomes.\nHowever, in practice, it is more frequent and realistic to see a weaker (conditional) unconfoundedness assumption, which requires that the realized treatment \\(X_i\\) and the collection of potential outcomes \\(\\curl{Y_i^x}_x\\) are independent only conditionally on some covariates \\(W_i\\): \\[\n\\curl{Y_i^x}_x \\independent X_i|W_i.\n\\tag{20.1}\\] This setting is depicted on Figure 20.1. The setting of Equation 20.1 may arise both in observational settings and in experiments where randomization occurs within strata defined by \\(W_i\\).",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness</span>"
    ]
  },
  {
    "objectID": "qte-dte/dte-id-conf.html#setting",
    "href": "qte-dte/dte-id-conf.html#setting",
    "title": "20  Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness",
    "section": "",
    "text": "Figure 20.1: representation of setting of Equation 20.1. Note that the arrow is going from \\(x\\) to \\(Y^x_i\\), not from \\(X_i\\) to \\(Y^x_i\\). Likewise, the arrow is going from \\(W_i\\) to \\(X_i\\), not \\(x\\).",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness</span>"
    ]
  },
  {
    "objectID": "qte-dte/dte-id-conf.html#parameters-of-interest",
    "href": "qte-dte/dte-id-conf.html#parameters-of-interest",
    "title": "20  Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness",
    "section": "20.2 Parameters of Interest",
    "text": "20.2 Parameters of Interest\nThe setting of Equation 20.1 is richer than that of Equation 19.1. It allows us to consider all of the unconditional and conditional QTEs and DTEs described in section 18.\nSpecifically, this section focuses exclusively on DTE, whose identification is more explicit and allows us to showcase the key tools. QTEs are deferred to the next section.\nIn order of identification, our parameters of interest are:\n\nDTE conditional on \\(W_i=w\\) (Equation 18.3): \\[\n\\begin{align}\nDTE(x_1, x_2, y|w) & = F_{Y^{x_2}|W}(y|w) - F_{Y^{x_1}|W}(y|w).\n\\end{align}\n\\]\nUnconditional DTE (Equation 18.1): \\[\n\\begin{aligned}\nDTE(x_1, x_2, y) & = F_{Y^{x_2}}(y) - F_{Y^{x_1}}(y).\n\\end{aligned}\n\\]\nDTE conditional on \\(X_i=x_3\\) (Equation 18.2): \\[\n\\begin{align}\nDTE(x_1, x_2, y|x_3) & = F_{Y^{x_2}|X}(y|x_3) - F_{Y^{x_1}|X}(y|x_3).\n\\end{align}\n\\]\n\nAs before, identifying DTEs reduces to identifying the CDFs \\(F_{Y^{x}|W}(y|w)\\), \\(F_{Y^{x}}(y)\\), and \\(F_{Y^{x}|X}(y|x_3)\\). The following subsections tackle these CDFs in turn.",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness</span>"
    ]
  },
  {
    "objectID": "qte-dte/dte-id-conf.html#cdf-conditional-on-w_i",
    "href": "qte-dte/dte-id-conf.html#cdf-conditional-on-w_i",
    "title": "20  Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness",
    "section": "20.3 CDF Conditional on \\(W_i\\)",
    "text": "20.3 CDF Conditional on \\(W_i\\)\nWe begin with with the most straightforward identification argument — that of the conditional distribution of \\(Y^x_i\\) given \\(W_i=w\\).\nBy definition, we can write the target CDF as \\[\nF_{Y^x|W}(y|w) = \\E[\\I\\curl{Y^x_i \\leq y}|W_i=w].\n\\]\nAs before, the overall goal is to connect the above expression to the realized variables \\((Y_i, X_i, W_i)\\), chief of them the outcome \\(Y_i\\). To do so, we first use the unconfoundedness assumption (20.1) to condition on \\(X_i=x\\): \\[\n\\begin{aligned}\n& \\E[\\I\\curl{Y^x_i \\leq y}|W_i=w] \\\\\n& =  \\E[\\I\\curl{Y^x_i \\leq y}|X_i=x, W_i=w].\n\\end{aligned}\n\\] Since \\(Y^x_i\\) and \\(X_i\\) are conditionally independent given \\(W_i\\), this conditioning operation does not change the value of the expectation regardless of the value of \\(X_i\\), including \\(x\\).\nNext, we use that \\(Y_i=Y^x_i\\) if \\(X_i=x\\) to replace \\(Y^x_i\\) with \\(Y_i\\): \\[\n\\begin{align}\n    &   \\E\\left[\\I\\curl{Y^x_i\\leq y}|X_i=d, W_i=w \\right]\\\\\n    & = \\E\\left[ \\I\\curl{Y_i\\leq y}|X_i=d, W_i=w   \\right]\\\\\n    & \\equiv F_{Y|X, W}(y|x, w),\n\\end{align}\n\\] where \\(F_{Y|X, W}(y|x, w)\\) is the conditional CDF of the realized outcome \\(Y_i\\) given \\(X_i=x\\) and \\(W_i=w\\).\nCombining the above chain of equalities, we conclude that \\[\nF_{Y^x|W}(y|w) = F_{Y|X, W}(y|x, w).\n\\tag{20.2}\\] In words, the CDF of interest is equal to a particular conditional distribution of the observed outcome \\(Y_i\\).",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness</span>"
    ]
  },
  {
    "objectID": "qte-dte/dte-id-conf.html#unconditional-cdf",
    "href": "qte-dte/dte-id-conf.html#unconditional-cdf",
    "title": "20  Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness",
    "section": "20.4 Unconditional CDF",
    "text": "20.4 Unconditional CDF\nWe now turn to the unconditional CDF of \\(Y^x_i\\): \\[\nF_{Y^x}(y)   = \\E\\left[ \\I\\curl{Y^x_i \\leq y} \\right].\n\\] As before, we would like to replace \\(Y^x_i\\) with the observed outcome \\(Y_i\\). However, this time we cannot naively insert conditioning on \\(\\curl{X_i=x}\\) without changing the expectation.\nThe solution we follow is to iterate expectations:\n\nFirst condition on \\(W_i\\) using the defining property of conditional expectations.\nApply the condition (20.1) to the conditional expectation.\n\nThe first step produces the following identity: \\[\n\\begin{aligned}\n& \\E\\left[ \\I\\curl{Y^x_i \\leq y} \\right] \\\\\n& = \\E\\left[ \\E\\left[\\I\\curl{Y^x_i \\leq y} | W_i  \\right]\\right].\n\\end{aligned}\n\\]\nWe can now use the unconfoundedness assumption (20.1) to condition on \\(X_i\\) inside the conditional expectation. By conditional independence in (20.1), this conditioning does not affect the expectation (more precisely, the distribution of the conditional expectation in question):\n\\[\n\\begin{aligned}\n& \\E\\left[ \\E\\left[\\I\\curl{Y^x_i \\leq y} | W_i  \\right]\\right]\\\\\n& = \\E\\left[ \\E\\left[\\I\\curl{Y^x_i \\leq y} | X_i=x, W_i  \\right]\\right].\n\\end{aligned}\n\\]\nFrom this point out, we can replace \\(Y^x_i\\) with \\(Y_i\\) under the conditional expectation to finish the argument. In order to obtain a more explicit representation, let \\(F_W(\\cdot)\\) be the marginal CDF of \\(W_i\\). Then:\n\\[\n\\begin{aligned}\n    &   \\E\\left[ \\E\\left[\\I\\curl{Y^x_i \\leq y} | X_i=x, W_i  \\right]\\right] \\\\\n    & = \\E\\left[ \\E\\left[ \\I\\curl{Y_i\\leq y}| X_i=x, W_i \\right]  \\right]\\\\\n    & = \\E\\left[  F_{Y|X, W}(y|x, W_i) \\right]\\\\\n    & = \\int F_{Y|X, W}(y|x, w)F_W(dw).\n\\end{aligned}\n\\]\nCombining the above equalities, we obtain the identifying expression: \\[\nF_{Y^x}(y) = \\int F_{Y|X, W}(y|x, w)F_W(dw).\n\\] In words, the marginal CDF of \\(Y^x_i\\) is equal to a reweighted conditional CDF of the observed outcome, where the weights are calculated over \\(w\\) and correspond to the marginal distribution of \\(W_i\\). Observe that the object in the last line is not equal to \\(F_{Y|X}(y|x)\\).\nNote that the same result can be obtained by simply integrating \\(W_i\\) out in the conditional CDF expression in Equation 20.2. Since the CDF is an average, Warning 18.1 does not apply to it. However, there is merit in studying the conditioning argument presented above, as it is useful both for the remaining conditional CDF and gives a path forward for unconditional quantiles (to which Warning 18.1 does apply).",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness</span>"
    ]
  },
  {
    "objectID": "qte-dte/dte-id-conf.html#cdf-conditional-on-x_i",
    "href": "qte-dte/dte-id-conf.html#cdf-conditional-on-x_i",
    "title": "20  Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness",
    "section": "20.5 CDF Conditional on \\(X_i\\)",
    "text": "20.5 CDF Conditional on \\(X_i\\)\nFinally, we consider the CDF \\(F_{Y^x|X}(y|x_3)\\) — the distribution of the potential outcome under \\(x\\) in the group which received a potentially different treatment \\(x_3\\). This CDF can be represented as \\[\nF_{Y^x|X}(y|x_3) = \\E\\left[ \\I\\curl{Y^x_i \\leq y}|X_i=x_3 \\right].\n\\] Identification here requires a new trick: exploiting conditional independence (20.1) to swap treatment values in the conditioning set.\nLike for the unconditional CDF, we start by iterating expectations with respect to \\(W_i\\) \\[\n\\begin{aligned}\n  &    \\E\\left[ \\I\\curl{Y^x_i \\leq y}|X_i=x_3 \\right] \\\\\n    & = \\E\\left[ \\E\\left[ \\I\\curl{Y^x_i \\leq y}|X_i=x_3, W_i \\right]|X_i=x_3\\right].\n\\end{aligned}\n\\]\nPreviously, at this point we would replace \\(Y^x_i\\) by \\(Y_i\\) to continue. However, the conditioning set in the internal conditional expectation specifies that \\(\\curl{X_i=x_3}\\), in which case \\(Y_i\\) is actually equal to \\(Y^{x_3}_i\\).\nTo resolve this obstacle, observe that by the unconfoundedness assumption (20.1), conditioning on \\(X_i\\) does not affect the expectation (more precisely, the distribution of the conditional expectation in question). We can then replace \\(\\curl{X_i=x_3}\\) with \\(\\curl{X_i=x}\\): \\[\n\\begin{aligned}\n    & \\E\\left[ \\E\\left[ \\I\\curl{Y^x_i \\leq y}|X_i=x_3, W_i \\right]|X_i=x_3\\right]\\\\\n    & = \\E\\left[ \\E\\left[ \\I\\curl{Y^x_i \\leq y}|X_i=x, W_i \\right]|X_i=x_3\\right] \\\\\n    & = \\E\\left[ \\E\\left[ \\I\\curl{Y_i\\leq y}|X_i=x, W_i \\right]|X_i=x_3\\right]\\\\\n    & = \\E\\left[ F_{Y|X, W}(y|x, W_i)|X_i=x_3 \\right]\\\\\n    & = \\int F_{Y|X, W}(y|x, w) F_{W|X}(dw|x_3).  \n\\end{aligned}\n\\] Note that this derivation makes an assumption of common support. Specifically, we assume that the support of \\(W_i\\) conditional on \\(X_i=x_3\\) is included in the support of \\(W_i\\) conditional on \\(X_i=x\\). Without such an assumption the integral may not be well-defined.\nCombining the above equalities, we obtain the identifying expression: \\[\nF_{Y^x|X}(y|x_3) = \\int F_{Y|X, W}(y|x, w) F_{W|X}(dw|x_3),\n\\] When \\(x=x_3\\), the above argument simply reduces to \\[\n     F_{Y^x|X}(y|x) = F_{Y|X}(y|x).\n\\]\n\n\n\n\n\n\nCounterfactual distributions\n\n\n\nIntegrals of the kind \\[\n\\int F_{Y|X, W}(y|x_1, w) F_{W|X}(dw|x_2),\n\\tag{20.3}\\] are sometimes called counterfactual distributions (Machado and Mata 2005; Chernozhukov, Fernández-Val, and Melly 2013).\nTo understand the name, consider the following example. Let \\(X_i\\) be gender, \\(Y_i\\) wages, and \\(W_i\\) be further earnings-relevant covariates. Then \\(\\int F_{Y|X, W}(y|\\text{man}, w) F_{W|X}(dw|\\text{woman})\\) may be interpreted as the distribution of wages for women (that is, computed using the demographic composition of women with \\(F_{W|X}(\\cdot|\\text{woman})\\)) if they faced men’s wage schedule \\(F_{Y|X, W}(\\cdot|\\text{man}, w)\\) (how \\(Y_i\\) is determined for each \\(w\\) for men). Such integrals may be used to compute Oaxaca-Blinder-type decompositions.\nNote that the counterfactual distribution (20.3) is expressed in terms of distributions of realized variables. It only acquires a causal interpretations under suitable conditions (e.g. unconfoundedness, as we describe in this section).\n\n\n\n\nNext Section\nIn the next section, we complement the results of this section with identification of quantile treatment effects in the setting of Equation 20.1.\n\n\n\n\nChernozhukov, Victor, Iván Fernández-Val, and Blaise Melly. 2013. “Inference on Counterfactual Distributions.” Econometrica 81 (6): 2205–68. https://doi.org/10.3982/ecta10582.\n\n\nMachado, José A. F., and José Mata. 2005. “Counterfactual Decomposition of Changes in Wage Distributions Using Quantile Regression.” Journal of Applied Econometrics 20 (4): 445–65. https://doi.org/10.1002/jae.788.",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Nonparametric Identification of Distributional Treatment Effects under Unconfoundedness</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-id-conf.html",
    "href": "qte-dte/qte-id-conf.html",
    "title": "21  Nonparametric Identification of Quantile Treatment Effects under Unconfoundedness",
    "section": "",
    "text": "21.1 Focus: Quantile Treatment Effects\nHaving established identification for distributional treatment effects (DTEs), we now turn to the question of quantile treatment effects (QTEs). Our arguments in this section are broadly similar to those for DTEs, but tackle some quantile-specific challenges.\nAs in the previous section, we maintain the (conditional) unconfoundedness assumption from Equation 20.1:\n\\[\n\\curl{Y_i^x}_x \\independent X_i|W_i.\n\\]\nAs noted in section 18, we focus on the following parameters:\nOf these three parameters, we show a detailed identification result for the first two, while the second conditional QTE can be identified using a combination of previous arguments. As before, it is sufficient to identify the quantile of \\(Y^x_i\\), a task to which we now turn.",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Nonparametric Identification of Quantile Treatment Effects under Unconfoundedness</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-id-conf.html#focus-quantile-treatment-effects",
    "href": "qte-dte/qte-id-conf.html#focus-quantile-treatment-effects",
    "title": "21  Nonparametric Identification of Quantile Treatment Effects under Unconfoundedness",
    "section": "",
    "text": "QTE conditional on \\(W_i=w\\) (Equation 18.3): \\[\n\\begin{align}\nQTE(x_1, x_2, \\tau|w) & = Q_{Y^{x_2}|W}(\\tau|w) - Q_{Y^{x_1}|W}(\\tau|w).\n\\end{align}\n\\]\nUnconditional QTE (Equation 18.1): \\[\n\\begin{aligned}\nQTE(x_1, x_2, \\tau ) &  =Q_{Y^{x_2}}(\\tau) -    Q_{Y^{x_1}}(\\tau) .\n\\end{aligned}\n\\]\nQTE conditional on \\(X_i=x_3\\) (Equation 18.2): \\[\n\\begin{align}\nQTE(x_1, x_2, \\tau|x_3) &  =Q_{Y^{x_2}|X}(\\tau|x_3) -   Q_{Y^{x_1}|X}(\\tau|x_3) .\n\\end{align}\n\\]",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Nonparametric Identification of Quantile Treatment Effects under Unconfoundedness</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-id-conf.html#quantile-function-conditional-on-w_i",
    "href": "qte-dte/qte-id-conf.html#quantile-function-conditional-on-w_i",
    "title": "21  Nonparametric Identification of Quantile Treatment Effects under Unconfoundedness",
    "section": "21.2 Quantile Function Conditional on \\(W_i\\)",
    "text": "21.2 Quantile Function Conditional on \\(W_i\\)\nWe again begin with the conditional quantiles of \\(Y^x_i\\) given \\(W_i=w\\), which is the most straightforward quantile to identify.\nLike in section 19, we assume that that for all \\(x\\) the conditional CDF of \\(Y^x_i\\) given \\(W_i=w\\) is continuous and strictly increasing. Under these assumptions the conditional quantile \\(Q_{Y^x|W}(\\tau|x)\\) uniquely satisfies the following moment condition:\n\\[\n\\E\\left[ \\I\\curl{ Y^x_i \\leq Q_{Y^x|W}(\\tau|w) } |W_i=w\\right] =  \\tau.\n\\]\nWe again aim to connect to write the above moment only in terms of \\((Y_i, W_i, X_i)\\) and the unknown target parameter \\(Q_{Y^x|W}(\\tau|w)\\).\nThe first step is to use the unconfoundedness assumption (20.1) to condition on \\(X_i=x\\): \\[\n\\begin{aligned}\n& \\E\\left[ \\I\\curl{ Y^x_i \\leq Q_{Y^x|W}(\\tau|w) } |W_i=w\\right] \\\\\n& =  \\E\\left[ \\I\\curl{ Y^x_i \\leq Q_{Y^x|W}(\\tau|w) } |X_i=x, W_i=w\\right].\n\\end{aligned}\n\\]\nSince \\(Y_i=Y^x_i\\) under \\(\\curl{X_i=x}\\), we can replace \\(Y^x_i\\) with \\(Y_i\\): \\[\n\\begin{aligned}\n&\n\\E\\left[ \\I\\curl{ Y^x_i \\leq Q_{Y^x|W}(\\tau|w) } |X_i=x, W_i=w\\right] \\\\\n& = \\E\\left[ \\I\\curl{ Y_i \\leq Q_{Y^x|W}(\\tau|w) } |X_i=x, W_i=w\\right] .\n\\end{aligned}\n\\] We conclude that \\(Q_{Y^x|W}(\\tau|w)\\) must satisfy the moment condition \\[\n\\tau = \\E\\left[ \\I\\curl{ Y_i \\leq Q_{Y^x|W}(\\tau|w) } |X_i=x, W_i=w\\right].\n\\] Here we again observe that the CDF of \\(Y_i\\) given \\(W_i=w\\) and \\(X_i=x\\) is exactly the CDF of \\(Y^x_i\\) given \\(W_i=w\\), and hence strictly increasing and continuous. Let \\(Q_{Y|X, W}(\\tau|x, w)\\) be the \\(\\tau\\)th quantile of \\(Y_i\\) given \\(\\curl{X_i=x, W_i=w}\\). By continuity and monotonicity, \\(Q_{Y|X, W}(\\tau|x, w)\\) also uniquely satisfies the above moment condition. We conclude the following identification result: \\[\n    Q_{Y^x|W}(\\tau|w) = Q_{Y|X, W}(\\tau|x, w).\n\\tag{21.1}\\] In other words, the conditional quantile of potential outcome given covariates is equal to the conditional quantile of the observed outcomes given both covariates and the treatment. This result is a direct mirror of the CDF identification result (Equation 20.2).",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Nonparametric Identification of Quantile Treatment Effects under Unconfoundedness</span>"
    ]
  },
  {
    "objectID": "qte-dte/qte-id-conf.html#unconditional-quantile-function",
    "href": "qte-dte/qte-id-conf.html#unconditional-quantile-function",
    "title": "21  Nonparametric Identification of Quantile Treatment Effects under Unconfoundedness",
    "section": "21.3 Unconditional Quantile Function",
    "text": "21.3 Unconditional Quantile Function\nThe argument for unconditional quantile \\(Q_{Y^x}(\\tau)\\) is more delicate. In contrast to the CDF, we cannot integrate Equation 21.1 with respect to the marginal distribution of \\(W_i\\) (see Warning 18.1). Instead, identifying \\(Q_{Y^x}(\\tau)\\) involves using the conditioning trick showcased for the unconditional CDF in the last session.\nAs before, we start with the moment condition satisfied by the quantile of interest: \\[\n\\tau = P(Y^x_i\\leq Q_{Y^x}(\\tau) ).\n\\]\nLike for the unconditional CDF, we use a conditioning trick to be able to apply unconfoundedness (20.1). We first iterate expectations with respect to \\(W_i\\):\n\\[\n\\begin{aligned}\n    \\tau & = P(Y^x_i\\leq Q_{Y^x}(\\tau) ) \\\\\n    & =  \\E\\left[ \\I\\curl{Y^x_i\\leq Q_{Y^x}(\\tau)  }\\right]\\\\\n    & =  \\E\\left[ \\E\\left[ \\I\\curl{Y^x_i\\leq  Q_{Y^x}(\\tau)  }|W_i \\right]  \\right] .\n\\end{aligned}\n\\] We can now apply unconfoundedness inside the conditional expectation to link the moment condition to the observed outcome \\(Y_i\\): \\[\n\\begin{aligned}\n    & \\E\\left[\\E\\left[ \\I\\curl{Y^x_i\\leq  Q_{Y^x}(\\tau)  }|W_i \\right]  \\right] \\\\\n    & =   \\E\\left[ \\E\\left[\\I\\curl{Y^x_i\\leq Q_{Y^x}(\\tau)  }| X_i=x, W_i\\right]\\right]  \\\\\n    & = \\E\\left[ \\E\\left[\\I\\curl{Y_i\\leq Q_{Y^x}(\\tau)  }| X_i=x, W_i\\right]\\right].\n\\end{aligned}\n\\] In principle, we now have obtained a moment condition that characterizes the quantile of interest: \\[\n\\tau = \\E\\left[ \\E\\left[\\I\\curl{Y_i\\leq Q_{Y^x}(\\tau)  }| X_i=x, W_i\\right]\\right].\n\\]\nHowever, this condition is impractical: it involves an iterated expectation, and the parameter of interest is inside that conditional expectation.\nInstead, we now develop a more practical characterization where conditioning is limited to propensity scores. To start, we observe that under the event \\(\\curl{X_i=x}\\) it holds that \\(\\I\\curl{X_i=x}\\) is equal to 1. We can then multiply by \\(\\I\\curl{X_i=x}\\)(=1) under the conditional expectation: \\[\n    \\begin{aligned}\n        &  \\E\\left[ \\E\\left[\\I\\curl{Y_i\\leq Q_{Y^x}(\\tau)  }| X_i=x, W_i\\right]\\right] \\\\\n        & =  \\E\\left[ \\E\\left[\\I\\curl{X_i=x}\\I\\curl{Y\\leq Q_{Y^x}(\\tau) }|X_i=x, W_i\\right]\\right]\n    \\end{aligned}\n\\tag{21.2}\\]\nTo proceed, we use the following version of the law of total expectations:\n\n\n\n\n\n\nIf \\(V, Z\\) are random variables and \\(T\\) is a 0/1 random variable, then \\[\n\\begin{aligned}\n        \\E[Z|V]  & =  P(T=1|V) \\E[Z|V, T=1] \\\\\n        & \\quad   + (1-P(T=1|V)) \\E[Z|V, T=0].\n\\end{aligned}\n\\tag{21.3}\\]\n\n\n\nTo apply the above law, we take \\(T = \\I\\curl{X_i=x}\\) and \\(Z = T\\I\\curl{Y_i\\leq Q_{Y^x}(\\tau)}\\). By construction, if \\(T=0\\), then \\(Z=0\\), and so the second term in Equation 21.3 is zero, and hence it holds that \\[\n    \\begin{aligned}\n&      \\E\\left[\\I\\curl{X_i=x}\\I\\curl{Y_i\\leq Q_{Y^x}(\\tau) }|X_i=x, W_i\\right]\n\\\\\n& = \\dfrac{1}{P\\left(X_i=x|W_i\\right)} \\E[\\I\\curl{X_i=x} \\I\\curl{Y_i\\leq  Q_{Y^x}(\\tau) }|W_i].\n    \\end{aligned}\n\\]\nWe can now insert this characterization back into Equation 21.2, and finish the argument as follows: \\[\n\\begin{aligned}\n  &  \\E\\left[ \\E\\left[\\I\\curl{X_i=x}\\I\\curl{Y_i\\leq Q_{Y^x}(\\tau) }|X_i=x, W_i\\right]\\right]\\\\\n    & = \\E\\left[ \\dfrac{1}{P\\left(X_i=x|W_i\\right)} \\E[\\curl{X_i=x} \\I\\curl{Y_i\\leq  Q_{Y^x}(\\tau) }|W_i] \\right]\\\\\n    & =  \\E\\left[\\E\\left[  \\dfrac{\\I\\curl{X_i=x}}{P\\left(X_i=x|W_i\\right)}  \\I\\curl{Y_i\\leq  Q_{Y^x}(\\tau) }|W_i\\right] \\right] \\\\\n    & = \\E\\left[  \\dfrac{\\I\\curl{X_i=x} }{P\\left(X_i=x|W_i\\right)}\\I\\curl{Y_i\\leq Q_{Y^x}(\\tau) }\\right].\n\\end{aligned}\n\\] Here we have again used the defining property of conditional expectations, this time to get rid of the internal conditional expectation.\nIn summary, we have shown that the unconditional quantile of interest \\(Q_{Y^x}(\\tau)\\) satisfies the following moment condition: \\[\n     \\E\\left[  \\dfrac{\\I\\curl{X_i=x} }{P\\left(X_i=x|W_i\\right)}\\I\\curl{Y_i\\leq Q_{Y^x}(\\tau) }\\right] = \\tau.\n\\] This argument is originally due to Firpo (2007).\nIntuitively, this moment conditions shows that \\(Q_{Y^x}(\\tau)\\) is the \\(\\tau\\)th quantile of a suitably reweighted distribution of the observed outcome. Weighting is done by the inverse of the propensity scores \\(P(X_i=x|W_i)\\), and hence the above result is an instance of identification using inverse probability weighting.\nAs a final observation, we note that it is also possible to obtain a somewhat similar IPW expression for \\(F_{Y^d}(y)\\) by proceeding as with \\(Q_{Y^d}\\) starting from the iterated expectation. See Donald and Hsu (2014).\n\n\nNext Section\nIn the next section, we move towards estimation of these parameters by briefly introducing quantile regression.\n\n\n\n\nDonald, Stephen G., and Yu-Chin Hsu. 2014. “Estimation and Inference for Distribution Functions and Quantile Functions in Treatment Effect Models.” Journal of Econometrics 178 (January): 383–97. https://doi.org/10.1016/j.jeconom.2013.03.010.\n\n\nFirpo, Sergio. 2007. “Efficient Semiparametric Estimation of Quantile Treatment Effects.” Econometrica 75 (1): 259–76. https://doi.org/10.1111/j.1468-0262.2007.00738.x.",
    "crumbs": [
      "Quantile and Distributional Treatment Effects",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Nonparametric Identification of Quantile Treatment Effects under Unconfoundedness</span>"
    ]
  },
  {
    "objectID": "references/references.html",
    "href": "references/references.html",
    "title": "References",
    "section": "",
    "text": "Ahn, Seung Chan, and Peter Schmidt. 1995. “Efficient Estimation of Models for Dynamic Panel\nData.” Journal of Econometrics 68 (1): 5–27. https://doi.org/10.1016/0304-4076(94)01641-C.\n\n\nAltonji, Joseph G., and Rosa L. Matzkin. 2005. “Cross Section and Panel Data Estimators for Nonseparable\nModels with Endogenous Regerssors.” Econometrica\n73 (4): 1053–1102. https://doi.org/10.1111/j.1468-0262.2005.00609.x.\n\n\nAlvarez, Javier, and Manuel Arellano. 2003. “The Time Series and Cross-Section Asymptotics of Dynamic\nPanel Data Estimators.” Econometrica 71 (4):\n1121–59.\n\n\nAnderson, T. W., and Cheng Hsiao. 1982. “Formulation and estimation of dynamic models using panel\ndata.” Journal of Econometrics 18 (1): 47–82. https://doi.org/10.1016/0304-4076(82)90095-1.\n\n\nArellano, Manuel. 1989. “A note on the\nAnderson-Hsiao estimator for panel data.” Economics\nLetters 31 (4): 337–41. https://doi.org/10.1016/0165-1765(89)90025-6.\n\n\nArellano, Manuel, and Stephen Bond. 1991. “Some Tests of Specification for Panel Carlo Application\nto Data: Monte Carlo Evidence and an Application to Employment\nEquations.” Review of Economic Studies 58:\n277–97.\n\n\nArellano, Manuel, and Stéphane Bonhomme. 2012. “Identifying distributional characteristics in random\ncoefficients panel data models.” Review of Economic\nStudies 79 (3): 987–1020. https://doi.org/10.1093/restud/rdr045.\n\n\nAthey, Susan, and G Imbens. 2017. “The\nEconometrics of Randomized Experiments.” In Handbook\nof Economic Field Experiments, 1:73–140. North-Holland. https://doi.org/10.1016/bs.hefe.2016.10.003.\n\n\nBai, Jushan. 2009. “Panel Data Models With Interactive Fixed\nEffects.” Econometrica 77 (4): 1229–79. https://doi.org/10.3982/ECTA6135.\n\n\nBai, Yu, Massimiliano Marcellino, and George Kapetanios. 2023.\n“Mean Group Instrumental Variable Estimation of Time-varying Large Heterogeneous Panels With Endogenous\nRegressors.” Econometrics and Statistics, June,\nS2452306223000412. https://doi.org/10.1016/j.ecosta.2023.06.004.\n\n\nBeckert, Walter, and Richard Blundell. 2008. “Heterogeneity and\nthe Non-Parametric Analysis of\nConsumer Choice: Conditions for\nInvertibility.” The Review of Economic\nStudies 75 (4): 1069–80. https://doi.org/10.1111/j.1467-937X.2008.00500.x.\n\n\nBeran, Rudolf, Andrey Feuerverger, and Peter Hall. 1996. “On Nonparametric Estimation of Intercept and Slope\nDistributions in Random Coefficient Regression.” The\nAnnals of Statistics 24 (6): 2569–92. https://doi.org/10.1214/aos/1032181170.\n\n\nBester, C Alan, and Christian Hansen. 2009. “Identification of Marginal Effects in a Nonparametric\nCorrelated Random Effects Model.” Journal of Business\nand Economic Statistics 27 (2): 235–50. https://doi.org/10.1198/jbes.2009.0017.\n\n\nBester, C Alan, and Christian B Hansen. 2016. “Grouped Effects Estimators in Fixed Effects\nModels.” Journal of Econometrics 190 (1):\n197–208. https://doi.org/10.1016/j.jeconom.2012.08.022.\n\n\nBlundell, Richard, and Stephen Bond. 1998. “Initial Conditions and Moment Restrictions in Dynamic\nPanel Data Models.” Journal of Econometrics 87:\n115–43.\n\n\nBlundell, Richard, Joel L. Horowitz, and Matthias Parey. 2012.\n“Measuring The Price Responsiveness of\nGasoline Demand: Economic Shape Restrictions and Nonparametric Demand\nEstimation.” Quantitative Economics 3 (1): 29–51.\nhttps://doi.org/10.3982/qe91.\n\n\nBoneva, Lena, Oliver Linton, and Michael Vogt. 2015. “A\nSemiparametric Model for Heterogeneous Panel Data with Fixed\nEffects.” Journal of Econometrics, Heterogeneity in\nPanel Data and in Nonparametric\nAnalysis in honor of Professor\nCheng Hsiao, 188 (2): 327–45. https://doi.org/10.1016/j.jeconom.2015.03.003.\n\n\nBonhomme, Stéphane, Thibaut Lamadon, and Elena Manresa. 2022.\n“Discretizing Unobserved Heterogeneity.”\nEconometrica 90 (2): 625–43. https://doi.org/10.3982/ECTA15238.\n\n\nBonhomme, Stéphane, and Elena Manresa. 2015. “Grouped Patterns of Heterogeneity in Panel\nData.” Econometrica 83 (3): 1147–84. https://doi.org/10.3982/ecta11319.\n\n\nBonhomme, Stéphane, and Jean-Marc Robin. 2009. “Assessing the\nEqualizing Force of Mobility Using Short\nPanels: France, 1990–2000.” The Review of\nEconomic Studies 76 (1): 63–92. https://doi.org/10.1111/j.1467-937X.2008.00521.x.\n\n\n———. 2010. “Generalized Non-Parametric Deconvolution\nwith an Application to Earnings\nDynamics.” Review of Economic Studies 77 (2):\n491–533. https://doi.org/10.1111/j.1467-937X.2009.00577.x.\n\n\nBreitung, Jörg, and Nazarii Salish. 2021. “Estimation of Heterogeneous Panels with Systematic Slope\nVariations.” Journal of Econometrics 220 (2):\n399–415. https://doi.org/10.1016/j.jeconom.2020.04.007.\n\n\nBrockwell, Peter J., and Richard A. Davis. 2016. Introduction to\nTime Series and Forecasting. Springer\nTexts in Statistics. Springer International\nPublishing.\n\n\nCampello, Murillo, Antonio F. Galvao, and Ted Juhl. 2019. “Testing for Slope Heterogeneity Bias in Panel Data\nModels.” Journal of Business and Economic\nStatistics 37 (4): 749–60. https://doi.org/10.1080/07350015.2017.1421545.\n\n\nCard, David. 2001. “Estimating the Return to\nSchooling: Progress on Some Persistent Econometric\nProblems.” Econometrica 69 (5): 1127–60. https://doi.org/10.1111/1468-0262.00237.\n\n\nChamberlain, Gary. 1982. “Multivariate\nRegression Models for Panel Data.” Journal of\nEconometrics 18 (1): 5–46. https://doi.org/10.1016/0304-4076(82)90094-X.\n\n\nChen, Xiaohong, and Chunrong Ai. 2003. “Efficient Estimation of Models with Conditional Moment\nRestrictions Containing Unknown Functions.”\nEconometrica 71 (6): 1795–1843.\n\n\nChernozhukov, Victor, Iván Fernández-Val, Jinyong Hahn, and Whitney K.\nNewey. 2013. “Average and Quantile Effects in\nNonseparable Panel Models.” Econometrica 81 (2):\n535–80. https://doi.org/10.3982/ecta8405.\n\n\nChernozhukov, Victor, Iván Fernández-Val, Stefan Hoderlein, Hajo\nHolzmann, and Whitney Newey. 2015. “Nonparametric Identification in Panels Using\nQuantiles.” Journal of Econometrics 188 (2):\n378–92. https://doi.org/10.1016/j.jeconom.2015.03.006.\n\n\nChernozhukov, Victor, Iván Fernández-Val, and Blaise Melly. 2013.\n“Inference on Counterfactual\nDistributions.” Econometrica 81 (6): 2205–68. https://doi.org/10.3982/ecta10582.\n\n\nChernozhukov, Victor, and Christian Hansen. 2005. “An IV\nModel of Quantile Treatment Effects.”\nEconometrica 73 (1): 245–61. https://doi.org/10.1111/j.1468-0262.2005.00570.x.\n\n\nChesher, Andrew, and Adam M. Rosen. 2017. “Generalized\nInstrumental Variable Models.” Econometrica 85\n(3): 959–89. https://doi.org/10.3982/ecta12223.\n\n\n———. 2020. Generalized Instrumental Variable\nModels, Methods, and Applications. Vol. 7. Elsevier B.V. https://doi.org/10.1016/bs.hoe.2019.11.001.\n\n\nCombes, Pierre Philippe, Gilles Duranton, Laurent Gobillon, Diego Puga,\nand Sébastien Roux. 2012. “The Productivity\nAdvantages of Large Cities: Distinguishing Agglomeration From Firm\nSelection.” Econometrica 80 (6): 2543–94. https://doi.org/10.3982/ecta8442.\n\n\nCooprider, Joseph, Stefan Hoderlein, and Alexander Meister. 2022.\n“A Panel Data Estimator for the Distribution\nand Quantiles of Marginal Effects in Nonlinear Structural Models with an\nApplication to the Demand for Junk Food.” https://doi.org/10.2139/ssrn.3545485.\n\n\nDoksum, Kjell. 1974. “Empirical Probability Plots and\nStatistical Inference for Nonlinear Models in\nthe Two-Sample Case.” The Annals of\nStatistics 2 (2). https://doi.org/10.1214/aos/1176342662.\n\n\nDonald, Stephen G., and Yu-Chin Hsu. 2014. “Estimation and\nInference for Distribution Functions and\nQuantile Functions in Treatment Effect\nModels.” Journal of Econometrics 178 (January):\n383–97. https://doi.org/10.1016/j.jeconom.2013.03.010.\n\n\nDurrett, Rick. 2019. Probability: Theory and\nExamples. Cambridge University Press. https://doi.org/10.1017/9781108591034.\n\n\nEvdokimov, Kirill. 2010. “Identification and\nEstimation of a Nonparametric Panel Data Model with Unobserved\nHeterogeneity.”\n\n\nEvdokimov, Kirill, and Halbert White. 2012. “Some Extensions of a Lemma of Kotlarski.”\nEconometric Theory 28 (4): 925–32. https://doi.org/10.1017/S0266466611000831.\n\n\nFan, Jianqing, and Irene Gijbels. 1996. Local\nPolynomial Modelling and Its Applications. Springer New\nYork.\n\n\nFan, Yanqin, and Sang Soo Park. 2010. “Sharp\nbounds on the distribution of treatment effects and their statistical\ninference.” Econometric Theory 26 (3): 931–51. https://doi.org/10.1017/S0266466609990168.\n\n\nFirpo, Sergio. 2007. “Efficient Semiparametric\nEstimation of Quantile Treatment Effects.”\nEconometrica 75 (1): 259–76. https://doi.org/10.1111/j.1468-0262.2007.00738.x.\n\n\nFirpo, Sergio, and Geert Ridder. 2019. “Partial identification of the treatment effect\ndistribution and its functionals.” Journal of\nEconometrics 213 (1): 210–34. https://doi.org/10.1016/j.jeconom.2019.04.012.\n\n\nGraham, Bryan S, and James L Powell. 2012. “Identification and Estimation of Average Partial Effects\nin \"Irregular\" Correlated Random Coefficient Panel Data\nModels.” Econometrica 80 (5): 2105–52. https://doi.org/10.3982/ecta8220.\n\n\nHansen, Bruce. 2022. Econometrics. Princeton University Press.\n\n\nHeckman, James J., Jeffrey Smith, and Nancy Clements. 1997. “Making the Most out of Programme Evaluations and Social\nExperiments : Accounting for Heterogeneity in Programme\nImpacts.” Review of Economic Studies 64 (4):\n487–535. https://doi.org/10.2307/2971729.\n\n\nHeckman, James, and Edward Vytlacil. 1998. “Instrumental variables methods for the correlated random\ncoefficient model.” Journal of Human Resources 33\n(4): 974–87.\n\n\nHenderson, Daniel J., Raymond J. Carroll, and Qi Li. 2008.\n“Nonparametric Estimation and Testing of Fixed Effects Panel Data\nModels.” Journal of Econometrics 144 (1): 257–75. https://doi.org/10.1016/j.jeconom.2008.01.005.\n\n\nHenderson, Daniel J., and Alexandra Soberon. 2024. “Nonparametric\nModels with Fixed\nEffects.” In The Econometrics of\nMulti-Dimensional Panels: Theory\nand Applications, edited by Laszlo Matyas, 285–323.\nCham: Springer International Publishing. https://doi.org/10.1007/978-3-031-49849-7_9.\n\n\nHernan, Miguel A., and James M. Robins. 2024. Causal\nInference: What If. First edition. Boca\nRaton: Taylor and Francis.\n\n\nHoderlein, Stefan, Jussi Klemelä, and Enno Mammen. 2010. “Analyzing the Random Coefficient Model\nNonparametrically.” Econometric Theory 26 (03):\n804–37. https://doi.org/10.1017/S0266466609990119.\n\n\nHoderlein, Stefan, and Enno Mammen. 2007. “Identification of Marginal Effects in Nonseparable Models\nwithout Monotonicity.” Econometrica 75 (5):\n1513–18. https://doi.org/10.1111/j.1468-0262.2007.00801.x.\n\n\nHoderlein, Stefan, and Halbert White. 2012. “Nonparametric Identification in Nonseparable Panel Data\nModels with Generalized Fixed Effects.” Journal of\nEconometrics 168 (2): 300–314. https://doi.org/10.1016/j.jeconom.2012.01.033.\n\n\nHsiao, Cheng, M. Hashem Pesaran, and A. Kamil Tahmiscioglu. 1999.\n“Bayes Estimation of Short-Run Coefficients\nin Dynamic Panel Data Models.” In Analysis of Panels\nand Limited Dependent Variable Models, 268–96. https://doi.org/10.1017/cbo9780511493140.013.\n\n\nHuntington-Klein, Nick. 2025. The Effect: An\nIntroduction to Research Design and\nCausality. S.l.: Chapman and Hall/CRC.\n\n\nImbens, Guido W., and Whitney K. Newey. 2009. “Identification and Estimation of Triangular Simultaneous\nEquations Models Without Additivity.”\nEconometrica 77 (5): 1481–1512. https://doi.org/10.3982/ecta7108.\n\n\nKato, Kengo, Yuya Sasaki, and Takuya Ura. 2021. “Robust\nInference in Deconvolution.”\nQuantitative Economics 12 (1): 109–42. https://doi.org/10.3982/QE1643.\n\n\nKiviet, Jan F. 1995. “On Bias, Inconsistency,\nand Efficiency of Various Estimators in Dynamic Panel Data\nModels.” Journal of Econometrics 68 (1): 53–78.\nhttps://doi.org/10.1016/0304-4076(94)01643-E.\n\n\nKoenker, Roger, Victor Chernozhukov, Xuming He, and Limin Peng, eds.\n2017. Handbook of Quantile Regression. 1st ed.\nChapman and Hall/CRC. https://doi.org/10.1201/9781315120256.\n\n\nKotlarski, Ignacy. 1967. “On Characterizing the\nGamma and the Normal Distribution.”\nPacific Journal of Mathematics 20 (1): 69–76. https://doi.org/10.2140/pjm.1967.20.69.\n\n\nLaage, Louise. 2024. “A Correlated Random Coefficient Panel\nModel With Time-Varying Endogeneity.” Journal of\nEconometrics 242 (2): 105804. https://doi.org/10.1016/j.jeconom.2024.105804.\n\n\nLee, Jungyoon, and Peter M. Robinson. 2015. “Panel Nonparametric\nRegression with Fixed Effects.” Journal of Econometrics,\nHeterogeneity in Panel Data and in\nNonparametric Analysis in honor of\nProfessor Cheng Hsiao, 188 (2):\n346–62. https://doi.org/10.1016/j.jeconom.2015.03.004.\n\n\nLewbel, Arthur. 2022. “Kotlarski with a\nFactor Loading.” Journal of Econometrics 229 (1):\n176–79. https://doi.org/10.1016/j.jeconom.2020.12.012.\n\n\nLewbel, Arthur, Susanne M. Schennach, and Linqi Zhang. 2024.\n“Identification of a Triangular Two Equation System Without\nInstruments.” Journal of Business & Economic\nStatistics 42 (1): 14–25. https://doi.org/10.1080/07350015.2023.2166052.\n\n\nLi, Qi, and Jeffrey Scott Racine. 2007. Nonparametric Econometrics: Theory and\nPractice. Princeton University Press.\n\n\nLiu, Laura, Alexandre Poirier, and Ji-Liang Shiu. 2024.\n“Identification and Estimation of Partial\nEffects in Nonlinear Semiparametric Panel\nModels.” Journal of Econometrics, September,\n105860. https://doi.org/10.1016/j.jeconom.2024.105860.\n\n\nLumsdaine, Robin L., Ryo Okui, and Wendun Wang. 2023. “Estimation\nof Panel Group Structure Models With Structural Breaks in\nGroup Memberships and Coefficients.”\nJournal of Econometrics 233 (1): 45–65. https://doi.org/10.1016/j.jeconom.2022.01.001.\n\n\nMachado, José A. F., and José Mata. 2005. “Counterfactual\nDecomposition of Changes in Wage\nDistributions Using Quantile Regression.” Journal of\nApplied Econometrics 20 (4): 445–65. https://doi.org/10.1002/jae.788.\n\n\nMakarov, G. D. 1981. “Estimates for the\nDistribution Function of a Sum of Two Random Variables When the Marginal\nDistributions are Fixed.” Theory of Probability Amd\nIts Applications 26 (4): 803–6. https://doi.org/10.1137/1126086.\n\n\nManski, Charles F. 1987. “Semiparametric\nAnalysis of Random Effects Linear Models from Binary Panel\nData.” Econometrica 55 (2): 357. https://doi.org/10.2307/1913240.\n\n\nMasry, Elias. 1996a. “Multivariate Local\nPolynomial Regression for Time Series: Uniform Strong Consistency and\nRates.” Journal of Time Series Analysis 17 (6):\n571–99. https://doi.org/10.1111/j.1467-9892.1996.tb00294.x.\n\n\n———. 1996b. “Multivariate Regression\nEstimation: Local Polynomial Fitting for Time Series.”\nStochastic Processes and Their Applications 65 (1): 3575–81. https://doi.org/10.1016/S0304-4149(96)00095-6.\n\n\nMasten, Matthew A. 2018. “Random Coefficients\non Endogenous Variables in Simultaneous Equations Models.”\nReview of Economic Studies 85 (2): 1193–1250. https://doi.org/10.1093/restud/rdx047.\n\n\nMatzkin, Rosa L. 2003. “Nonparametric\nEstimation of Nonadditive Random Functions.”\nEconometrica 71 (5): 1339–75. https://doi.org/10.1111/1468-0262.00452.\n\n\n———. 2007. “Nonparametric\nidentification.” In Handbook of Econometrics, Vol.\n6B, edited by James J Heckman and Edward E B T - Handbook of\nEconometrics Leamer, 6:5307–68. Elsevier. https://doi.org/https://doi.org/10.1016/S1573-4412(07)06073-4.\n\n\nMorozov, Vladislav. 2023. “Estimating the\nMoments and the Distribution of Heterogeneous Marginal Effects Using\nPanel Data.”\n\n\nMundlak, Yair. 1961. “Empirical Production Function\nFree of Management Bias.” Journal of Farm\nEconomics 43 (1): 44. https://doi.org/10.2307/1235460.\n\n\nMurtazashvili, Irina, and Jeffrey M. Wooldridge. 2008. “Fixed effects instrumental variables estimation in\ncorrelated random coefficient panel data models.”\nJournal of Econometrics 142 (1): 539–52. https://doi.org/10.1016/j.jeconom.2007.09.001.\n\n\nNickell, Stephen. 1981. “Biases In Dynamic Models With Fixed\nEffects.” Econometrica 49 (6): 1417–26.\n\n\nPesaran, Hashem, Ron Smith, and Kyung So Im. 1996. “Dynamic Linear Models for Heterogenous\nPanels.” In The Econometrics of Panel Data,\nedited by L. Matyas and P. Sevestre, 145–95. https://doi.org/10.1007/978-94-009-0137-7_8.\n\n\nPesaran, M. Hashem. 2006. “Estimation and\nInference in Large Heterogeneous Panels with a Multifactor Error\nStructure.” Econometrica 74 (4): 967–1012. https://doi.org/10.1111/j.1468-0262.2006.00692.x.\n\n\nPesaran, M. Hashem, Yongcheol Shin, and Ron P. Smith. 1999. “Pooled Mean Group Estimation of Dynamic Heterogeneous\nPanels.” Journal of the American Statistical\nAssociation 94 (446): 621–34. https://doi.org/10.1080/01621459.1999.10474156.\n\n\nPesaran, M. Hashem, and Ron P. Smith. 1995. “Estimating long-run relationships from dynamic\nheterogeneous panels.” Journal of Econometrics\n6061: 473–77.\n\n\nPowell, David. 2020. “Quantile Treatment\nEffects in the Presence of Covariates.” Review of\nEconomics and Statistics 102 (5): 994–1005. https://doi.org/10.1162/rest_a_00858.\n\n\nSasaki, Yuya, and Takuya Ura. 2021. “Slow\nMovers in Panel Data,” 1–40. https://arxiv.org/abs/2110.12041.\n\n\nSchennach, Susanne M. 2016. “Recent Advances in the\nMeasurement Error Literature.” Annual Review of\nEconomics 8 (1): 341–77. https://doi.org/10.1146/annurev-economics-080315-015058.\n\n\nStefanski, Leonard A., and Raymond J. Carroll. 1985. “Covariate Measurement Error in Logistic\nRegression.” The Annals of Statistics 13 (4):\n1335–51. https://doi.org/10.1214/aos/1176349741.\n\n\nSury, Tavneet. 2011. “Selection and\nComparative Advantage in Technology Adoption.”\nEconometrica 79 (1): 159–209. https://doi.org/10.3982/ecta7749.\n\n\nWilliamson, Robert C., and Tom Downs. 1990. “Probabilistic Arithmetic. I. Numerical Methods for\nCalculating Convolutions and Dependency Bounds.”\nInternational Journal of Approximate Reasoning 4 (2): 89–158.\nhttps://doi.org/10.1016/0888-613X(90)90022-T.\n\n\nWooldridge, Jeffrey M. 2003. “Fixed Effects\nEstimation of the Population-Averaged Slopes in a Panel Data Random\nCoefficient Model.” Econometric Theory 19:\n411–13. https://doi.org/10+10170S0266466603002081.\n\n\n———. 2005. “Fixed-effects and related\nestimators for correlated random-coefficient and treatment-effect panel\ndata models.�� The Review of Economics and\nStatistics 87 (May): 385–90.",
    "crumbs": [
      "References"
    ]
  }
]